@inproceedings{abadi_tensorflow:_2016,
  title = {Tensorflow: A System for Large-Scale Machine Learning.},
  booktitle = {{{OSDI}}},
  author = {Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael},
  year = {2016},
  volume = {16},
  pages = {265--283}
}

@article{adams_automating_2006,
  title = {Automating {{Image Matching}}, {{Cataloging}}, and {{Analysis}} for {{Photo-Identification Research}}},
  author = {Adams, Jeffrey D. and Speakman, Todd and Zolman, Eric and Schwacke, Lori H.},
  year = {2006},
  month = sep,
  journal = {Aquatic Mammals},
  volume = {32},
  number = {3},
  pages = {374--384},
  issn = {01675427, 00000000},
  doi = {10.1578/AM.32.3.2006.374},
  urldate = {2023-04-18},
  abstract = {The expanding use of digital photography for marine mammal photo-identification has created a need for tools to analyze and manage growing image file archives. While database management systems have been commonly employed to manage text and numerical data generated by photo-identification research, their use for the analysis and management of associated image files has been limited. This paper describes a photo-identification database management system with embedded image analysis and management capabilities. Matching and cataloging are expedited using a multiple-attribute, non-metric catalog sorting algorithm. Algorithm efficiency at locating catalog matches for bottlenose dolphins was compared to the performance of a more traditional singleattribute, non-metric approach. Locating catalog matches under the multiple-attribute approach required at least 50\% fewer comparisons for 90\% of the 409 individuals tested. For 50\% of the individuals, 80\% fewer comparisons were required. System utility is further extended through embedded mapping components that allow researchers to visually inspect sighting locations following each survey and to examine sighting histories for specific individuals. In addition, a companion ArcGIS\texttrademark{} extension allows researchers to quickly explore and interact with the photo-identification data within a GIS environment. This system, while created for a bottlenose dolphin research application, can be adapted to accommodate photo-identification research on a variety of other species.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/AFZGBJWQ/Adams et al. - 2006 - Automating Image Matching, Cataloging, and Analysi.pdf}
}

@article{akiba_optuna_2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10902 [cs.LG]},
  number = {arXiv:1907.10902},
  eprint = {1907.10902},
  primaryclass = {cs.LG},
  urldate = {2022-06-14},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/G7BZ4F6I/Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf}
}

@inproceedings{akkaynak_sea-thru_2019,
  title = {Sea-{{Thru}}: {{A Method}} for {{Removing Water From Underwater Images}}},
  shorttitle = {Sea-{{Thru}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Akkaynak, Derya and Treibitz, Tali},
  year = {2019},
  month = jun,
  pages = {1682--1691},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00178},
  urldate = {2020-01-16},
  abstract = {Robust recovery of lost colors in underwater images remains a challenging problem. We recently showed that this was partly due to the prevalent use of an atmospheric image formation model for underwater images and proposed a physically accurate model. The revised model showed: 1) the attenuation coefficient of the signal is not uniform across the scene but depends on object range and reflectance, 2) the coefficient governing the increase in backscatter with distance differs from the signal attenuation coefficient. Here, we present the first method that recovers color with our revised model, using RGBD images. The Sea-thru method estimates backscatter using the dark pixels and their known range information. Then, it uses an estimate of the spatially varying illuminant to obtain the range-dependent attenuation coefficient. Using more than 1,100 images from two optically different water bodies, which we make available, we show that our method with the revised model outperforms those using the atmospheric model. Consistent removal of water will open up large underwater datasets to powerful computer vision and machine learning algorithms, creating exciting opportunities for the future of underwater exploration and conservation.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/3XD6HGA4/Akkaynak and Treibitz - 2019 - Sea-Thru A Method for Removing Water From Underwa.pdf}
}

@inproceedings{albawi_understanding_2017,
  title = {Understanding of a Convolutional Neural Network},
  booktitle = {2017 {{International Conference}} on {{Engineering}} and {{Technology}} ({{ICET}})},
  author = {Albawi, Saad and Mohammed, Tareq Abed and {Al-Zawi}, Saad},
  year = {2017},
  month = aug,
  pages = {1--6},
  doi = {10.1109/ICEngTechnol.2017.8308186},
  abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
  keywords = {artificial neural networks,computer vision,Convolution,convolutional neural networks,Convolutional neural networks,deep learning,Feature extraction,Image edge detection,Image recognition,machine learning,Neurons},
  file = {/Users/b3020111/Zotero/storage/6RPQJKFJ/Albawi et al. - 2017 - Understanding of a convolutional neural network.pdf;/Users/b3020111/Zotero/storage/7DIXE8YK/8308186.html}
}

@article{alber_backprop_2018,
  title = {Backprop {{Evolution}}},
  author = {Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.02822 [cs, stat]},
  eprint = {1808.02822},
  primaryclass = {cs, stat},
  urldate = {2019-08-08},
  abstract = {The back-propagation algorithm is the cornerstone of deep learning. Despite its importance, few variations of the algorithm have been attempted. This work presents an approach to discover new variations of the back-propagation equation. We use a domain specific language to describe update equations as a list of primitive functions. An evolution-based method is used to discover new propagation rules that maximize the generalization performance after a few epochs of training. We find several update equations that can train faster with short training times than standard back-propagation, and perform similar as standard back-propagation at convergence.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/R9F77IAP/Alber et al. - 2018 - Backprop Evolution.pdf}
}

@article{alves_population_2013,
  title = {Population Structure of Short-Finned Pilot Whales in the Oceanic Archipelago of {{Madeira}} Based on Photo-Identification and Genetic Analyses: Implications for Conservation},
  shorttitle = {Population Structure of Short-Finned Pilot Whales in the Oceanic Archipelago of {{Madeira}} Based on Photo-Identification and Genetic Analyses},
  author = {Alves, Filipe and Qu{\'e}rouil, Sophie and Dinis, Ana and Nicolau, C{\'a}tia and Ribeiro, Cl{\'a}udia and Freitas, Lu{\'i}s and Kaufmann, Manfred and Fortuna, Caterina},
  year = {2013},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {23},
  number = {5},
  pages = {758--776},
  issn = {1099-0755},
  doi = {10.1002/aqc.2332},
  urldate = {2021-06-11},
  abstract = {Pilot whales Globicephala spp. are known to display a hierarchical social pattern, but longitudinal data to infer population structure of short-finned pilot whales Globicephala macrorhynchus are rare. Using data collected between 2003-2011 in the oceanic archipelago of Madeira, the grouping structure of short-finned pilot whales was studied using photo-identification methods and mtDNA sequences and microsatellite markers to test the hypotheses that (1) there is at least one pelagic and one or more island-associated communities, and (2) groups are made of related individuals, with a matrilineal social structure. Pilot whales demonstrated a large degree of variability in site fidelity, including residents (up to 14-year interval), regular visitors and transients. The social and temporal analyses revealed a well-differentiated society with long-lasting relationships (of years). The genetic analyses suggested that individuals of the three residency patterns may not be genetically isolated, and that small groups are made up of related individuals, suggesting some degree of social philopatry, while large groups are probably temporary associations of smaller groups. It is proposed that the pilot whales encountered in Madeira belong to a single population encompassing several clans, possibly three clans of island-associated whales and others of transients, each containing two to three matrilineal pods, each with a mean of 15 individuals (SD=9, range: 4-29). We suggest that the clans interact for mating purposes when they meet. For management decisions, it is considered that the island-associated whales should not be regarded as demographically independent populations, but instead as stable social entities to be included in governmental management plans and requiring periodic evaluation of their status. The high proportion of marked individuals and low rate of mark change encourages further research in this species. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {ecological status,genetics,habitat management,island,mammals,ocean},
  file = {/Users/b3020111/Zotero/storage/EDH768JA/Alves et al. - 2013 - Population structure of short-finned pilot whales .pdf;/Users/b3020111/Zotero/storage/5JYZL3JI/aqc.html}
}

@inproceedings{anantharaman_utilizing_2018,
  title = {Utilizing {{Mask R-CNN}} for {{Detection}} and {{Segmentation}} of {{Oral Diseases}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Anantharaman, R. and Velazquez, M. and Lee, Y.},
  year = {2018},
  month = dec,
  pages = {2197--2204},
  doi = {10.1109/BIBM.2018.8621112},
  abstract = {In this paper, we demonstrate the application of Mask-RCNN, the state-of-the-art convolutional neural network algorithm for object detection and segmentation to the oral pathology domain. Mask-RCNN was originally developed for object detection, and object instance segmentation of natural images. With this experiment, we show that Mask-RCNN can also be used in a very specialized area such as oral pathology. While the number of oral diseases are numerous and varied in the form of Thrush, Leukoplakia, Lichenplanus, etc., we limited our scope to the detection and instance segmentation of two of the most commonly occurring conditions, herpes labialis (commonly referred to as ``cold sore`` and aphthous ulcer (commonly referred to as ``canker sore`` This paper aims at detecting and segmenting cold sores and canker sores only. As always, no computer based detection system can be 100\% reliable. An accurate diagnosis by a trained health care professional is necessary since several conditions of the mouth including oral cancer may mimic canker sores.},
  keywords = {aphthous ulcer,cancer,canker sore,cold sore ulcer,computer based detection system,convolutional neural nets,Convolutional neural networks,detecting segmenting cold sores,diseases,Diseases,Feature extraction,health care,image segmentation,Image segmentation,learning (artificial intelligence),Mask-RCNN,medical image processing,Mouth,object detection,Object detection,Object Detection,object instance segmentation,Object Segmentation,oral cancer,Oral disease,oral diseases,oral pathology domain,patient diagnosis,patient treatment,state-of-the-art convolutional neural network algorithm,Training},
  file = {/Users/b3020111/Zotero/storage/8QPX8RRR/8621112.html}
}

@article{andrews_best_2019,
  title = {Best Practice Guidelines for Cetacean Tagging},
  author = {Andrews, Russel D. and Baird, Robin W. and Calambokidis, John and Goertz, Caroline E. C. and Gulland, Frances M. D. and {Heide-Jorgensen}, Mads Peter and Hooker, Sascha K. and Johnson, Mark and Mate, Bruce and Mitani, Yoko and Nowacek, Douglas P. and Owen, Kylie and Quakenbush, Lori T. and Raverty, Stephen and Robbins, Jooke and Schorr, Gregory S. and Shpak, Olga V. and Townsend Jr., Forrest I. and Uhart, Marcela and Wells, Randall S. and Zerbini, Alexandre N.},
  year = {2019},
  month = jan,
  journal = {IWC Journal of Cetacean Research and Management},
  volume = {20},
  number = {1},
  pages = {27--66},
  issn = {2312-2706, 1561-0713},
  doi = {10.47536/jcrm.v20i1.237},
  urldate = {2022-09-20},
  abstract = {Animal-borne electronic instruments (tags) are valuable tools for collecting information on cetacean physiology, behaviour and ecology, and forenhancing conservation and management policies for cetacean populations. Tags allow researchers to track the movement patterns, habitat use andother aspects of the behaviour of animals that are otherwise difficult to observe. They can even be used to monitor the physiology of a taggedanimal within its changing environment. Such tags are ideal for identifying and predicting responses to anthropogenic threats, thus facilitating thedevelopment of robust mitigation measures. With the increasing need for data best provided by tagging and the increasing availability of tags, suchresearch is becoming more common. Tagging can, however, pose risks to the health and welfare of cetaceans and to personnel involved in taggingoperations. Here we provide `best practice' recommendations for cetacean tag design, deployment and follow-up assessment of tagged individuals,compiled by biologists and veterinarians with significant experience in cetacean tagging. This paper is intended to serve as a resource to assist tagusers, veterinarians, ethics committees and regulatory agency staff in the implementation of high standards of practice, and to promote the trainingof specialists in this area. Standardised terminology for describing tag design and illustrations of tag types and attachment sites are provided, alongwith protocols for tag testing and deployment (both remote and through capture-release), including training of operators. The recommendationsemphasise the importance of ensuring that tagging is ethically and scientifically justified for a particular project and that tagging only be used toaddress bona fide research or conservation questions that are best addressed with tagging, as supported by an exploration of alternative methods.Recommendations are provided for minimising effects on individual animals (e.g. through careful selection of the individual, tag design and implantsterilisation) and for improving knowledge of tagging effects on cetaceans through increased post-tagging monitoring.},
  file = {/Users/b3020111/Zotero/storage/AJHNPZ8I/Andrews et al. - 2019 - Best practice guidelines for cetacean tagging.pdf}
}

@inproceedings{anwar_invariant_2015,
  title = {Invariant {{Image-Based Species Classification}} of {{Butterflies}} and {{Reef Fish}}},
  booktitle = {Procedings of the {{Machine Vision}} of {{Animals}} and Their {{Behaviour Workshop}} 2015},
  author = {Anwar, Hafeez and Zambanini, Sebastian and Kampel, Martin},
  year = {2015},
  pages = {5.1-5.8},
  publisher = {{British Machine Vision Association}},
  address = {{Swansea}},
  doi = {10.5244/C.29.MVAB.5},
  urldate = {2020-05-15},
  abstract = {We propose a framework for species-based image classification of butterflies and reef fish. To support such image-based classification, we use an image representation which enriches the famous bag-of-visual words (BoVWs) model with spatial information. This image representation is developed by encoding the global geometric relationships of visual words in the 2D image plane in a scale- and rotation-invariant manner. In this way, invariance is achieved to the most common variations found in the images of these animals as they can be imaged at different image locations, exhibit various in-plane orientations and have various scales in the images. The images in our butterfly and reef fish datasets belong to 30 species of each animal. We achieve better classification rates on both the datasets than the ordinary BoVWs model while still being invariant to the mentioned image variations. Our proposed image-based classification framework for butterfly and reef fish species can be considered as a helpful tool for scientific research, conversation and education.},
  isbn = {978-1-901725-57-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/3HAHU6CR/Anwar et al. - 2015 - Invariant Image-Based Species Classification of Bu.pdf}
}

@article{araujo_photo-id_2019,
  title = {Photo-{{ID}} and Telemetry Highlight a Global Whale Shark Hotspot in {{Palawan}}, {{Philippines}}},
  author = {Araujo, Gonzalo and Agustines, Ariana and Tracey, Brian and Snow, Sally and Labaja, Jessica and Ponzo, Alessandro},
  year = {2019},
  month = dec,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {17209},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-53718-w},
  urldate = {2022-04-19},
  abstract = {Abstract             The Philippines is home to the second largest known population of whale sharks in the world. The species is listed as endangered due to continued population declines in the Indo-Pacific. Knowledge about the connectivity within Southeast Asia remains poor, and thus international management is difficult. Here, we employed pop-up archival tags, data mining and dedicated effort to understand an aggregation of whale sharks at Honda Bay, Palawan, Philippines, and its role in the species' conservation. Between Apr and Oct 2018, we conducted 159 surveys identifying 117 individual whale sharks through their unique spot patterns (96.5\% male, mean 4.5\,m). A further 66 individual whale sharks were identified from local operators, and data mined on social media platforms. The satellite telemetry data showed that the whale sharks moved broadly, with one individual moving to Sabah, Malaysia, before returning to the site {$<$}1 year later. Similarly, another tagged whale shark returned to the site at a similar periodicity after reaching the Malay-Filipino border. One individual whale shark first identified in East Kalimantan, Indonesia by a citizen scientist was resighted in Honda Bay \textasciitilde 3.5 years later. Honda Bay is a globally important site for the endangered whale shark with connectivity to two neighbouring countries, highlighting the need for international cooperation to manage the species.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/UMNA6MZR/Araujo et al. - 2019 - Photo-ID and telemetry highlight a global whale sh.pdf}
}

@article{araujo_population_2017,
  title = {Population Structure, Residency Patterns and Movements of Whale Sharks in {{Southern Leyte}}, {{Philippines}}: Results from Dedicated Photo-{{ID}} and Citizen Science},
  shorttitle = {Population Structure, Residency Patterns and Movements of Whale Sharks in {{Southern Leyte}}, {{Philippines}}},
  author = {Araujo, Gonzalo and Snow, Sally and So, Catherine Lee and Labaja, Jessica and Murray, Ryan and Colucci, Anna and Ponzo, Alessandro},
  year = {2017},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {27},
  number = {1},
  pages = {237--252},
  issn = {1099-0755},
  doi = {10.1002/aqc.2636},
  urldate = {2022-04-19},
  abstract = {The whale shark, Rhincodon typus, is a charismatic umbrella species whose highly mobile nature is not yet fully understood. Whale sharks roam the Philippine archipelago with two major aggregations known to occur at Donsol and at a provisioning site in Cebu. This is the first description of a previously identified aggregation occurring off Panaon Island, Southern Leyte through the use of photographic identification. In total, 93 individual whale sharks were identified, with significant male bias (58\%). The mean estimated total length of individuals was 5.72 {$\pm$} 1.02 m S.D., indicating a juvenile aggregation. Partial or complete fin amputations, potentially resulting from fishing lines, boat propellers or net entanglement, were observed on 27\% of animals, highlighting some of the risks human activities can have on this threatened species. Multiple parallel scars, identified as propeller impact, were observed on 45\% of animals. Dedicated research seasons in 2013 and 2014 yielded very different whale shark encounters with 366 in 2013 and 12 in 2014, yet highlighted the recurrence of individuals at the study site. Complemented by data collected through citizen science, maximum likelihood methods were used to model mean residency of whale sharks at Panaon Island of 27.04 days. The modelled lagged identification rate showed that many whale sharks return to the study site over time. Whale sharks from Panaon Island were identified through photo-ID and citizen science at other sites in the Philippines, as well as a match to Taiwan, representing the first international match through photo-ID in South-east Asia with a minimum distance covered of 1600 km. Given the highly mobile nature and recent exploitation of this species, management is recommended as a single unit regionally in South-east Asia. Additional research is needed to focus on the drivers of variation in encounters at whale shark aggregation sites. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {climate change,coastal,distribution,endangered species,fish,habitat management,ocean,recreation},
  file = {/Users/b3020111/Zotero/storage/245P4W6N/Araujo et al. - 2017 - Population structure, residency patterns and movem.pdf;/Users/b3020111/Zotero/storage/44GQJMT9/aqc.html}
}

@article{araujo_two-view_2022,
  title = {Two-View Fine-Grained Classification of Plant Species},
  author = {Ara{\'u}jo, Voncarlos M. and Britto Jr., Alceu S. and Oliveira, Luiz S. and Koerich, Alessandro L.},
  year = {2022},
  month = jan,
  journal = {Neurocomputing},
  volume = {467},
  pages = {427--441},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.10.015},
  urldate = {2022-06-12},
  abstract = {Automatic plant classification is challenging due to the vast biodiversity of the existing plant species in a fine-grained scenario. Robust deep learning architectures have been used to improve the classification performance in such a fine-grained problem but usually build models that are highly dependent on a large training dataset and are not scalable. This paper proposes a novel method based on a two-view leaf image representation and a hierarchical classification strategy for fine-grained plant species recognition. It uses the botanical taxonomy as a basis for a coarse-to-fine strategy applied to identify the plant genus and species. The two-view representation provides complementary global and local features of leaf images. A deep metric based on Siamese Convolutional Neural Networks is used to reduce the dependence on many training samples and make the method scalable to new plant species. The experimental results on two challenging fine-grained datasets of leaf images (i.e., PlantCLEF 2015 and LeafSnap) have shown the proposed method's effectiveness, which achieved recognition accuracy of 0.87 and 0.96, respectively.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/Y9JNDWU4/AraÃºjo et al. - 2022 - Two-view fine-grained classification of plant spec.pdf}
}

@article{armstrong_photographic_2019,
  title = {Photographic Identification and Citizen Science Combine to Reveal Long Distance Movements of Individual Reef Manta Rays {{Mobula}} Alfredi along {{Australia}}'s East Coast},
  author = {Armstrong, Asia O. and Armstrong, Amelia J. and Bennett, Michael B. and Richardson, Anthony J. and Townsend, Kathy A. and Dudgeon, Christine L.},
  year = {2019},
  month = dec,
  journal = {Marine Biodiversity Records},
  volume = {12},
  number = {1},
  pages = {14},
  issn = {1755-2672},
  doi = {10.1186/s41200-019-0173-6},
  urldate = {2022-04-19},
  abstract = {Research into the movement ecology of terrestrial and marine animals is growing globally, especially for threatened species. Understanding how far an animal can move and the extent of its range can inform conservation planning and management. On the east coast of Australia, reef manta rays Mobula alfredi are the subject of a photographic identification study, Project Manta. In June 2018, videos of reef manta rays from the SS Yongala (19.31\textdegree{} S, 147.62\textdegree{} E), were submitted to the Project Manta east coast sightings database. The videos were of two individuals previously identified from North Stradbroke Island (27.42\textdegree{} S, 153.55\textdegree{} E), about 1150 km to the south of the SS Yongala. This represents the greatest point-to-point distance travelled by individual M. afredi and extends the latitudinal range for this sub-population on the east coast. This study highlights that citizen science input can provide valuable data to address knowledge gaps in the distribution and population range of marine species. Knowledge of the 1000+ km range movement potential of individual M. alfredi, highlights the possibility that regional sub-populations may span jurisdictional zones of more countries than previously considered likely, complicating conservation management of this species.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/CKFSHD32/Armstrong et al. - 2019 - Photographic identification and citizen science co.pdf}
}

@article{arnbom_individual_1987,
  title = {Individual Identification of Sperm Whales},
  author = {Arnbom, Tom},
  year = {1987},
  journal = {Report of the International Whaling Commission},
  volume = {37},
  number = {20},
  pages = {201--204}
}

@article{arso_civil_changing_2019,
  title = {Changing Distribution of the East Coast of {{Scotland}} Bottlenose Dolphin Population and the Challenges of Area-based Management},
  author = {Arso Civil, M{\`o}nica and Quick, Nicola J. and Cheney, Barbara and Pirotta, Enrico and Thompson, Paul M. and Hammond, Philip S.},
  year = {2019},
  month = sep,
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {29},
  number = {S1},
  pages = {178--196},
  issn = {1052-7613, 1099-0755},
  doi = {10.1002/aqc.3102},
  urldate = {2022-07-11},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/TRPGVNJF/Arso Civil et al. - 2019 - Changing distribution of the east coast of Scotlan.pdf}
}

@article{atapour-abarghouei_kings_2019,
  title = {A {{Kings Ransom}} for {{Encryption}}: {{Ransomware Classification}} Using {{Augmented One-Shot Learning}} and {{Bayesian Approximation}}},
  shorttitle = {A {{Kings Ransom}} for {{Encryption}}},
  author = {{Atapour-Abarghouei}, Amir and Bonner, Stephen and McGough, Andrew Stephen},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.06750 [cs]},
  eprint = {1908.06750},
  primaryclass = {cs},
  urldate = {2020-10-05},
  abstract = {Newly emerging variants of ransomware pose an ever-growing threat to computer systems governing every aspect of modern life through the handling and analysis of big data. While various recent security-based approaches have focused on detecting and classifying ransomware at the network or system level, easy-to-use post-infection ransomware classification for the lay user has not been attempted before. In this paper, we investigate the possibility of classifying the ransomware a system is infected with simply based on a screenshot of the splash screen or the ransom note captured using a consumer camera commonly found in any modern mobile device. To train and evaluate our system, we create a sample dataset of the splash screens of 50 well-known ransomware variants. In our dataset, only a single training image is available per ransomware. Instead of creating a large training dataset of ransomware screenshots, we simulate screenshot capture conditions via carefully designed data augmentation techniques, enabling simple and efficient oneshot learning. Moreover, using model uncertainty obtained via Bayesian approximation, we ensure special input cases such as unrelated non-ransomware images and previously-unseen ransomware variants are correctly identified for special handling and not mis-classified. Extensive experimental evaluation demonstrates the efficacy of our work, with accuracy levels of up to 93.6\% for ransomware classification.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/5SY5REE3/Atapour-Abarghouei et al. - 2019 - A Kings Ransom for Encryption Ransomware Classifi.pdf}
}

@article{badrinarayanan_segnet:_2015,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.00561 [cs]},
  eprint = {1511.00561},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/b3020111/Zotero/storage/QI9XLE8U/Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf}
}

@article{baird_population_2009,
  title = {Population Structure of Island-Associated Dolphins: {{Evidence}} from Photo-Identification of Common Bottlenose Dolphins ({{Tursiops}} Truncatus) in the Main {{Hawaiian Islands}}},
  shorttitle = {Population Structure of Island-Associated Dolphins},
  author = {Baird, Robin W. and Gorgone, Antoinette M. and McSweeney, Daniel J. and Ligon, Allan D. and Deakos, Mark H. and Webster, Daniel L. and Schorr, Gregory S. and Martien, Karen K. and Salden, Dan R. and Mahaffy, Sabre D.},
  year = {2009},
  journal = {Marine Mammal Science},
  volume = {25},
  number = {2},
  pages = {251--274},
  issn = {1748-7692},
  doi = {10.1111/j.1748-7692.2008.00257.x},
  urldate = {2021-01-07},
  abstract = {Management agencies often use geopolitical boundaries as proxies for biological boundaries. In Hawaiian waters a single stock is recognized of common bottlenose dolphins, Tursiops truncatus, a species that is found both in open water and near-shore among the main Hawaiian Islands. To assess population structure, we photo-identified 336 distinctive individuals from the main Hawaiian Islands, from 2000 to 2006. Their generally shallow-water distribution, and numerous within-year and between-year resightings within island areas suggest that individuals are resident to the islands, rather than part of an offshore population moving through the area. Comparisons of identifications obtained from Kaua`i/Ni`ihau, O`ahu, the ``4-island area,'' and the island of Hawai`i showed no evidence of movements among these island groups, although movements from Kaua`i to Ni`ihau and among the ``4-islands'' were documented. A Bayesian analysis examining the probability of missing movements among island groups, given our sample sizes for different areas, indicates that interisland movement rates are less than 1\% per year with 95\% probability. Our results suggest the existence of multiple demographically independent populations of island-associated common bottlenose dolphins around the main Hawaiian islands.},
  copyright = {No claim to original US government works},
  langid = {english},
  keywords = {bottlenose dolphin,Hawai`i,movements,population structure,Tursiops truncatus},
  file = {/Users/b3020111/Zotero/storage/EVWU9G54/Baird et al. - 2009 - Population structure of island-associated dolphins.pdf;/Users/b3020111/Zotero/storage/VLZA9FUN/j.1748-7692.2008.00257.html}
}

@inproceedings{bandara_transformer-based_2022,
  title = {A {{Transformer-Based Siamese Network}} for {{Change Detection}}},
  booktitle = {{{IGARSS}} 2022 - 2022 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Bandara, Wele Gedara Chaminda and Patel, Vishal M.},
  year = {2022},
  month = jul,
  pages = {207--210},
  issn = {2153-7003},
  doi = {10.1109/IGARSS46834.2022.9883686},
  abstract = {This paper presents a transformer-based Siamese network architecture (abbreviated by ChangeFormer) for Change Detection (CD) from a pair of co-registered remote sensing images. Different from recent CD frameworks, which are based on fully convolutional networks (ConvNets), the proposed method unifies hierarchically structured transformer encoder with Multi-Layer Perception (MLP) decoder in a Siamese network architecture to efficiently render multi-scale long-range details required for accurate CD. Experiments on two CD datasets show that the proposed end-to-end trainable ChangeFormer architecture achieves better CD performance than previous counterparts. Our code and pre-trained models are available at github.com/wgcban/ChangeFormer.},
  keywords = {attention mechanism,Change detection,Convolutional codes,Decoding,multilayer perceptron,Network architecture,remote sensing,Remote sensing,Sensors,transformer Siamese network,Transformers},
  file = {/Users/b3020111/Zotero/storage/4EUYGNV9/Bandara and Patel - 2022 - A Transformer-Based Siamese Network for Change Det.pdf;/Users/b3020111/Zotero/storage/BVZPIHXA/stamp.html}
}

@article{barrowclift_social_2017,
  title = {Social, Economic and Trade Characteristics of the Elasmobranch Fishery on {{Unguja Island}}, {{Zanzibar}}, {{East Africa}}},
  author = {Barrowclift, E. and Temple, A.J. and Stead, S. and Jiddawi, N.S. and Berggren, P.},
  year = {2017},
  month = sep,
  journal = {Marine Policy},
  volume = {83},
  pages = {128--136},
  issn = {0308597X},
  doi = {10.1016/j.marpol.2017.06.002},
  urldate = {2022-04-13},
  abstract = {Understanding the socio-economic drivers underpinning fishers' decisions to target elasmobranchs is considered vital in determining sustainable management objectives for these species, yet limited empirical data is collected. This study presents an overview of elasmobranch catch, trade and socio-economic characteristics of Zanzibar's small-scale, artisanal fishery. The value of applying this information to future elasmobranch fisheries policy is demonstrated. In August 2015, interviews were conducted with fishers (n = 39) and merchants (n = 16) at two landing sites, Kizimkazi-Dimbani and Mkokotoni, along with the main market site in Stone Town. Additionally, elasmobranch catches were recorded across the same locations between June and August 2015. Elasmobranchs were listed as target species by 49\% of fishers interviewed. Whilst most fishers (n = 30) stated that 76\textendash 100\% of their household income came from fishing, there was variation in how elasmobranch catch and trade contributed. One-third of fishers (n = 36) that caught and sold elasmobranchs reported that 41\textendash 60\% of their income came from elasmobranch catch. However, for some fishers (n = 8) elasmobranch catch represented 0\textendash 20\% of their income, whilst for others (n = 4) it represented 81\textendash 100\%. Differences in fisheries income and elasmobranch price could be attributed to several interacting factors including season, weather, fishing effort, fishing gear, target catch and consumer demand. Further, elasmobranch price was influenced by size and species. The study revealed information on catch, trade, markets and socio-economy that is important for future research, conservation and management of elasmobranchs and fisheries in Zanzibar. The methods utilised have potential for broader application to understudied, artisanal elasmobranch fisheries in the western Indian Ocean.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ZZW8I3RZ/Barrowclift et al. - 2017 - Social, economic and trade characteristics of the .pdf}
}

@article{battle_siamese_2022,
  title = {Siamese {{Neural Networks}} for {{Skin Cancer Classification}} and {{New Class Detection}} Using {{Clinical}} and {{Dermoscopic Image Datasets}}},
  author = {Battle, Michael Luke and {Atapour-Abarghouei}, Amir and McGough, Andrew Stephen},
  year = {2022},
  month = dec,
  journal = {arXiv:2212.06130 [cs.CV]},
  eprint = {2212.06130},
  primaryclass = {cs.CV},
  urldate = {2023-01-06},
  abstract = {Skin cancer is the most common malignancy in the world. Automated skin cancer detection would significantly improve early detection rates and prevent deaths. To help with this aim, a number of datasets have been released which can be used to train Deep Learning systems \textendash{} these have produced impressive results for classification. However, this only works for the classes they are trained on whilst they are incapable of identifying skin lesions from previously unseen classes, making them unconducive for clinical use. We could look to massively increase the datasets by including all possible skin lesions, though this would always leave out some classes. Instead, we evaluate Siamese Neural Networks (SNNs), which not only allows us to classify images of skin lesions, but also allow us to identify those images which are different from the trained classes \textendash{} allowing us to determine that an image is not an example of our training classes. We evaluate SNNs on both dermoscopic and clinical images of skin lesions. We obtain top-1 classification accuracy levels of 74.33\% and 85.61\% on clinical and dermoscopic datasets, respectively. Although this is slightly lower than the state-of-the-art results, the SNN approach has the advantage that it can detect out-of-class examples. Our results highlight the potential of an SNN approach as well as pathways towards future clinical deployment.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/ZRKCYBWD/Battle et al. - 2022 - Siamese Neural Networks for Skin Cancer Classifica.pdf}
}

@article{bay_speeded-up_2008,
  title = {Speeded-{{Up Robust Features}} ({{SURF}})},
  author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
  year = {2008},
  month = jun,
  journal = {Computer Vision and Image Understanding},
  series = {Similarity {{Matching}} in {{Computer Vision}} and {{Multimedia}}},
  volume = {110},
  number = {3},
  pages = {346--359},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2007.09.014},
  urldate = {2021-01-08},
  abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.},
  langid = {english},
  keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
  file = {/Users/b3020111/Zotero/storage/KRZM89EF/Bay et al. - 2008 - Speeded-Up Robust Features (SURF).pdf;/Users/b3020111/Zotero/storage/S66J48T8/S1077314207001555.html}
}

@article{bedetti_system_2020,
  title = {System for {{Elephant Ear-pattern Knowledge}} ({{SEEK}}) to Identify Individual {{African}} Elephants},
  author = {Bedetti, Anka and Greyling, Cathy and Paul, Barry and Blondeau, Jennifer and Clark, Amy and Malin, Hannah and Horne, Jackie and Makukule, Ronny and Wilmot, Jessica and Eggeling, Tammy and Kern, Julie and Henley, Michelle},
  year = {2020},
  month = oct,
  journal = {Pachyderm},
  volume = {61},
  pages = {63--77},
  issn = {1026-2881},
  urldate = {2021-07-21},
  copyright = {Copyright (c) 2020 Pachyderm},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/VBWNXJKC/Bedetti et al. - 2020 - System for Elephant Ear-pattern Knowledge (SEEK) t.pdf}
}

@article{beery_can_2021,
  title = {Can Poachers Find Animals from Public Camera Trap Images?},
  author = {Beery, Sara and Bondi, Elizabeth},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.11236 [cs, q-bio]},
  eprint = {2106.11236},
  primaryclass = {cs, q-bio},
  urldate = {2022-05-03},
  abstract = {To protect the location of camera trap data containing sensitive, high-target species, many ecologists randomly obfuscate the latitude and longitude of the camera when publishing their data. For example, they may publish a random location within a 1km radius of the true camera location for each camera in their network. In this paper, we investigate the robustness of geo-obfuscation for maintaining camera trap location privacy, and show via a case study that a few simple, intuitive heuristics and publicly available satellite rasters can be used to reduce the area likely to contain the camera by 87\% (assuming random obfuscation within 1km), demonstrating that geo-obfuscation may be less effective than previously believed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Populations and Evolution},
  file = {/Users/b3020111/Zotero/storage/M2YZVK9R/Beery and Bondi - 2021 - Can poachers find animals from public camera trap .pdf;/Users/b3020111/Zotero/storage/6PYLQZS4/2106.html}
}

@article{beery_efficient_2019,
  title = {Efficient {{Pipeline}} for {{Camera Trap Image Review}}},
  author = {Beery, Sara and Morris, Dan and Yang, Siyu},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.06772 [cs]},
  eprint = {1907.06772},
  primaryclass = {cs},
  urldate = {2021-01-05},
  abstract = {Biologists all over the world use camera traps to monitor biodiversity and wildlife population density. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but it has proven difficult to to apply models trained in one region to images collected in different geographic areas. In some cases, accuracy falls off catastrophically in new region, due to both changes in background and the presence of previously-unseen species. We propose a pipeline that takes advantage of a pre-trained general animal detector and a smaller set of labeled images to train a classification model that can efficiently achieve accurate results in a new region.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/UDDFUMLZ/Beery et al. - 2019 - Efficient Pipeline for Camera Trap Image Review.pdf;/Users/b3020111/Zotero/storage/S7NSR9IC/1907.html}
}

@article{beery_iwildcam_2019,
  title = {The {{iWildCam}} 2019 {{Challenge Dataset}}},
  author = {Beery, Sara and Morris, Dan and Perona, Pietro},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.07617 [cs]},
  eprint = {1907.07617},
  primaryclass = {cs},
  urldate = {2020-05-15},
  abstract = {Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to different areas we are faced with an interesting problem: how do you classify a species in a new region that you may not have seen in previous training data? In order to tackle this problem, we have prepared a dataset and challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. We use the Caltech Camera Traps dataset, collected from the American Southwest, as training data. We add a new dataset from the American Northwest, curated from data provided by the Idaho Department of Fish and Game (IDFG), as our test dataset. The test data has some class overlap with the training data, some species are found in both datasets, but there are both species seen during training that are not seen during test and vice versa. To help fill the gaps in the training species, we allow competitors to utilize transfer learning from two alternate domains: human-curated images from iNaturalist and synthetic images from Microsoft's TrapCam-AirSim simulation environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/K4HQZZHJ/Beery et al. - 2019 - The iWildCam 2019 Challenge Dataset.pdf;/Users/b3020111/Zotero/storage/9DHRMU4D/1907.html}
}

@article{belhumeur_localizing_2013,
  title = {Localizing Parts of Faces Using a Consensus of Exemplars},
  author = {Belhumeur, Peter N. and Jacobs, David W. and Kriegman, David J. and Kumar, Neeraj},
  year = {2013},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  number = {12},
  pages = {2930--2940}
}

@article{belko_feathers_2020,
  title = {Feathers Dataset for {{Fine-Grained Visual Categorization}}},
  author = {Belko, Alina and Dobratulin, Konstantin and Kuznetsov, Andrey},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08606 [cs]},
  eprint = {2004.08606},
  primaryclass = {cs},
  urldate = {2021-04-21},
  abstract = {This paper introduces a novel dataset FeatherV1, containing 28,272 images of feathers categorized by 595 bird species. It was created to perform taxonomic identification of bird species by a single feather, which can be applied in amateur and professional ornithology. FeatherV1 is the first publicly available bird's plumage dataset for machine learning, and it can raise interest for a new task in fine-grained visual recognition domain. The latest version of the dataset can be downloaded at https://github.com/feathers-dataset/feathersv1-dataset. We also present feathers classification task results. We selected several deep learning architectures (DenseNet based) for categorical crossentropy values comparison on the provided dataset.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/RXZEH767/Belko et al. - 2020 - Feathers dataset for Fine-Grained Visual Categoriz.pdf}
}

@inproceedings{bengio_use_1994,
  title = {Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks},
  booktitle = {Proceedings of the {{First IEEE Conference}} on {{Evolutionary Computation}}. {{IEEE World Congress}} on {{Computational Intelligence}}},
  author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn},
  year = {1994},
  pages = {324--327},
  publisher = {{IEEE}},
  isbn = {0-7803-1899-4}
}

@incollection{bennour_bat_2022,
  title = {Bat {{Echolocation Call Detection}} and {{Species Recognition}} by {{Transformers}} with {{Self-attention}}},
  booktitle = {Intelligent {{Systems}} and {{Pattern Recognition}}},
  author = {Bellafkir, Hicham and Vogelbacher, Markus and Gottwald, Jannis and M{\"u}hling, Markus and Korfhage, Nikolaus and Lampe, Patrick and Frie{\ss}, Nicolas and Nauss, Thomas and Freisleben, Bernd},
  editor = {Bennour, Akram and Ensari, Tolga and Kessentini, Yousri and Eom, Sean},
  year = {2022},
  volume = {1589},
  pages = {189--203},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-08277-1_16},
  urldate = {2023-01-06},
  abstract = {Biodiversity is important for several ecosystem services that provide the existential basis for human life. The current decline in biodiversity requires a transformation from manual, periodic assessment to automatic real-time biodiversity monitoring. Bats as one of the most widespread species among terrestrial mammals serve as important bioindicators for the health of ecosystems. Typically, bats are monitored by recording and analyzing their echolocation calls. In this paper, we present a novel approach for detecting bat echolocation calls and recognizing bat species in audio spectrograms. It is based on a transformer neural network architecture and relies on self-attention. Our experiments show that our approach outperforms state-of-the-art approaches for bat echolocation call detection and species recognition on several publicly available data sets. While our bat echolocation call detection approach achieves a performance of up to 90.2\% in terms of average precision, our bat species recognition model obtains up to 88.7\% accuracy for 14 bat classes occurring in Germany, some of which are difficult to distinguish even for human experts.},
  isbn = {978-3-031-08276-4 978-3-031-08277-1},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/JS4WKLX6/Bellafkir et al. - 2022 - Bat Echolocation Call Detection and Species Recogn.pdf}
}

@article{berger-wolf_wildbook:_2017,
  title = {Wildbook: {{Crowdsourcing}}, Computer Vision, and Data Science for Conservation},
  shorttitle = {Wildbook},
  author = {{Berger-Wolf}, Tanya Y. and Rubenstein, Daniel I. and Stewart, Charles V. and Holmberg, Jason A. and Parham, Jason and Menon, Sreejith and Crall, Jonathan and Van Oast, Jon and Kiciman, Emre and Joppa, Lucas},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.08880 [cs]},
  eprint = {1710.08880},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Photographs, taken by field scientists, tourists, automated cameras, and incidental photographers, are the most abundant source of data on wildlife today. Wildbook is an autonomous computational system that starts from massive collections of images and, by detecting various species of animals and identifying individuals, combined with sophisticated data management, turns them into high resolution information database, enabling scientific inquiry, conservation, and citizen science. We have built Wildbooks for whales (flukebook.org), sharks (whaleshark.org), two species of zebras (Grevy's and plains), and several others. In January 2016, Wildbook enabled the first ever full species (the endangered Grevy's zebra) census using photographs taken by ordinary citizens in Kenya. The resulting numbers are now the official species census used by IUCN Red List: http://www.iucnredlist.org/details/7950/0. In 2016, Wildbook partnered up with WWF to build Wildbook for Sea Turtles, Internet of Turtles (IoT), as well as systems for seals and lynx. Most recently, we have demonstrated that we can now use publicly available social media images to count and track wild animals. In this paper we present and discuss both the impact and challenges that the use of crowdsourced images can have on wildlife conservation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/b3020111/Zotero/storage/7XVBYQTL/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf;/Users/b3020111/Zotero/storage/SNQUXM4C/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf;/Users/b3020111/Zotero/storage/GTMS4JR9/1710.html;/Users/b3020111/Zotero/storage/UG5YRIIF/1710.html}
}

@article{berggren_sustainable_2007,
  title = {Sustainable {{Dolphin Tourism}} in {{East Africa}}},
  author = {Berggren, P. and Amir, O. A. and Guissamulo, A. and Jiddawi, N. S. and Ngazy, Z. and Stensland, E. and S{\"a}rnblad, A. and Cockroft, V. G.},
  year = {2007},
  journal = {WIOMSA Book Series},
  publisher = {{Newcastle University}},
  urldate = {2021-10-10},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/EJUB74WL/Berggren et al. - 2007 - Sustainable Dolphin Tourism in East Africa.pdf;/Users/b3020111/Zotero/storage/9G4XDKHZ/154924.html}
}

@article{bergstra_algorithms_2011,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  year = {2011},
  journal = {Advances in neural information processing systems},
  volume = {24},
  pages = {9},
  abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/TBVGRIXP/Bergstra et al. - Algorithms for Hyper-Parameter Optimization.pdf}
}

@article{bessesen_lacaziosis-like_2014,
  title = {Lacaziosis-like Disease among Bottlenose Dolphins {{Tursiops}} Truncatus Photographed in {{Golfo Dulce}}, {{Costa Rica}}},
  author = {Bessesen, Bl and Oviedo, L and Burdett Hart, L and {Herra-Miranda}, D and {Pacheco-Polanco}, Jd and Baker, L and {Sabor{\'i}o-Rodriguez}, G and {Berm{\'u}dez-Villapol}, L and {Acevedo-Guti{\'e}rrez}, A},
  year = {2014},
  month = jan,
  journal = {Diseases of Aquatic Organisms},
  volume = {107},
  number = {3},
  pages = {173--180},
  issn = {0177-5103, 1616-1580},
  doi = {10.3354/dao02692},
  urldate = {2022-04-12},
  abstract = {Lacaziosis (also known as lobomycosis) is a chronic dermal disease caused by the fungal agent Lacazia loboi, which affects both humans and dolphins. Photographic data have been used to identify lacaziosis-like disease (LLD) among dolphins in the waters of North and South America, and here we report LLD in bottlenose dolphins Tursiops truncatus off the coast of Costa Rica, the first reporting in Central American waters. During the periods of 1991 to 1992 and 2010 to 2011, 3 research teams conducted separate dolphin surveys in the Pacific tropical fiord Golfo Dulce, and each documented skin lesions in the resident population of bottlenose dolphins. Photo-ID records were used to identify LLD-affected bottlenose dolphins and to assess their lesions. Findings showed between 13.2 and 16.1\% of the identified dolphins exhibited lesions grossly resembling lacaziosis. By combining efforts and cross-referencing photographic data, the teams explored the presence of LLD in Golfo Dulce over a time gap of approximately 20 yr. Our findings expand the geographical range of the disease and offer insight into its longevity within a given population of dolphins.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/C2W2B9QC/Bessesen et al. - 2014 - Lacaziosis-like disease among bottlenose dolphins .pdf}
}

@article{bevan_measuring_2018,
  title = {Measuring Behavioral Responses of Sea Turtles, Saltwater Crocodiles, and Crested Terns to Drone Disturbance to Define Ethical Operating Thresholds},
  author = {Bevan, Elizabeth and Whiting, Scott and Tucker, Tony and Guinea, Michael and Raith, Andrew and Douglas, Ryan},
  year = {2018},
  month = mar,
  journal = {PLOS ONE},
  volume = {13},
  number = {3},
  pages = {e0194460},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0194460},
  urldate = {2021-01-05},
  abstract = {Drones are being increasingly used in innovative ways to enhance environmental research and conservation. Despite their widespread use for wildlife studies, there are few scientifically justified guidelines that provide minimum distances at which wildlife can be approached to minimize visual and auditory disturbance. These distances are essential to ensure that behavioral and survey data have no observer bias and form the basis of requirements for animal ethics and scientific permit approvals. In the present study, we documented the behaviors of three species of sea turtle (green turtles, Chelonia mydas, flatback turtles, Natator depressus, hawksbill turtles, Eretmochelys imbricata), saltwater crocodiles (Crocodylus porosus), and crested terns (Thalasseus bergii) in response to a small commercially available (1.4 kg) multirotor drone flown in Northern Territory and Western Australia. Sea turtles in nearshore waters off nesting beaches or in foraging habitats exhibited no evasive behaviors (e.g. rapid diving) in response to the drone at or above 20\textendash 30 m altitude, and at or above 10 m altitude for juvenile green and hawksbill turtles foraging on shallow, algae-covered reefs. Adult female flatback sea turtles were not deterred by drones flying forward or stationary at 10 m altitude when crawling up the beach to nest or digging a body pit or egg chamber. In contrast, flyovers elicited a range of behaviors from crocodiles, including minor, lateral head movements, fleeing, or complete submergence when a drone was present below 50 m altitude. Similarly, a colony of crested terns resting on a sand-bank displayed disturbance behaviors (e.g. flight response) when a drone was flown below 60 m altitude. The current study demonstrates a variety of behavioral disturbance thresholds for diverse species and should be considered when establishing operating conditions for drones in behavioral and conservation studies.},
  langid = {english},
  keywords = {Animal behavior,Animal flight,Bird flight,Crocodiles,Habitats,Reefs,Sea water,Turtles},
  file = {/Users/b3020111/Zotero/storage/ZMAIBCR2/Bevan et al. - 2018 - Measuring behavioral responses of sea turtles, sal.pdf;/Users/b3020111/Zotero/storage/TSJN9SWF/article.html}
}

@article{biederman_subordinate-level_1999,
  title = {Subordinate-Level Object Classification Reexamined},
  author = {Biederman, Irving and Subramaniam, Suresh and Bar, Moshe and Kalocsai, Peter and Fiser, J{\'o}zsef},
  year = {1999},
  month = jul,
  journal = {Psychological Research},
  volume = {62},
  number = {2},
  pages = {131--153},
  issn = {1430-2772},
  doi = {10.1007/s004260050047},
  urldate = {2023-03-26},
  abstract = {The classification of a table as round rather than square, a car as a Mazda rather than a Ford, a drill bit as 3/8-inch rather than 1/4-inch, and a face as Tom have all been regarded as a single process termed ``subordinate classification.'' Despite the common label, the considerable heterogeneity of the perceptual processing required to achieve such classifications requires, minimally, a more detailed taxonomy. Perceptual information relevant to subordinate-level shape classifications can be presumed to vary on continua of (a) the type of distinctive information that is present, nonaccidental or metric, (b) the size of the relevant contours or surfaces, and (c) the similarity of the to-be-discriminated features, such as whether a straight contour has to be distinguished from a contour of low curvature versus high curvature. We consider three, relatively pure cases. Case 1 subordinates may be distinguished by a representation, a geon structural description (GSD), specifying a nonaccidental characterization of an object's large parts and the relations among these parts, such as a round table versus a square table. Case 2 subordinates are also distinguished by GSDs, except that the distinctive GSDs are present at a small scale in a complex object so the location and mapping of the GSDs are contingent on an initial basic-level classification, such as when we use a logo to distinguish various makes of cars. Expertise for Cases 1 and 2 can be easily achieved through specification, often verbal, of the GSDs. Case 3 subordinates, which have furnished much of the grist for theorizing with ``view-based'' template models, require fine metric discriminations. Cases 1 and 2 account for the overwhelming majority of shape-based basic- and subordinate-level object classifications that people can and do make in their everyday lives. These classifications are typically made quickly, accurately, and with only modest costs of viewpoint changes. Whereas the activation of an array of multiscale, multiorientation filters, presumed to be at the initial stage of all shape processing, may suffice for determining the similarity of the representations mediating recognition among Case 3 subordinate stimuli (and faces), Cases 1 and 2 require that the output of these filters be mapped to classifiers that make explicit the nonaccidental properties, parts, and relations specified by the GSDs.},
  langid = {english},
  keywords = {Complex Object,Considerable Heterogeneity,Perceptual Information,Round Table,Template Model}
}

@misc{bielski_adambielskisiamese-triplet_2018,
  title = {Adambielski/Siamese-Triplet: {{Siamese}} and Triplet Networks with Online Pair/Triplet Mining in {{PyTorch}}},
  author = {Bielski, Ada},
  year = {2018},
  journal = {GitHub},
  urldate = {2022-07-08},
  howpublished = {https://github.com/adambielski/siamese-triplet}
}

@article{bigg_assessment_1982,
  title = {An {{Assessment}} of {{Killer Whale}} ({{Orcinus}} Orca) {{Stocks}} off  {{Vancouver Island}}, {{British Columbia}}},
  author = {Bigg, Michael},
  year = {1982},
  journal = {Report of the International Whaling Commission},
  volume = {32},
  number = {65},
  pages = {12},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/IE2ACJW9/Bigg - An Assessment of Killer Whale (Orcinus orca) Stock.pdf}
}

@article{birenbaum_sealnet_2022,
  title = {{{SEALNET}}: {{Facial}} Recognition Software for Ecological Studies of Harbor Seals},
  shorttitle = {{{SEALNET}}},
  author = {Birenbaum, Zach and Do, Hieu and Horstmyer, Lauren and Orff, Hailey and Ingram, Krista and Ay, Ahmet},
  year = {2022},
  month = may,
  journal = {Ecology and Evolution},
  volume = {12},
  number = {5},
  issn = {2045-7758, 2045-7758},
  doi = {10.1002/ece3.8851},
  urldate = {2022-07-01},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/Y694RKPB/Birenbaum et al. - 2022 - SEALNET Facial recognition software for ecologica.pdf}
}

@misc{bloom_injury_1994,
  title = {The Injury and Subsequent Healing of a Serious Propeller Strike to a Wild Bottlenose Dolphin ({{Tursiops}} Truncatus) Resident in Cold Waters off the {{Northumberland}} Coast of {{England}}},
  author = {Bloom, P. and Jager, M.},
  year = {1994},
  journal = {undefined},
  urldate = {2021-01-12},
  abstract = {A solitary male bottlenose dolphin was resident close to the fishing port of Amble from April 1987 (Bloom 1991). This animal remained in a very small triangular home range of less than 0.5 km and was rarely seen to leave that established territory. The home range was contained within Alnmouth Bay between Cocquet Island and associated shoals and the mainland. This shallow protected bay area provided good protection from gales and storms from most directions. The boundaries, as defined by the dolphin\&\#39;s observed movements, are the Pan Bush shoals and its navigational buoy to the north\- east, the sewer outfall buoy to the south and the harbour mouth area on the westward apex of the triangle (see Figure 1). The river Coquet is a fine salmon and seatrout river. Since these fish, by instinct, have to pass through the narrow harbour entrance to gain access to and egress from, the river estuary, they provided a regular supply of large fish that were therefore concentrated into a predictable and small forage area. This was perhaps the primary reason for this animal\&\#39;s prolonged residency in the Amble area. It was the dolphin\&\#39;s increasing interest in commercial fishing and recreational boat traffic, and later with swimmers, that brought this animal into intimate contact with people and their activities.},
  howpublished = {/paper/The-injury-and-subsequent-healing-of-a-serious-to-a-Bloom-Jager/6bee7aff1e0eaa9c79319d10fb13ae5532396edd},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/I86UUG4N/6bee7aff1e0eaa9c79319d10fb13ae5532396edd.html}
}

@article{bloom_injury_nodate,
  title = {The Injury and Subsequent Healing of a Serious Propeller Strike to a Wild Bottlenose Dolphin ({{Tursiops}} Truncatus) Resident in Cold Waters off the {{Northumberland}} Coast of {{England}}},
  author = {Bloom, Peter and Jager, Martina},
  pages = {6},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/4IN9XC5P/Bloom and Jager - The injury and subsequent healing of a serious pro.pdf}
}

@article{bochkovskiy_yolov4_2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.10934 [cs, eess]},
  eprint = {2004.10934},
  primaryclass = {cs, eess},
  urldate = {2021-01-13},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {$\sim$}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/b3020111/Zotero/storage/YCCN3PEP/Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf}
}

@misc{bochkovskiy_yolov4_2020-1,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  year = {2020},
  month = apr,
  number = {arXiv:2004.10934},
  eprint = {2004.10934},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {$\sim$}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/b3020111/Zotero/storage/N32XB4HD/Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf}
}

@article{bogucki_applying_2019,
  title = {Applying Deep Learning to Right Whale Photo Identification},
  author = {Bogucki, Robert and Cygan, Marek and Khan, Christin Brangwynne and Klimek, Maciej and Milczek, Jan Kanty and Mucha, Marcin},
  year = {2019},
  journal = {Conservation Biology},
  volume = {33},
  number = {3},
  pages = {676--684},
  issn = {1523-1739},
  doi = {10.1111/cobi.13226},
  urldate = {2021-01-05},
  abstract = {Photo identification is an important tool for estimating abundance and monitoring population trends over time. However, manually matching photographs to known individuals is time-consuming. Motivated by recent developments in image recognition, we hosted a data science challenge on the crowdsourcing platform Kaggle to automate the identification of endangered North Atlantic right whales (Eubalaena glacialis). The winning solution automatically identified individual whales with 87\% accuracy with a series of convolutional neural networks to identify the region of interest on an image, rotate, crop, and create standardized photographs of uniform size and orientation and then identify the correct individual whale from these passport-like photographs. Recent advances in deep learning coupled with this fully automated workflow have yielded impressive results and have the potential to revolutionize traditional methods for the collection of data on the abundance and distribution of wild populations. Presenting these results to a broad audience should further bridge the gap between the data science and conservation science communities.},
  copyright = {\textcopyright{} 2018 The Authors. Conservation Biology published by Wiley Periodicals, Inc. on behalf of Society for Conservation Biology.},
  langid = {english},
  keywords = {algorithm,algoritmo,aprendizaje autom\'atico,automated image recognition,competencia Kaggle,computer vision,convolutional neural networks,identificaci\'on fotogr\'afica,Kaggle competition,Kaggle ç½ç«ç«èµ,machine learning,photo identification,reconocimiento automatizado de im\'agenes,redes neurales convolucionales,visi\'on computarizada,å·ç§¯ç¥ç»ç½ç»,æºå¨å­¦ä¹ ,ç§çè¯å«,ç®æ³,èªå¨å¾åè¯å«,è®¡ç®æºè§è§},
  file = {/Users/b3020111/Zotero/storage/QVLG8KHQ/Bogucki et al. - 2019 - Applying deep learning to right whale photo identi.pdf;/Users/b3020111/Zotero/storage/D3IRYY2C/cobi.html}
}

@article{bolya_yolact_2019,
  title = {{{YOLACT}}: {{Real-time Instance Segmentation}}},
  shorttitle = {{{YOLACT}}},
  author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
  year = {2019},
  month = oct,
  journal = {arXiv:1904.02689 [cs.CV]},
  eprint = {1904.02689},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {We present a simple, fully-convolutional model for realtime instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/UQ6ECQEV/Bolya et al. - 2019 - YOLACT Real-time Instance Segmentation.pdf}
}

@inproceedings{bottou_large-scale_2010,
  title = {Large-{{Scale Machine Learning}} with {{Stochastic Gradient Descent}}},
  booktitle = {Proceedings of {{COMPSTAT}}'2010},
  author = {Bottou, L{\'e}on},
  editor = {Lechevallier, Yves and Saporta, Gilbert},
  year = {2010},
  pages = {177--186},
  publisher = {{Physica-Verlag HD}},
  abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
  isbn = {978-3-7908-2604-3},
  langid = {english},
  keywords = {efficiency,online learning,stochastic gradient descent},
  file = {/Users/b3020111/Zotero/storage/7ITG9H83/Bottou - 2010 - Large-Scale Machine Learning with Stochastic Gradi.pdf}
}

@incollection{bottou_tradeoffs_2008,
  title = {The {{Tradeoffs}} of {{Large Scale Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  author = {Bottou, L{\'e}on and Bousquet, Olivier},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  year = {2008},
  pages = {161--168},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-08-08},
  file = {/Users/b3020111/Zotero/storage/TUEX7IP8/Bottou and Bousquet - 2008 - The Tradeoffs of Large Scale Learning.pdf;/Users/b3020111/Zotero/storage/XLK98IPG/3323-the-tradeoffs-of-large-scale-learning.html}
}

@article{boulais_seamapd21_2021,
  title = {{{SEAMAPD21}}: A Large-Scale Reef Fish Dataset for Fine-Grained Categorization},
  author = {Boulais, Oceane and Alaba, Simegnew Yihunie and Ball, John E and Campbell, Matthew and Iftekhar, Ahmed Tashfin and Moorehead, Robert and Primrose, James},
  year = {2021},
  journal = {Proceedings of the FGVC8: The Eight Workshop on Fine-Grained Visual Categorization},
  pages = {7},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/KTDFF7SU/Boulais et al. - SEAMAPD21 a large-scale reef ï¬sh dataset for ï¬ne-.pdf}
}

@article{boulent_scaling_2023,
  title = {Scaling Whale Monitoring Using Deep Learning: {{A}} Human-in-the-Loop Solution for Analyzing Aerial Datasets},
  shorttitle = {Scaling Whale Monitoring Using Deep Learning},
  author = {Boulent, Justine and Charry, Bertrand and Kennedy, Malcolm McHugh and Tissier, Emily and Fan, Raina and Marcoux, Marianne and Watt, Cortney A. and {Gagn{\'e}-Turcotte}, Antoine},
  year = {2023},
  month = mar,
  journal = {Frontiers in Marine Science},
  volume = {10},
  pages = {1099479},
  issn = {2296-7745},
  doi = {10.3389/fmars.2023.1099479},
  urldate = {2023-03-12},
  abstract = {To ensure effective cetacean management and conservation policies, it is necessary to collect and rigorously analyze data about these populations. Remote sensing allows the acquisition of images over large observation areas, but due to the lack of reliable automatic analysis techniques, biologists usually analyze all images by hand. In this paper, we propose a human-in-the-loop approach to couple the power of deep learning-based automation with the expertise of biologists to develop a reliable artificial intelligence assisted annotation tool for cetacean monitoring. We tested this approach to analyze a dataset of 5334 aerial images acquired in 2017 by Fisheries and Oceans Canada to monitor belugas (               Delphinapterus leucas               ) from the threatened Cumberland Sound population in Clearwater Fjord, Canada. First, we used a test subset of photographs to compare predictions obtained by the fine-tuned model to manual annotations made by three Observers, expert marine mammal biologists. With only 100 annotated images for training, the model obtained between 90\% and 91.4\% mutual agreement with the three Observers, exceeding the minimum inter-observer agreement of 88.6\% obtained between the experts themselves. Second, this model was applied to the full dataset. The predictions were then verified by an Observer and compared to annotations made completely manually and independently by another Observer. The annotating Observer and the human-in-the-loop pipeline detected 4051 belugas in common, out of a total of 4572 detections for the Observer and 4298 for our pipeline. This experiment shows that the proposed human-in-the-loop approach is suitable for processing novel aerial datasets for beluga counting and can be used to scale cetacean monitoring. It also highlights that human observers, even experienced ones, have varied detection bias, underlining the need to discuss standardization of annotation protocols.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/4CESR3NZ/Boulent et al. - 2023 - Scaling whale monitoring using deep learning A hu.pdf}
}

@inproceedings{bouma_individual_2018,
  title = {Individual {{Common Dolphin Identification Via Metric Embedding Learning}}},
  booktitle = {2018 {{International Conference}} on {{Image}} and {{Vision Computing New Zealand}} ({{IVCNZ}})},
  author = {Bouma, Soren and Pawley, Matthew D.M and Hupman, Krista and Gilman, Andrew},
  year = {2018},
  month = nov,
  pages = {1--6},
  issn = {2151-2191},
  doi = {10.1109/IVCNZ.2018.8634778},
  abstract = {Photo-identification (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin fin photograph captured in the field to a catalogue of known individuals. We examine this problem in the context of open-set recognition and utilise a triplet loss function to learn a compact representation of fin images in a Euclidean embedding, where the Euclidean distance metric represents fin similarity. We show that this compact representation can be successfully learnt from a fairly small (in deep learning context) training set and still generalise well to out-of-sample identities (completely new dolphin individuals), with top-1 and top-5 test set (37 individuals) accuracy of 90.5 {$\pm$} 2 and 93.6 {$\pm$} 1 percent. In the presence of 1200 distractors, top-1 accuracy dropped by 12\%; however, top-5 accuracy saw only a 2.8\% drop.},
  keywords = {compact representation,deep learning context,dolphin identification via metric embedding learning,dolphin individuals,Dolphins,ecological sciences,Euclidean distance metric,Euclidean embedding,fin similarity,image capture,image matching,image recognition,image registration,image segmentation,known individuals,laborious manual process,learning (artificial intelligence),Measurement,open-set recognition,photo-identification,pose estimation,social structure,Sociology,Statistics,Task analysis,top-5 test set accuracy,Training,Training data,triplet loss function},
  file = {/Users/b3020111/Zotero/storage/3YYHZJAM/Bouma et al. - 2018 - Individual Common Dolphin Identification Via Metri.pdf;/Users/b3020111/Zotero/storage/3WWIAWWK/8634778.html}
}

@inproceedings{bourdev_poselets:_2009,
  title = {Poselets: {{Body}} Part Detectors Trained Using 3d Human Pose Annotations},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Bourdev, Lubomir and Malik, Jitendra},
  year = {2009},
  pages = {1365--1372},
  publisher = {{IEEE}},
  isbn = {1-4244-4420-9}
}

@article{boureau_theoretical_2010,
  title = {A {{Theoretical Analysis}} of {{Feature Pooling}} in {{Visual Recognition}}},
  author = {Boureau, Y-Lan and Ponce, Jean and LeCun, Yann},
  year = {2010},
  journal = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages = {8},
  abstract = {Many modern visual recognition algorithms incorporate a step of spatial `pooling', where the outputs of several nearby feature detectors are combined into a local or global `bag of features', in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly influence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/BKAPV48R/Boureau et al. - A Theoretical Analysis of Feature Pooling in Visua.pdf}
}

@article{breuel_effects_2015,
  title = {The {{Effects}} of {{Hyperparameters}} on {{SGD Training}} of {{Neural Networks}}},
  author = {Breuel, Thomas M.},
  year = {2015},
  month = aug,
  journal = {arXiv:1508.02788 [cs]},
  eprint = {1508.02788},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {The performance of neural network classifiers is determined by a number of hyperparameters, including learning rate, batch size, and depth. A number of attempts have been made to explore these parameters in the literature, and at times, to develop methods for optimizing them. However, exploration of parameter spaces has often been limited. In this note, I report the results of large scale experiments exploring these different parameters and their interactions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,K.3.2},
  file = {/Users/b3020111/Zotero/storage/3R9PU3QP/Breuel - 2015 - The Effects of Hyperparameters on SGD Training of .pdf;/Users/b3020111/Zotero/storage/XIN5IH2B/1508.html}
}

@article{brookes_triple-stream_2023,
  title = {Triple-Stream {{Deep Metric Learning}} of {{Great Ape Behavioural Actions}}},
  author = {Brookes, Otto and Mirmehdi, Majid and K{\"u}hl, Hjalmar and Burghardt, Tilo},
  year = {2023},
  month = jan,
  journal = {arXiv:2301.02642 [cs.CV]},
  eprint = {2301.02642},
  primaryclass = {cs.CV},
  urldate = {2023-01-10},
  abstract = {We propose the first metric learning system for the recognition of great ape behavioural actions. Our proposed triple stream embedding architecture works on camera trap videos taken directly in the wild and demonstrates that the utilisation of an explicit DensePose-C chimpanzee body part segmentation stream effectively complements traditional RGB appearance and optical flow streams. We evaluate system variants with different feature fusion techniques and long-tail recognition approaches. Results and ablations show performance improvements of \textasciitilde 12\% in top-1 accuracy over previous results achieved on the PanAf-500 dataset containing 180,000 manually annotated frames across nine behavioural actions. Furthermore, we provide a qualitative analysis of our findings and augment the metric learning system with long-tail recognition techniques showing that average per class accuracy -- critical in the domain -- can be improved by \textasciitilde 23\% compared to the literature on that dataset. Finally, since our embedding spaces are constructed as metric, we provide first data-driven visualisations of the great ape behavioural action spaces revealing emerging geometry and topology. We hope that the work sparks further interest in this vital application area of computer vision for the benefit of endangered great apes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/MUYKUXNN/Brookes et al. - 2023 - Triple-stream Deep Metric Learning of Great Ape Be.pdf;/Users/b3020111/Zotero/storage/B7DLSNYB/2301.html}
}

@misc{brown_language_2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-09-27},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \textendash{} something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/b3020111/Zotero/storage/RKB5TFVT/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@inproceedings{brust_towards_2017,
  title = {Towards {{Automated Visual Monitoring}} of {{Individual Gorillas}} in the {{Wild}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Brust, Clemens-Alexander and Burghardt, Tilo and Groenenberg, Milou and Kading, Christoph and K{\"u}hl, Hjalmar S. and Manguette, Marie L. and Denzler, Joachim},
  year = {2017},
  month = oct,
  pages = {2820--2830},
  issn = {2473-9944},
  doi = {10.1109/ICCVW.2017.333},
  abstract = {In this paper we report on the context and evaluation of a system for an automatic interpretation of sightings of individual western lowland gorillas (Gorilla gorilla gorilla) as captured in facial field photography in the wild. This effort aligns with a growing need for effective and integrated monitoring approaches for assessing the status of biodiversity at high spatio-temporal scales. Manual field photography and the utilisation of autonomous camera traps have already transformed the way ecological surveys are conducted. In principle, many environments can now be monitored continuously, and with a higher spatio-temporal resolution than ever before. Yet, the manual effort required to process photographic data to derive relevant information delimits any large scale application of this methodology. The described system applies existing computer vision techniques including deep convolutional neural networks to cover the tasks of detection and localisation, as well as individual identification of gorillas in a practically relevant setup. We evaluate the approach on a relatively large and challenging data corpus of 12,765 field images of 147 individual gorillas with image-level labels (i.e. missing bounding boxes) photographed at Mbeli Bai at the Nouabal-Ndoki National Park, Republic of Congo. Results indicate a facial detection rate of 90.8\% AP and an individual identification accuracy for ranking within the Top 5 set of 80.3\%. We conclude that, whilst keeping the human in the loop is critical, this result is practically relevant as it exemplifies model transferability and has the potential to assist manual identification efforts. We argue further that there is significant need towards integrating computer vision deeper into ecological sampling methodologies and field practice to move the discipline forward and open up new research horizons.},
  keywords = {Biodiversity,Biological system modeling,Cameras,Monitoring,Sociology,Statistics},
  file = {/Users/b3020111/Zotero/storage/XLX6HIHT/Brust et al. - 2017 - Towards Automated Visual Monitoring of Individual .pdf;/Users/b3020111/Zotero/storage/W58W3ZKZ/8265544.html}
}

@article{buhrmester_amazons_2011,
  title = {Amazon's {{Mechanical Turk}}: {{A}} New Source of Inexpensive, yet High-Quality, Data?},
  author = {Buhrmester, Michael and Kwang, Tracy and Gosling, Samuel D},
  year = {2011},
  journal = {Perspectives on psychological science},
  volume = {6},
  number = {1},
  pages = {3--5},
  issn = {1745-6916}
}

@inproceedings{burges_learning_2005,
  title = {Learning to Rank Using Gradient Descent},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
  year = {2005},
  pages = {89--96},
  publisher = {{ACM Press}},
  address = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102363},
  urldate = {2023-04-21},
  isbn = {978-1-59593-180-1},
  langid = {english}
}

@inproceedings{buric_ball_2018,
  title = {Ball {{Detection Using Yolo}} and {{Mask R-CNN}}},
  booktitle = {2018 {{International Conference}} on {{Computational Science}} and {{Computational Intelligence}} ({{CSCI}})},
  author = {Buric, M. and Pobar, M. and {Ivasic-Kos}, M.},
  year = {2018},
  month = dec,
  pages = {319--323},
  doi = {10.1109/CSCI46756.2018.00068},
  abstract = {Many computer vision applications rely on accurate and fast object detection, and in our case, ball detection serves as a prerequisite for action recognition in handball scenes. We compare the performance of two of the state-of-the-art convolutional neural network-based object detectors for the task of ball detection in non-staged, real-world conditions. The comparison is performed in terms of speed and accuracy measures on a dataset comprising custom handball footage and a sample of images obtained from the Internet. The performance of the models is compared with and without additional training with examples from our dataset.},
  keywords = {action recognition,ball detection,Computer architecture,computer vision,computer vision applications,convolutional neural nets,convolutional neural network-based object detectors,custom handball footage,dataset,Detectors,Feature extraction,handball scenes,Image segmentation,Internet,object detection,Object detection,object recognition,R-CNN,Task analysis,Training},
  file = {/Users/b3020111/Zotero/storage/TKZ2ISV5/8947818.html}
}

@article{burke_deblending_2019,
  title = {Deblending and Classifying Astronomical Sources with {{Mask R-CNN}} Deep Learning},
  author = {Burke, Colin J. and Aleo, Patrick D. and Chen, Yu-Ching and Liu, Xin and Peterson, John R. and Sembroski, Glenn H. and Lin, Joshua Yao-Yu},
  year = {2019},
  month = dec,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {490},
  number = {3},
  pages = {3952--3965},
  publisher = {{Oxford Academic}},
  issn = {0035-8711},
  doi = {10.1093/mnras/stz2845},
  urldate = {2020-12-03},
  abstract = {ABSTRACT. We apply a new deep learning technique to detect, classify, and deblend sources in multiband astronomical images. We train and evaluate the performanc},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/UK34TRF7/Burke et al. - 2019 - Deblending and classifying astronomical sources wi.pdf;/Users/b3020111/Zotero/storage/J6DQJE6W/5585422.html}
}

@inproceedings{cai_cascade_2018,
  title = {Cascade {{R-CNN}}: {{Delving Into High Quality Object Detection}}},
  shorttitle = {Cascade {{R-CNN}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cai, Zhaowei and Vasconcelos, Nuno},
  year = {2018},
  month = jun,
  pages = {6154--6162},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00644},
  urldate = {2023-04-27},
  abstract = {In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/XP7LMB32/Cai and Vasconcelos - 2018 - Cascade R-CNN Delving Into High Quality Object De.pdf}
}

@article{caldwell_evidence_1955,
  title = {Evidence of {{Home Range}} of an {{Atlantic Bottlenose Dolphin}}},
  author = {Caldwell, David K.},
  year = {1955},
  month = may,
  journal = {Journal of Mammalogy},
  volume = {36},
  number = {2},
  pages = {304--305},
  issn = {0022-2372},
  doi = {10.2307/1375913},
  urldate = {2021-01-07},
  abstract = {An Atlantic bottlenose dolphin, Tursiops truncatus (Montague), with a damaged dorsal fin was observed by the author on November 1, 1953, at approximately 0900 at Cedar Key, Florida. The healed injury, the absence of the upper two-thirds of the fin, served as a natural "tag" and permitted ready identification of this individual, even in the brief period of a normal breathing roll. This dolphin was again seen in mid-afternoon on January 24, 1954, and a third time at 1430 on March 20,1954. The first observation was made in the main ship channel between Atsena Otie and Grassy Keys, approximately 1600 yards SSW of the main pier. On the second observation, the dolphin was again in the main ship channel about 200 yards east of the main pier, and approximately 600 yards WSW of the pier in the same channel on the third sighting. Joining these reference points (Mohr, Amer. Midi. Nat., 37: 233, 1947), an area of some 475,000 square yards is yielded as an estimated minimum home range for this animal at Cedar Key. Further observations would undoubtedly affect this calculation, and may show that the animal is actually restricted to the channel at Cedar Key and does not include some of the shallower flats in its home range, as would be indicated by the theoretical area encompassed by this approximation. On the other hand, it cannot of course be concluded from the small sample that this area actually corresponds to the full home range of this individual.},
  file = {/Users/b3020111/Zotero/storage/W756QMF3/960828.html}
}

@incollection{carneiro_visual_2019,
  title = {Visual {{Siamese Clustering}} for {{Cosmetic Product Recommendation}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ACCV}} 2018 {{Workshops}}},
  author = {Holder, Christopher J. and Obara, Boguslaw and Ricketts, Stephen},
  editor = {Carneiro, Gustavo and You, Shaodi},
  year = {2019},
  volume = {11367},
  pages = {510--522},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-21074-8_40},
  urldate = {2023-03-16},
  abstract = {We investigate the problem of a visual similarity-based recommender system, where cosmetic products are recommended based on the preferences of people who share similarity of visual features. In this work we train a Siamese convolutional neural network, using our own dataset of cropped eye regions from images of 91 female subjects, such that it learns to output feature vectors that place images of the same subject close together in high-dimensional space. We evaluate the trained network based on its ability to correctly identify existing subjects from unseen images, and then assess its capability to find visually similar matches amongst the existing subjects when an image of a new subject is input.},
  isbn = {978-3-030-21073-1 978-3-030-21074-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/FZR9HL2J/Holder et al. - 2019 - Visual Siamese Clustering for Cosmetic Product Rec.pdf}
}

@article{centelleghe_use_2020,
  title = {The Use of {{Unmanned Aerial Vehicles}} ({{UAVs}}) to Sample the Blow Microbiome of Small Cetaceans},
  author = {Centelleghe, Cinzia and Carraro, Lisa and Gonzalvo, Joan and Rosso, Massimiliano and Esposti, Erika and Gili, Claudia and Bonato, Marco and Pedrotti, Davide and Cardazzo, Barbara and Povinelli, Michele and Mazzariol, Sandro},
  year = {2020},
  month = jul,
  journal = {PLOS ONE},
  volume = {15},
  number = {7},
  pages = {e0235537},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0235537},
  urldate = {2021-01-05},
  abstract = {Recent studies describe the use of UAVs in collecting blow samples from large whales to analyze the microbial and viral community in exhaled air. Unfortunately, attempts to collect blow from small cetaceans have not been successful due to their swimming and diving behavior. In order to overcome these limitations, in this study we investigated the application of a specific sampling tool attached to a UAV to analyze the blow from small cetaceans and their respiratory microbiome. Preliminary trials to set up the sampling tool were conducted on a group of 6 bottlenose dolphins (Tursiops truncatus) under human care, housed at Acquario di Genova, with approximately 1 meter distance between the blowing animal and the tool to obtain suitable samples. The same sampling kit, suspended via a 2 meter rope assembled on a waterproof UAV, flying 3 meters above the animals, was used to sample the blows of 5 wild bottlenose dolphins in the Gulf of Ambracia (Greece) and a sperm whale (Physeter macrocephalus) in the southern Tyrrhenian Sea (Italy), to investigate whether this experimental assembly also works for large whale sampling. In order to distinguish between blow-associated microbes and seawater microbes, we pooled 5 seawater samples from the same area where blow samples' collection were carried out. The the respiratory microbiota was assessed by using the V3-V4 region of the 16S rRNA gene via Illumina Amplicon Sequencing. The pooled water samples contained more bacterial taxa than the blow samples of both wild animals and the sequenced dolphin maintained under human care. The composition of the bacterial community differed between the water samples and between the blow samples of wild cetaceans and that under human care, but these differences may have been mediated by different microbial communities between seawater and aquarium water. The sperm whale's respiratory microbiome was more similar to the results obtained from wild bottlenose dolphins. Although the number of samples used in this study was limited and sampling and analyses were impaired by several limitations, the results are rather encouraging, as shown by the evident microbial differences between seawater and blow samples, confirmed also by the meta-analysis carried out comparing our results with those obtained in previous studies. Collecting exhaled air from small cetaceans using drones is a challenging process, both logistically and technically. The success in obtaining samples from small cetacean blow in this study in comparison to previous studies is likely due to the distance the sampling kit is suspended from the drone, which reduced the likelihood that the turbulence of the drone propeller interfered with successfully sampling blow, suggested as a factor leading to poor success in previous studies.},
  langid = {english},
  keywords = {Bacteria,Dolphins,Humpback whales,Microbiome,Sea water,Sperm whales,Swimming,Wildlife},
  file = {/Users/b3020111/Zotero/storage/YQ5Z2TM3/Centelleghe et al. - 2020 - The use of Unmanned Aerial Vehicles (UAVs) to samp.pdf}
}

@misc{cheeseman_happywhale_2019,
  title = {Happywhale {{Presentation}} - {{World Marine Mammal Conference}} 2019 - {{Rise}} of the {{Machines Workshop}}},
  author = {Cheeseman, Ted},
  year = {2019},
  journal = {Google Docs},
  urldate = {2020-02-12},
  howpublished = {drive.google.com/file/d/1yRtNKagANZOgpY6852zzUpZmiFXuZmLM},
  file = {/Users/b3020111/Zotero/storage/E6KJSPX9/view.html}
}

@article{chen_blendmask_2020,
  title = {{{BlendMask}}: {{Top-Down Meets Bottom-Up}} for {{Instance Segmentation}}},
  shorttitle = {{{BlendMask}}},
  author = {Chen, Hao and Sun, Kunyang and Tian, Zhi and Shen, Chunhua and Huang, Yongming and Yan, Youliang},
  year = {2020},
  month = apr,
  journal = {arXiv:2001.00309 [cs]},
  eprint = {2001.00309},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {Instance segmentation is one of the fundamental vision tasks. Recently, fully convolutional instance segmentation methods have drawn much attention as they are often simpler and more efficient than two-stage approaches like Mask R-CNN. To date, almost all such approaches fall behind the two-stage Mask R-CNN method in mask precision when models have similar computation complexity, leaving great room for improvement. In this work, we achieve improved mask prediction by effectively combining instance-level information with semantic information with lower-level fine-granularity. Our main contribution is a blender module which draws inspiration from both top-down and bottom-up instance segmentation approaches. The proposed BlendMask can effectively predict dense per-pixel position-sensitive instance features with very few channels, and learn attention maps for each instance with merely one convolution layer, thus being fast in inference. BlendMask can be easily incorporated with the state-of-the-art one-stage detection frameworks and outperforms Mask R-CNN under the same training schedule while being 20\% faster. A light-weight version of BlendMask achieves \$ 34.2\% \$ mAP at 25 FPS evaluated on a single 1080Ti GPU card. Because of its simplicity and efficacy, we hope that our BlendMask could serve as a simple yet strong baseline for a wide range of instance-wise prediction tasks. Code is available at https://git.io/AdelaiDet},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/Q958CAMQ/Chen et al. - 2020 - BlendMask Top-Down Meets Bottom-Up for Instance S.pdf;/Users/b3020111/Zotero/storage/7NFT9NBD/2001.html}
}

@article{chen_deeplab_2018,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  shorttitle = {{{DeepLab}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2018},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {4},
  pages = {834--848},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2017.2699184},
  urldate = {2023-04-27},
  file = {/Users/b3020111/Zotero/storage/BYDXJ99A/Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Con.pdf}
}

@misc{chen_encoder-decoder_2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2018},
  month = aug,
  number = {arXiv:1802.02611},
  eprint = {1802.02611},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: //github.com/tensorflow/models/tree/master/research/deeplab.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/NE46IUN5/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf}
}

@inproceedings{chen_hybrid_2019,
  title = {Hybrid {{Task Cascade}} for {{Instance Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Kai and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua and Pang, Jiangmiao and Wang, Jiaqi and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Shi, Jianping},
  year = {2019},
  month = jun,
  pages = {4969--4978},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00511},
  urldate = {2023-04-27},
  abstract = {Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4\% and 1.5\% improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at: https://github.com/ open-mmlab/mmdetection.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/MZHWJGUT/Chen et al. - 2019 - Hybrid Task Cascade for Instance Segmentation.pdf}
}

@misc{chen_rethinking_2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  month = dec,
  number = {arXiv:1706.05587},
  eprint = {1706.05587},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/HB22YMG4/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf;/Users/b3020111/Zotero/storage/AA48BFVA/1706.html}
}

@article{chen_semantic_2014,
  title = {Semantic {{Image Segmentation}} with {{Deep Convolutional Nets}} and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.7062 [cs]},
  eprint = {1412.7062},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ''semantic image segmentation''). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our ``DeepLab'' system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/b3020111/Zotero/storage/E4EEQ3ZR/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf}
}

@article{cheney_integrating_2013,
  title = {Integrating Multiple Data Sources to Assess the Distribution and Abundance of Bottlenose Dolphins {{Tursiops}} Truncatus in {{Scottish}} Waters},
  author = {Cheney, Barbara and Thompson, Paul M. and Ingram, Simon N. and Hammond, Philip S. and Stevick, Peter T. and Durban, John W. and Culloch, Ross M. and Elwen, Simon H. and Mandleberg, Laura and Janik, Vincent M. and Quick, Nicola J. and ISLAS-Villanueva, Valentina and Robinson, Kevin P. and Costa, Marina and Eisfeld, Sonja M. and Walters, Alice and Phillips, Charlie and Weir, Caroline R. and Evans, Peter G. H. and Anderwald, Pia and Reid, Robert J. and Reid, James B. and Wilson, Ben},
  year = {2013},
  journal = {Mammal Review},
  volume = {43},
  number = {1},
  pages = {71--88},
  issn = {1365-2907},
  doi = {10.1111/j.1365-2907.2011.00208.x},
  urldate = {2021-01-07},
  abstract = {The distribution, movements and abundance of highly mobile marine species such as bottlenose dolphins Tursiops truncatus are best studied at large spatial scales, but previous research effort has generally been focused on relatively small areas, occupied by populations with high site fidelity. We aimed to characterize the distribution, movements and abundance of bottlenose dolphins around the coasts of Scotland, exploring how data from multiple sources could be integrated to build a broader-scale picture of their ecology. We reviewed existing historical data, integrated data from ongoing studies and developed new collaborative studies to describe distribution patterns. We adopted a Bayesian multi-site mark-recapture model to estimate abundance of bottlenose dolphins throughout Scottish coastal waters and quantified movements of individuals between study areas. The majority of sightings of bottlenose dolphins around the Scottish coastline are concentrated on the east and west coasts, but records are rare before the 1990s. Dedicated photo-identification studies in 2006 and 2007 were used to estimate the size of two resident populations: one on the east coast from the Moray Firth to Fife, population estimate 195 [95\% highest posterior density intervals (HPDI): 162\textendash 253] and the second in the Hebrides, population estimate 45 (95\% HPDI: 33\textendash 66). Interaction parameters demonstrated that the dolphins off the east coast of Scotland are highly mobile, whereas those off the west coast form two discrete communities. We provide the first comprehensive assessment of the abundance of bottlenose dolphins in the inshore waters of Scotland. The combination of dedicated photo-identification studies and opportunistic sightings suggest that a relatively small number of bottlenose dolphins (200\textendash 300 individuals) occur regularly in Scottish coastal waters. On both east and west coasts, re-sightings of identifiable individuals indicate that the animals have been using these coastal areas since studies began.},
  langid = {english},
  keywords = {cetacean,citizen science,coastal zone management,renewable energy,spatial ecology},
  file = {/Users/b3020111/Zotero/storage/TI3YU2DX/Cheney et al. - 2013 - Integrating multiple data sources to assess the di.pdf;/Users/b3020111/Zotero/storage/456HGXNA/j.1365-2907.2011.00208.html}
}

@article{cheney_long-term_2014,
  title = {Long-Term Trends in the Use of a Protected Area by Small Cetaceans in Relation to Changes in Population Status},
  author = {Cheney, Barbara and Corkrey, Ross and Durban, John W. and Grellier, Kate and Hammond, Philip S. and {Islas-Villanueva}, Valentina and Janik, Vincent M. and Lusseau, Susan M. and Parsons, Kim M. and Quick, Nicola J. and Wilson, Ben and Thompson, Paul M.},
  year = {2014},
  month = dec,
  journal = {Global Ecology and Conservation},
  volume = {2},
  pages = {118--128},
  issn = {23519894},
  doi = {10.1016/j.gecco.2014.08.010},
  urldate = {2022-07-11},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/XKJR5SM4/Cheney et al. - 2014 - Long-term trends in the use of a protected area by.pdf}
}

@article{cheney_long-term_2014-1,
  title = {Long-Term Trends in the Use of a Protected Area by Small Cetaceans in Relation to Changes in Population Status},
  author = {Cheney, Barbara and Corkrey, Ross and Durban, John W. and Grellier, Kate and Hammond, Philip S. and {Islas-Villanueva}, Valentina and Janik, Vincent M. and Lusseau, Susan M. and Parsons, Kim M. and Quick, Nicola J. and Wilson, Ben and Thompson, Paul M.},
  year = {2014},
  month = dec,
  journal = {Global Ecology and Conservation},
  volume = {2},
  pages = {118--128},
  issn = {2351-9894},
  doi = {10.1016/j.gecco.2014.08.010},
  urldate = {2021-01-07},
  abstract = {The requirement to monitor listed species in European designated sites is challenging for long-lived mobile species that only temporarily occupy protected areas. We use a 21 year time series of bottlenose dolphin photo-identification data to assess trends in abundance and conservation status within a Special Area of Conservation (SAC) in Scotland. Mark\textendash recapture methods were used to estimate annual abundance within the SAC from 1990 to 2010. A Bayesian mark\textendash recapture model with a state-space approach was used to estimate overall population trends using data collected across the populations' range. Despite inter-annual variability in the number of dolphins within the SAC, there was a {$>$}99\% probability that the wider population was stable or increasing. Results indicate that use of the SAC by the wider population has declined. This is the first evidence of long-term trends in the use of an EU protected area by small cetaceans in relation to changes in overall population status. Our results highlight the importance of adapting the survey protocols used in long-term photo-identification studies to maintain high capture probabilities and minimise sampling heterogeneity. Crucially, these data demonstrate the value of collecting data from the wider population to assess the success of protected areas designated for mobile predators.},
  langid = {english},
  keywords = {Abundance,Bayesian,Bottlenose dolphin,Mark\textendash recapture,Photo-identification,Special Area of Conservation},
  file = {/Users/b3020111/Zotero/storage/4B9SGZQW/Cheney et al. - 2014 - Long-term trends in the use of a protected area by.pdf;/Users/b3020111/Zotero/storage/BC5FW83C/S2351989414000250.html}
}

@article{cheng_mean_1995,
  title = {Mean Shift, Mode Seeking, and Clustering},
  author = {Cheng, Yizong},
  year = {1995},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {17},
  number = {8},
  pages = {790--799},
  issn = {1939-3539},
  doi = {10.1109/34.400568},
  abstract = {Mean shift, a simple interactive procedure that shifts each data point to the average of data points in its neighborhood is generalized and analyzed in the paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on the surface constructed with a "shadow" kernal. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis if treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.{$<>$}},
  keywords = {Algorithm design and analysis,Clustering algorithms,Computer science,Convergence,Iterative algorithms,Kernel,Surface treatment},
  file = {/Users/b3020111/Zotero/storage/RA8LWFJB/Cheng - 1995 - Mean shift, mode seeking, and clustering.pdf;/Users/b3020111/Zotero/storage/PMJZKI2E/stamp.html}
}

@article{chiao_detection_2019,
  title = {Detection and Classification the Breast Tumors Using Mask {{R-CNN}} on Sonograms},
  author = {Chiao, Jui-Ying and Chen, Kuan-Yung and Liao, Ken Ying-Kai and Hsieh, Po-Hsin and Zhang, Geoffrey and Huang, Tzung-Chi},
  year = {2019},
  month = may,
  journal = {Medicine},
  volume = {98},
  number = {19},
  issn = {0025-7974},
  doi = {10.1097/MD.0000000000015200},
  urldate = {2020-12-03},
  abstract = {Breast cancer is one of the most harmful diseases for women with the highest morbidity. An efficient way to decrease its mortality is to diagnose cancer earlier by screening. Clinically, the best approach of screening for Asian women is ultrasound images combined with biopsies. However, biopsy is invasive and it gets incomprehensive information of the lesion. The aim of this study is to build a model for automatic detection, segmentation, and classification of breast lesions with ultrasound images. Based on deep learning, a technique using Mask regions with convolutional neural network was developed for lesion detection and differentiation between benign and malignant. The mean average precision was 0.75 for the detection and segmentation. The overall accuracy of benign/malignant classification was 85\%. The proposed method provides a comprehensive and noninvasive way to detect and classify breast lesions.},
  pmcid = {PMC6531264},
  pmid = {31083152},
  file = {/Users/b3020111/Zotero/storage/TWDL3543/Chiao et al. - 2019 - Detection and classification the breast tumors usi.pdf}
}

@article{choromanska_loss_2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  year = {2015},
  month = jan,
  journal = {arXiv:1412.0233 [cs]},
  eprint = {1412.0233},
  primaryclass = {cs},
  urldate = {2021-06-14},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/WCFQGNHN/Choromanska et al. - 2015 - The Loss Surfaces of Multilayer Networks.pdf}
}

@article{christiansen_effects_2010,
  title = {Effects of Tourist Boats on the Behaviour of {{Indo-Pacific}} Bottlenose Dolphins off the South Coast of {{Zanzibar}}},
  author = {Christiansen, F and Lusseau, D and Stensland, E and Berggren, P},
  year = {2010},
  month = mar,
  journal = {Endangered Species Research},
  volume = {11},
  pages = {91--99},
  issn = {1863-5407, 1613-4796},
  doi = {10.3354/esr00265},
  urldate = {2022-05-04},
  abstract = {The short-term effects of tourist boats on the behaviour of Indo-Pacific bottlenose dolphins Tursiops aduncus were investigated off the south coast of Zanzibar, Tanzania, by comparing dolphin group behaviour in the presence (impact) and absence (control) of tourist boats. Groupfollows were conducted from a carefully maneuvered (non-invasive) independent research vessel and behavioural data on group activity were collected using scan sampling methods. By using a timediscrete Markov chain model, the transition probabilities of passing/changing from one behavioural state to another were calculated and compared between impact and control situations. The data were further used to construct behavioural budgets. In the presence of tourist boats, dolphins were less likely to stay in a resting or socialising activity but were more likely to start travelling or foraging, as inferred from the Markov chain model. The behavioral budgets showed that foraging, resting and socialising all decreased as an effect of tourist boat presence, while travelling increased. The behavioural responses are likely to have energetic implications, mainly by increasing physical demands. Further, the results demonstrate that the current level of tourism intensity off the south coast of Zanzibar affects the dolphins' cumulative behavioural budget. Regulations on dolphin tourism are therefore urgently needed to minimise potential long-term negative effects on the dolphins.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/JWATVIHG/Christiansen et al. - 2010 - Effects of tourist boats on the behaviour of Indo-.pdf}
}

@article{chu_deepapple_2020,
  title = {{{DeepApple}}: {{Deep Learning-based Apple Detection}} Using a {{Suppression Mask R-CNN}}},
  shorttitle = {{{DeepApple}}},
  author = {Chu, Pengyu and Li, Zhaojian and Lammers, Kyle and Lu, Renfu and Liu, Xiaoming},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.09870 [cs]},
  eprint = {2010.09870},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {Robotic apple harvesting has received much research attention in the past few years due to growing shortage and rising cost in labor. One key enabling technology towards automated harvesting is accurate and robust apple detection, which poses great challenges as a result of the complex orchard environment that involves varying lighting conditions and foliage/branch occlusions. This letter reports on the development of a novel deep learning-based apple detection framework named DeepApple. Specifically, we first collect a comprehensive apple orchard dataset for 'Gala' and 'Blondee' apples, using a color camera, under different lighting conditions (sunny vs. overcast and front lighting vs. back lighting). We then develop a novel suppression Mask R-CNN for apple detection, in which a suppression branch is added to the standard Mask R-CNN to suppress non-apple features generated by the original network. Comprehensive evaluations are performed, which show that the developed suppression Mask R-CNN network outperforms state-of-the-art models with a higher F1-score of 0.905 and a detection time of 0.25 second per frame on a standard desktop computer.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/DC3A75JF/Chu et al. - 2020 - DeepApple Deep Learning-based Apple Detection usi.pdf;/Users/b3020111/Zotero/storage/7Z6MLTUA/2010.html}
}

@article{clapham_automated_2020,
  title = {Automated Facial Recognition for Wildlife That Lack Unique Markings: {{A}} Deep Learning Approach for Brown Bears},
  shorttitle = {Automated Facial Recognition for Wildlife That Lack Unique Markings},
  author = {Clapham, Melanie and Miller, Ed and Nguyen, Mary and Darimont, Chris T.},
  year = {2020},
  journal = {Ecology and Evolution},
  volume = {10},
  number = {23},
  issn = {2045-7758},
  doi = {10.1002/ece3.6840},
  urldate = {2020-11-09},
  abstract = {Emerging technologies support a new era of applied wildlife research, generating data on scales from individuals to populations. Computer vision methods can process large datasets generated through image-based techniques by automating the detection and identification of species and individuals. With the exception of primates, however, there are no objective visual methods of individual identification for species that lack unique and consistent body markings. We apply deep learning approaches of facial recognition using object detection, landmark detection, a similarity comparison network, and an support vector machine-based classifier to identify individuals in a representative species, the brown bear Ursus arctos. Our open-source application, BearID, detects a bear's face in an image, rotates and extracts the face, creates an ``embedding'' for the face, and uses the embedding to classify the individual. We trained and tested the application using labeled images of 132 known individuals collected from British Columbia, Canada, and Alaska, USA. Based on 4,674 images, with an 80/20\% split for training and testing, respectively, we achieved a facial detection (ability to find a face) average precision of 0.98 and an individual classification (ability to identify the individual) accuracy of 83.9\%. BearID and its annotated source code provide a replicable methodology for applying deep learning methods of facial recognition applicable to many other species that lack distinguishing markings. Further analyses of performance should focus on the influence of certain parameters on recognition accuracy, such as age and body size. Combining BearID with camera trapping could facilitate fine-scale behavioral research such as individual spatiotemporal activity patterns, and a cost-effective method of population monitoring through mark\textendash recapture studies, with implications for species and landscape conservation and management. Applications to practical conservation include identifying problem individuals in human\textendash wildlife conflicts, and evaluating the intrapopulation variation in efficacy of conservation strategies, such as wildlife crossings.},
  copyright = {\textcopyright{} 2020 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {deep learning,face recognition,grizzly bear,individual ID,machine learning,wildlife monitoring},
  file = {/Users/b3020111/Zotero/storage/3RIUER68/Clapham et al. - Automated facial recognition for wildlife that lac.pdf;/Users/b3020111/Zotero/storage/KJLUNNCP/ece3.html}
}

@inproceedings{cobb_context-aware_2022,
  title = {Context-{{Aware Drift Detection}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Cobb, Oliver and Looveren, Arnaud Van},
  year = {2022},
  volume = {162},
  pages = {25},
  publisher = {{PMLR}},
  address = {{Baltimore, Maryland, USA}},
  abstract = {When monitoring machine learning systems, twosample tests of homogeneity form the foundation upon which existing approaches to drift detection build. They are used to test for evidence that the distribution underlying recent deployment data differs from that underlying the historical reference data. Often, however, various factors such as time-induced correlation mean that batches of recent deployment data are not expected to form an i.i.d. sample from the historical data distribution. Instead we may wish to test for differences in the distributions conditional on context that is permitted to change. To facilitate this we borrow machinery from the causal inference domain to develop a more general drift detection framework built upon a foundation of two-sample tests for conditional distributional treatment effects. We recommend a particular instantiation of the framework based on maximum conditional mean discrepancies. We then provide an empirical study demonstrating its effectiveness for various drift detection problems of practical interest, such as detecting drift in the distributions underlying subpopulations of data in a manner that is insensitive to their respective prevalences. The study additionally demonstrates applicability to ImageNet-scale vision problems.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/HHCI52B5/Cobb and Looveren - Context-Aware Drift Detection.pdf}
}

@article{cohn_improving_1994,
  title = {Improving Generalization with Active Learning},
  author = {Cohn, David and Atlas, Les and Ladner, Richard},
  year = {1994},
  month = may,
  journal = {Machine Learning},
  volume = {15},
  number = {2},
  pages = {201--221},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993277},
  urldate = {2022-09-26},
  abstract = {Activelearningdiffersfrom "learningfromexamples"in that the learningalgorithmassumesat least somecontroloverwhatpart of the inputdomainit receivesinformationabout.In somesituations,activelearning is provablymore powerfulthan learningfrom examplesalone, givingbetter generalizationfor a fixed number of training examples.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/XSC7Q7SK/Cohn et al. - 1994 - Improving generalization with active learning.pdf}
}

@misc{cole_label_2022,
  title = {On {{Label Granularity}} and {{Object Localization}}},
  author = {Cole, Elijah and Wilber, Kimberly and Van Horn, Grant and Yang, Xuan and Fornoni, Marco and Perona, Pietro and Belongie, Serge and Howard, Andrew and Mac Aodha, Oisin},
  year = {2022},
  month = jul,
  number = {arXiv:2207.10225},
  eprint = {2207.10225},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-16},
  abstract = {Weakly supervised object localization (WSOL) aims to learn representations that encode object location using only image-level category labels. However, many objects can be labeled at different levels of granularity. Is it an animal, a bird, or a great horned owl? Which image-level labels should we use? In this paper we study the role of label granularity in WSOL. To facilitate this investigation we introduce iNatLoc500, a new large-scale fine-grained benchmark dataset for WSOL. Surprisingly, we find that choosing the right training label granularity provides a much larger performance boost than choosing the best WSOL algorithm. We also show that changing the label granularity can significantly improve data efficiency.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/I98P6TH2/Cole et al. - 2022 - On Label Granularity and Object Localization.pdf}
}

@misc{cole_when_2022,
  title = {When {{Does Contrastive Visual Representation Learning Work}}?},
  author = {Cole, Elijah and Yang, Xuan and Wilber, Kimberly and Mac Aodha, Oisin and Belongie, Serge},
  year = {2022},
  month = apr,
  number = {arXiv:2105.05837},
  eprint = {2105.05837},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-16},
  abstract = {Recent self-supervised representation learning techniques have largely closed the gap between supervised and unsupervised learning on ImageNet classification. While the particulars of pretraining on ImageNet are now relatively well understood, the field still lacks widely accepted best practices for replicating this success on other datasets. As a first step in this direction, we study contrastive selfsupervised learning on four diverse large-scale datasets. By looking through the lenses of data quantity, data domain, data quality, and task granularity, we provide new insights into the necessary conditions for successful selfsupervised learning. Our key findings include observations such as: (i) the benefit of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representations, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learning on fine-grained visual classification tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/ZAUGRFSK/Cole et al. - 2022 - When Does Contrastive Visual Representation Learni.pdf}
}

@article{connor_male_2015,
  title = {Male Dolphin Alliances in {{Shark Bay}}: Changing Perspectives in a 30-Year Study},
  shorttitle = {Male Dolphin Alliances in {{Shark Bay}}},
  author = {Connor, Richard C. and Kr{\"u}tzen, Michael},
  year = {2015},
  month = may,
  journal = {Animal Behaviour},
  volume = {103},
  pages = {223--235},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2015.02.019},
  urldate = {2019-08-07},
  abstract = {Bottlenose dolphins, Tursiops cf. aduncus, in Shark Bay, Western Australia exhibit the most complex alliances known outside of humans. Advances in our understanding of these alliances have occurred with expansions of our study area each decade. In the 1980s, we discovered that males cooperated in stable trios and pairs (first-order alliances) to herd individual oestrous females, and that two such alliances of four to six, sometimes related, individuals (second-order alliances) cooperated against other males in contests over females. The 1990s saw the discovery of a large 14-member second-order alliance whose members exhibited labile first-order alliance formation among nonrelatives. Partner preferences as well as a relationship between first-order alliance stability and consortship rate in this `super-alliance' indicated differentiated relationships. The contrast between the super-alliance and the 1980s alliances suggested two alliance tactics. An expansion of the study area in the 2000s revealed a continuum of second-order alliance sizes in an open social network and no simple relationship between second-order alliance size and alliance stability, but generalized the relationship between first-order alliance stability and consortship rate within second-order alliances. Association preferences and contests involving three second-order alliances indicated the presence of third-order alliances. Second-order alliances may persist for 20 years with stability thwarted by gradual attrition, but underlying flexibility is indicated by observations of individuals joining other alliances, including old males joining young or old second-order alliances. The dolphin research has informed us on the evolution of complex social relationships and large brain evolution in mammals and the ecology of alliance formation. Variation in odontocete brain size and the large radiation of delphinids into a range of habitats holds great promise that further effort to describe their societies will be rewarded with similar advances in our understanding of these important issues.},
  keywords = {alliance,bottlenose dolphin,social organization,social structure},
  file = {/Users/b3020111/Zotero/storage/QRX3NQY5/S0003347215000810.html}
}

@article{constantine_abundance_2012,
  title = {Abundance of Humpback Whales in {{Oceania}} Using Photo-Identification and Microsatellite Genotyping},
  author = {Constantine, Rochelle and Jackson, Jennifer A. and Steel, Debbie and Baker, C. Scott and Brooks, Lyndon and Burns, Daniel and Clapham, Phillip and Hauser, Nan and Madon, B{\'e}n{\'e}dicte and Mattila, David and Oremus, Marc and Poole, Michael and Robbins, Jooke and Thompson, Kirsten and Garrigue, Claire},
  year = {2012},
  month = may,
  journal = {Marine Ecology Progress Series},
  volume = {453},
  pages = {249--261},
  issn = {0171-8630, 1616-1599},
  doi = {10.3354/meps09613},
  urldate = {2021-01-07},
  abstract = {Estimating the abundance of long-lived, migratory animals is challenging but essential for managing populations. We provide the first abundance estimates of endangered humpback whales Megaptera novaeangliae from their breeding grounds in Oceania, South Pacific. Using fluke photo-identification (1999-2004, n = 660 individuals) and microsatellite genotypes (1999-2005, n = 840 individuals), we estimated abundance with open capture-recapture statistical models. Total Oceania abundance and trends were estimated from 4 primary and 5 secondary sampling sites across the region. Sex-specific genotype data enabled us to account for the difference in capturability of males and females, by doubling male-specific estimates of abundance derived from genotypes. Abundance estimates were congruent between primary- and secondary-region data sets, suggesting that the primary regions are representative of all Oceania. The best estimate of total abundance was 4329 whales (3345-5313) in 2005, from a sex-specific POPAN super-population model, which includes resident whales and those migrating through the surveyed areas. A doubled-male POPAN abundance estimate from 2003 (n = 2941, 95\% CI = 1648-4234) was considered the most plausible for the 4 primary survey areas and was similar to the 2003 doubled-male estimate derived from Pradel capture probabilities (n = 2952, 95\% CI = 2043-4325). Our results confirm that Oceania is the least abundant humpback whale breeding population in the southern hemisphere. Pradel models showed no significant trend in abundance, which contradicts the recovery seen in most other populations throughout the world. Thus we suggest that the whales in this area warrant continued study and management attention.},
  langid = {english},
  keywords = {Capture-recapture,Endangered species,Genotyping,Megaptera novaeangliae,South Pacific},
  file = {/Users/b3020111/Zotero/storage/8ZVJBYRQ/Constantine et al. - 2012 - Abundance of humpback whales in Oceania using phot.pdf;/Users/b3020111/Zotero/storage/ERIBKA5R/p249-261.html}
}

@article{couteaux_automatic_2019,
  title = {Automatic Knee Meniscus Tear Detection and Orientation Classification with {{Mask-RCNN}} - {{ScienceDirect}}},
  author = {Couteaux, Vincent and {Si-Mohamed}, Salim and Nempont, Olivier and Lefevre, Thierry and Popoff, Alexandre and Pizaine, Guillaume and Villain, Nicolas and Bloch, Isabelle and Cotten, Anne and Boussel, Lo{\"i}c},
  year = {2019},
  journal = {Diagnostic and Interventional Imaging},
  volume = {100},
  number = {4},
  pages = {235--242},
  urldate = {2021-01-20},
  file = {/Users/b3020111/Zotero/storage/9VMLDTMS/Couteaux et al. - 2019 - Automatic knee meniscus tear detection and orienta.pdf;/Users/b3020111/Zotero/storage/SBUX4KQ4/S2211568419300580.html}
}

@article{creager_environment_2021,
  title = {Environment {{Inference}} for {{Invariant Learning}}},
  author = {Creager, Elliot and Jacobsen, Jorn-Henrik and Zemel, Richard},
  year = {2021},
  journal = {arXiv:2010.07249 [cs.LG]},
  eprint = {2010.07249},
  primaryclass = {cs.LG},
  pages = {12},
  abstract = {Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domaininvariant. An important assumption in this area is that the training examples are partitioned into ``domains'' or ``environments''. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds and CivilComments datasets. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/LLQXP8PA/Creager et al. - Environment Inference for Invariant Learning.pdf}
}

@misc{cui_measuring_2019,
  title = {Measuring {{Dataset Granularity}}},
  author = {Cui, Yin and Gu, Zeqi and Mahajan, Dhruv and {van der Maaten}, Laurens and Belongie, Serge and Lim, Ser-Nam},
  year = {2019},
  month = dec,
  number = {arXiv:1912.10154},
  eprint = {1912.10154},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-26},
  abstract = {Despite the increasing visibility of fine-grained recognition in our field, ``fine-grained'' has thus far lacked a precise definition. In this work, building upon clustering theory, we pursue a framework for measuring dataset granularity. We argue that dataset granularity should depend not only on the data samples and their labels, but also on the distance function we choose. We propose an axiomatic framework to capture desired properties for a dataset granularity measure and provide examples of measures that satisfy these properties. We assess each measure via experiments on datasets with hierarchical labels of varying granularity. When measuring granularity in commonly used datasets with our measure, we find that certain datasets that are widely considered fine-grained in fact contain subsets of considerable size that are substantially more coarse-grained than datasets generally regarded as coarse-grained. We also investigate the interplay between dataset granularity with a variety of factors and find that fine-grained datasets are more difficult to learn from, more difficult to transfer to, more difficult to perform few-shot learning with, and more vulnerable to adversarial attacks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/YVWI9FI7/Cui et al. - 2019 - Measuring Dataset Granularity.pdf}
}

@article{currie_conservation_2018,
  title = {Conservation and {{Education Through Ecotourism}}: {{Using Citizen Science}} to {{Monitor Cetaceans}} in the {{Four-Island Region}} of {{Maui}}, {{Hawaii}}},
  shorttitle = {Conservation and {{Education Through Ecotourism}}},
  author = {Currie, Jens J. and Stack, Stephanie H. and Kaufman, Gregory D.},
  year = {2018},
  month = aug,
  journal = {Tourism in Marine Environments},
  volume = {13},
  number = {2},
  pages = {65--71},
  issn = {1544-273X},
  doi = {10.3727/154427318X15270394903273},
  urldate = {2022-04-19},
  abstract = {Pacific Whale Foundation (PWF) Eco-Adventures operates a fleet of nine ecotour vessels in Maui, Hawaii and has used these vessels as an opportunistic research platform since 2010. The researchers at PWF have utilized ecotour vessels as a platform of opportunity (PoP) to collect photo-ID  data, through a program called Researcher-on-Board (ROB) and for the development of an application to log cetacean sightings, called Whale and Dolphin Tracker (WDT). In this article we compare the amount of data collected using these two methods and contrast to systematic research surveys  taking place in the same location and same time period to demonstrate the value of citizen science. Both the ROB and WDT programs have been shown to be cost-effective alternatives to surveys aboard dedicated research vessels, with the additional benefit of having tour operations contribute  directly to the management and monitoring of marine mammals.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/IB5U8SF3/Currie et al. - 2018 - Conservation and Education Through Ecotourism Usi.pdf}
}

@inproceedings{curry_application_2021,
  title = {Application of Deep Learning to Camera Trap Data for Ecologists in Planning / Engineering \textendash{} {{Can}} Captivity Imagery Train a Model Which Generalises to the Wild?},
  booktitle = {2021 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Curry, Ryan and Trotter, Cameron and McGough, A. Stephen},
  year = {2021},
  month = dec,
  pages = {4011--4020},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}},
  doi = {10.1109/BigData52589.2021.9671661},
  urldate = {2022-08-30},
  copyright = {All rights reserved},
  isbn = {978-1-66543-902-2},
  file = {/Users/b3020111/Zotero/storage/L9NBKIUK/Curry et al. - 2021 - Application of deep learning to camera trap data f.pdf}
}

@incollection{dauphin_identifying_2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2933--2941},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-08-08},
  file = {/Users/b3020111/Zotero/storage/FDV754IE/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf;/Users/b3020111/Zotero/storage/EEX376Q9/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimiza.html}
}

@article{deng_arcface_2019,
  title = {{{ArcFace}}: {{Additive Angular Margin Loss}} for {{Deep Face Recognition}}},
  shorttitle = {{{ArcFace}}},
  author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  year = {2019},
  month = feb,
  journal = {arXiv:1801.07698 [cs]},
  eprint = {1801.07698},
  primaryclass = {cs},
  urldate = {2020-02-12},
  abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/IRNLNN3A/Deng et al. - 2019 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf;/Users/b3020111/Zotero/storage/5UJNLEDI/1801.html}
}

@article{deng_arcface_2019-1,
  title = {{{ArcFace}}: {{Additive Angular Margin Loss}} for {{Deep Face Recognition}}},
  shorttitle = {{{ArcFace}}},
  author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  year = {2019},
  month = feb,
  journal = {arXiv:1801.07698 [cs]},
  eprint = {1801.07698},
  primaryclass = {cs},
  urldate = {2020-02-12},
  abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/H327VFAM/Deng et al. - 2019 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf;/Users/b3020111/Zotero/storage/PW7Z6QVH/1801.html}
}

@inproceedings{deng_imagenet:_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {{IEEE}},
  address = {{Miami, FL}},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2019-05-30},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  isbn = {978-1-4244-3992-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/DRIS6Y3J/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf}
}

@article{devlin_bert_2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs.CL]},
  eprint = {1810.04805},
  primaryclass = {cs.CL},
  urldate = {2022-09-27},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/b3020111/Zotero/storage/YYZJDAR4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{devries_improved_2017,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.04552 [cs]},
  eprint = {1708.04552},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/URI6BI5T/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf;/Users/b3020111/Zotero/storage/K735KXT3/1708.html}
}

@article{dey_signet_2017,
  title = {{{SigNet}}: {{Convolutional Siamese Network}} for {{Writer Independent Offline Signature Verification}}},
  shorttitle = {{{SigNet}}},
  author = {Dey, Sounak and Dutta, Anjan and Toledo, J. Ignacio and Ghosh, Suman K. and Llados, Josep and Pal, Umapada},
  year = {2017},
  month = sep,
  journal = {arXiv:1707.02131 [cs.CV]},
  number = {arXiv:1707.02131},
  eprint = {1707.02131},
  primaryclass = {cs.CV},
  urldate = {2022-06-24},
  abstract = {Offline signature verification is one of the most challenging tasks in biometrics and document forensics. Unlike other verification problems, it needs to model minute but critical details between genuine and forged signatures, because a skilled falsification might only differ from a real signature by some specific kinds of deformation. This verification task is even harder in writer independent scenarios which is undeniably fiscal for realistic cases. In this paper, we model an offline writer independent signature verification task with a convolutional Siamese network. Siamese networks are twin networks with shared weights, which can be trained to learn a feature space where similar observations are placed in proximity. This is achieved by exposing the network to a pair of similar and dissimilar observations and minimizing the Euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs. Experiments conducted on cross-domain datasets emphasize the capability of our network to handle forgery in different languages (scripts) and handwriting styles. Moreover, our designed Siamese network, named SigNet, provided better results than the state-of-the-art results on most of the benchmark signature datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/DPV6Y7B8/Dey et al. - 2017 - SigNet Convolutional Siamese Network for Writer I.pdf}
}

@inproceedings{di_style_2013,
  title = {Style {{Finder}}: {{Fine-Grained Clothing Style Detection}} and {{Retrieval}}},
  shorttitle = {Style {{Finder}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Di, Wei and Wah, Catherine and Bhardwaj, Anurag and Piramuthu, Robinson and Sundaresan, Neel},
  year = {2013},
  month = jun,
  pages = {8--13},
  publisher = {{IEEE}},
  address = {{OR, USA}},
  doi = {10.1109/CVPRW.2013.6},
  urldate = {2019-03-14},
  abstract = {With the rapid proliferation of smartphones and tablet computers, search has moved beyond text to other modalities like images and voice. For many applications like Fashion, visual search offers a compelling interface that can capture stylistic visual elements beyond color and pattern that cannot be as easily described using text. However, extracting and matching such attributes remains an extremely challenging task due to high variability and deformability of clothing items. In this paper, we propose a fine-grained learning model and multimedia retrieval framework to address this problem. First, an attribute vocabulary is constructed using human annotations obtained on a novel finegrained clothing dataset. This vocabulary is then used to train a fine-grained visual recognition system for clothing styles. We report benchmark recognition and retrieval results on Women's Fashion Coat Dataset and illustrate potential mobile applications for attribute-based multimedia retrieval of clothing items and image annotation.},
  isbn = {978-0-7695-4990-3},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/BMFGKBV8/Di et al. - 2013 - Style Finder Fine-Grained Clothing Style Detectio.pdf}
}

@article{dietterich_overfitting_1995,
  title = {Overfitting and Undercomputing in Machine Learning},
  author = {Dietterich, Tom},
  year = {1995},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {27},
  number = {3},
  pages = {326--327},
  issn = {03600300},
  doi = {10.1145/212094.212114},
  urldate = {2019-08-12}
}

@article{dosovitskiy_image_2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.11929 [cs.CV]},
  eprint = {2010.11929},
  primaryclass = {cs.CV},
  urldate = {2022-09-27},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/6PIK5EA4/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@article{duchi_adaptive_2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of machine learning research},
  volume = {12},
  number = {7},
  pages = {39},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/LNMIL7P7/Duchi et al. - Adaptive Subgradient Methods for Online Learning a.pdf}
}

@article{dumoulin_guide_2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  journal = {arXiv:1603.07285 [cs, stat]},
  eprint = {1603.07285},
  primaryclass = {cs, stat},
  urldate = {2020-04-22},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/A86KZPMN/Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf}
}

@inproceedings{dutta_evaluation_2018,
  title = {Evaluation of the Impact of Deep Learning Architectural Components Selection and Dataset Size on a Medical Imaging Task},
  booktitle = {Medical {{Imaging}} 2018: {{Imaging Informatics}} for {{Healthcare}}, {{Research}}, and {{Applications}}},
  author = {Dutta, Sandeep and Gros, Eric},
  editor = {Zhang, Jianguo and Chen, Po-Hao},
  year = {2018},
  month = mar,
  pages = {36},
  publisher = {{SPIE}},
  address = {{Houston, United States}},
  doi = {10.1117/12.2293395},
  urldate = {2022-09-28},
  isbn = {978-1-5106-1647-9 978-1-5106-1648-6}
}

@inproceedings{dutta_via_2019,
  title = {The {{VIA Annotation Software}} for {{Images}}, {{Audio}} and {{Video}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Dutta, Abhishek and Zisserman, Andrew},
  year = {2019},
  month = oct,
  pages = {2276--2279},
  publisher = {{ACM}},
  address = {{Nice France}},
  doi = {10.1145/3343031.3350535},
  urldate = {2021-01-18},
  abstract = {In this paper, we introduce a simple and standalone manual annotation tool for images, audio and video: the VGG Image Annotator (VIA). This is a light weight, standalone and offline software package that does not require any installation or setup and runs solely in a web browser. The VIA software allows human annotators to define and describe spatial regions in images or video frames, and temporal segments in audio or video. These manual annotations can be exported to plain text data formats such as JSON and CSV and therefore are amenable to further processing by other software tools. VIA also supports collaborative annotation of a large dataset by a group of human annotators. The BSD open source license of this software allows it to be used in any academic project or commercial application.},
  isbn = {978-1-4503-6889-6},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/5CE54RD6/Dutta and Zisserman - 2019 - The VIA Annotation Software for Images, Audio and .pdf}
}

@inproceedings{ekambaram_finding_2017,
  title = {Finding Label Noise Examples in Large Scale Datasets},
  booktitle = {2017 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Ekambaram, Rajmadhan and Goldgof, Dmitry B. and Hall, Lawrence O.},
  year = {2017},
  month = oct,
  pages = {2420--2424},
  doi = {10.1109/SMC.2017.8122985},
  abstract = {Mislabeled examples are difficult to avoid while building large scale datasets. In this paper we discuss an efficient approach for finding those mislabeled examples. Our approach involves selecting a small number of potentially mislabeled examples for review by an expert. We demonstrate the utility of our method by finding some mislabeled examples in one large scale dataset (ImageNet). We found 92 errors by automatically selecting 3607 examples to review out of 22951 images from 18 classes. This requires reviewing 9 times fewer examples than the random sampling method to find an equivalent number of mislabels.},
  keywords = {Character recognition,Computer science,Electronic mail,Feature extraction,Manuals,Support vector machines,Training},
  file = {/Users/b3020111/Zotero/storage/HZX7UEIA/Ekambaram et al. - 2017 - Finding label noise examples in large scale datase.pdf;/Users/b3020111/Zotero/storage/VL83G99L/stamp.html}
}

@article{el-nouby_training_2021,
  title = {Training {{Vision Transformers}} for {{Image Retrieval}}},
  author = {{El-Nouby}, Alaaeldin and Neverova, Natalia and Laptev, Ivan and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.05644 [cs.CV]},
  eprint = {2102.05644},
  primaryclass = {cs.CV},
  urldate = {2022-09-27},
  abstract = {Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/FDX3TI9U/El-Nouby et al. - 2021 - Training Vision Transformers for Image Retrieval.pdf}
}

@article{elsken_neural_2018,
  title = {Neural {{Architecture Search}}: {{A Survey}}},
  shorttitle = {Neural {{Architecture Search}}},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.5377 [stat.ML]},
  eprint = {1808.5377},
  primaryclass = {stat.ML},
  urldate = {2019-01-08},
  abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/U7VMLSNG/Elsken et al. - 2018 - Neural Architecture Search A Survey.pdf;/Users/b3020111/Zotero/storage/753P6DD5/1808.html}
}

@article{elsken_simple_2017,
  title = {Simple {{And Efficient Architecture Search}} for {{Convolutional Neural Networks}}},
  author = {Elsken, Thomas and Metzen, Jan-Hendrik and Hutter, Frank},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.04528 [cs, stat]},
  eprint = {1711.04528},
  primaryclass = {cs, stat},
  urldate = {2019-01-08},
  abstract = {Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6\% in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/843UYA4I/Elsken et al. - 2017 - Simple And Efficient Architecture Search for Convo.pdf;/Users/b3020111/Zotero/storage/GBEBMT6I/1711.html}
}

@article{ester_density-based_1996,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  year = {1996},
  journal = {KDD-96 Proceedings},
  pages = {226--231},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/N95TV7IK/Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf}
}

@article{evans_monitoring_2004,
  title = {Monitoring Cetaceans in {{European}} Waters},
  author = {Evans, Peter G. H. and Hammond, Philip S.},
  year = {2004},
  journal = {Mammal Review},
  volume = {34},
  number = {1-2},
  pages = {131--156},
  issn = {1365-2907},
  doi = {10.1046/j.0305-1838.2003.00027.x},
  urldate = {2021-06-11},
  abstract = {1. Monitoring spatial and temporal patterns in cetacean abundance involves a variety of approaches depending upon the target species and the resources available. As a first step, the collection of incidental sightings or strandings information aids the construction of a species list and a rough measure of status and seasonal variation in abundance. These often make use of networks of volunteer observers although the wide variation in abilities and experience means that special attention must be paid to training and to data quality control. More robust monitoring of numbers requires quantification of effort and some correction for factors that influence detectability, such as sea state. 2. The presence of cetaceans may be recorded visually, or indirectly by acoustics. Each has advantages and disadvantages, and their applicability may vary between species. The use of fixed stations tends to allow sustained monitoring at relatively low cost but coverage is limited to the immediate vicinity. For more extensive coverage, mobile platforms are necessary. Platforms of opportunity such as ferries, whale-watching boats, etc. are often used to survey areas at low cost. These may allow repeat observations to be made over time, but with no control over where the vessel goes, it is typically not possible to sample wide areas, thus limiting abundance estimation. 3. Line transect surveys using dedicated platforms allow representative coverage of areas from which abundance estimates can be made (either using indices or absolute measures derived from density estimation). Assumptions relating to detectability and responsiveness need to be addressed and various methods (such as two-platform surveys) have been developed to accommodate these. 4. For some cetacean species, mark-recapture methods can be applied using photo-identification of recognizable individuals. Again, a number of assumptions are made, particularly relating to recognizability, representativeness of sampling and capture probabilities. Capturing, on film, as many animals in the population as possible helps to reduce the problem of heterogeneity of capture probabilities. Mark-recapture methods require at least two sampling occasions. If multiple sampling is employed, either open or closed population models can be used. 5. Measuring population change represents a particular challenge for mobile animals such as cetaceans. Changes in ranging patterns may have a large impact on abundance estimates unless very large areas are adequately covered. Power analysis is a useful method to indicate the ability of the data to detect a trend of a given magnitude. Increasingly, spatial modelling using GLMs and GAMs is being used to provide a better understanding of the biotic and hydrographic factors influencing cetacean distribution.},
  langid = {english},
  keywords = {acoustics,distribution,dolphins,line transect surveys,marine mammals,photo-identification,population change,whales},
  file = {/Users/b3020111/Zotero/storage/6Q4QJA99/Evans and Hammond - 2004 - Monitoring cetaceans in European waters.pdf;/Users/b3020111/Zotero/storage/YF8PAWPV/j.0305-1838.2003.00027.html}
}

@article{everingham_pascal_2010,
  title = {The {{Pascal Visual Object Classes}} ({{VOC}}) {{Challenge}}},
  author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  year = {2010},
  month = jun,
  journal = {International Journal of Computer Vision},
  volume = {88},
  number = {2},
  pages = {303--338},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-009-0275-4},
  urldate = {2019-03-20},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/Z9XEBVKA/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf}
}

@book{fang_artificial_2019,
  title = {Artificial {{Intelligence}} and {{Conservation}}},
  editor = {Fang, Fei and Tambe, Milind and Dilkina, Bistra and Plumptre, Andrew J.},
  year = {2019},
  month = feb,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108587792},
  urldate = {2021-06-02},
  isbn = {978-1-108-58779-2 978-1-316-51292-0 978-1-108-46473-4},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/PVGFC5XD/Fang et al. - 2019 - Artificial Intelligence and Conservation.pdf}
}

@article{fang_eva_2022,
  title = {{{EVA}}: {{Exploring}} the {{Limits}} of {{Masked Visual Representation Learning}} at {{Scale}}},
  shorttitle = {{{EVA}}},
  author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  year = {2022},
  month = dec,
  journal = {arXiv:2211.07636 [cs.CV]},
  eprint = {2211.07636},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {We launch EVA, a vision-centric foundation model to Explore the limits of Visual representation at scAle using only publicly accessible data. EVA is a vanilla ViT pretrained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and billion-scale models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/GVK28QYF/Fang et al. - 2022 - EVA Exploring the Limits of Masked Visual Represe.pdf}
}

@article{fang_you_2021,
  title = {You {{Only Look}} at {{One Sequence}}: {{Rethinking Transformer}} in {{Vision}} through {{Object Detection}}},
  shorttitle = {You {{Only Look}} at {{One Sequence}}},
  author = {Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
  year = {2021},
  month = oct,
  journal = {arXiv:2106.00666 [cs.CV]},
  eprint = {2106.00666},
  primaryclass = {cs.CV},
  urldate = {2022-09-27},
  abstract = {Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/DEKEAAXK/Fang et al. - 2021 - You Only Look at One Sequence Rethinking Transfor.pdf}
}

@inproceedings{farrell_birdlets:_2011,
  title = {Birdlets: {{Subordinate}} Categorization Using Volumetric Primitives and Pose-Normalized Appearance},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Farrell, Ryan and Oza, Om and Zhang, Ning and Morariu, Vlad I. and Darrell, Trevor and Davis, Larry S.},
  year = {2011},
  pages = {161--168},
  publisher = {{IEEE}},
  isbn = {1-4577-1102-8}
}

@article{fei-fei_learning_2007,
  title = {Learning Generative Visual Models from Few Training Examples: {{An}} Incremental {{Bayesian}} Approach Tested on 101 Object Categories},
  shorttitle = {Learning Generative Visual Models from Few Training Examples},
  author = {{Fei-Fei}, Li and Fergus, Rob and Perona, Pietro},
  year = {2007},
  month = apr,
  journal = {Computer Vision and Image Understanding},
  volume = {106},
  number = {1},
  pages = {59--70},
  issn = {10773142},
  doi = {10.1016/j.cviu.2005.09.012},
  urldate = {2019-03-20},
  langid = {english}
}

@article{felzenszwalb_object_2010,
  title = {Object Detection with Discriminatively Trained Part-Based Models},
  author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
  year = {2010},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {32},
  number = {9},
  pages = {1627--1645}
}

@inproceedings{feng_triplet_2020,
  title = {Triplet {{Distillation For Deep Face Recognition}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Feng, Yushu and Wang, Huan and Hu, Haoji Roland and Yu, Lu and Wang, Wei and Wang, Shiyan},
  year = {2020},
  month = oct,
  pages = {808--812},
  issn = {2381-8549},
  doi = {10.1109/ICIP40778.2020.9190651},
  abstract = {Convolutional neural networks (CNNs) have achieved great successes in face recognition, which unfortunately comes at the cost of massive computation and storage consumption. Many compact face recognition networks are thus proposed to resolve this problem, and triplet loss is effective to further improve the performance of these compact models. However, it normally employs a fixed margin to all the samples, which neglects the informative similarity structures between different identities. In this paper, we borrow the idea of knowledge distillation and define the informative similarity as the transferred knowledge. Then, we propose an enhanced version of triplet loss, named triplet distillation, which exploits the capability of a teacher model to transfer the similarity information to a student model by adaptively varying the margin between positive and negative pairs. Experiments on the LFW, AgeDB and CPLFW datasets show the merits of our method compared to the original triplet loss.},
  keywords = {Adaptation models,Computational modeling,Conferences,Convolutional neural networks,Face recognition,Face Recognition,Image coding,Image recognition,Knowledge Distillation,Network Compression,Triplet Loss},
  file = {/Users/b3020111/Zotero/storage/5MP35KRH/Feng et al. - 2020 - Triplet Distillation For Deep Face Recognition.pdf;/Users/b3020111/Zotero/storage/GN69H8XJ/stamp.html}
}

@article{feyrer_origin_2021,
  title = {Origin and {{Persistence}} of {{Markings}} in a {{Long-Term Photo-Identification Dataset Reveal}} the {{Threat}} of {{Entanglement}} for {{Endangered Northern Bottlenose Whales}} ({{Hyperoodon}} Ampullatus)},
  author = {Feyrer, Laura Joan and Stewart, Madison and Yeung, Jas and Soulier, Colette and Whitehead, Hal},
  year = {2021},
  journal = {Frontiers in Marine Science},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {2296-7745},
  doi = {10.3389/fmars.2021.620804},
  urldate = {2021-05-11},
  abstract = {Photo-identification methods depend on markings that are stable over time. Using a large dataset of photographs taken over a 31-year period, we evaluate the reliability, rate of change and demographic trends in different mark types on northern bottlenose whales (Hyperoodon ampullatus) in the Endangered Scotian Shelf population, and assess the prevalence and severity of anthropogenically caused markings. Only fin notches and back indentations were stable over long timescales, leading to 48\% of the overall population being assessed as reliably marked. Males and mature males were found to have higher incidence of most mark types compared to females and juveniles. The proportion of reliably marked individuals increased over time, a trend that should be accounted for in any temporal analysis of population size using mark-recapture methods. An overall increase in marked individuals may reflect the accumulation of scars on an aging population post whaling. Anthropogenic markings, including probable entanglement and propeller-vessel strike scars, occurred at a steady rate over the study period and were observed on 6.6\% of the population. The annual gain rate for all injuries associated with anthropogenic interactions was over 5 times the annual potential biological removal (PBR) calculated for the endangered population. As entanglement incidents and propeller-vessel strike injuries are typically undetected in offshore areas, we provide the first minimum estimate of harmful human interactions for northern bottlenose whales. With low observer effort for fisheries across the Canadian Atlantic, photo-identification offers an important line of evidence of the risks faced by this Endangered whale population.},
  langid = {english},
  keywords = {Beaked whale,Endangered Species,Fisheries bycatch,Marine Protected Area,Potential biological removal (PBR),Vessel strikes},
  file = {/Users/b3020111/Zotero/storage/7J55IEBK/Feyrer et al. - 2021 - Origin and Persistence of Markings in a Long-Term .pdf}
}

@article{fiori_using_2020,
  title = {Using {{Unmanned Aerial Vehicles}} ({{UAVs}}) to Assess Humpback Whale Behavioral Responses to Swim-with Interactions in {{Vava}}'u, {{Kingdom}} of {{Tonga}}},
  author = {Fiori, Lorenzo and Martinez, Emmanuelle and Orams, Mark B. and Bollard, Barbara},
  year = {2020},
  month = nov,
  journal = {Journal of Sustainable Tourism},
  volume = {28},
  number = {11},
  pages = {1743--1761},
  issn = {0966-9582},
  doi = {10.1080/09669582.2020.1758706},
  urldate = {2021-01-05},
  abstract = {The Kingdom of Tonga is a one of the few countries worldwide that allow swim-with-whales tourism activities. Most of the tour operators are based in Vava'u archipelago which represents an important breeding ground for Oceania humpback whales (Megaptera novaeangliae). This study represents an assessment of the effects of swimmer approaches on humpback whales' behaviour using Unmanned Aerial Vehicles (UAVs). UAV flights took place during the 2016 and 2017 whale breeding seasons from onboard research and swim-with-whales vessels. Whales' behavioural states (resting, travelling, surface-active, socialising, nurturing) were assessed from aerial videos and the proportions of time spent in each state in the presence and absence of swimmers were compared. Whale agonistic behaviours directed towards swimmers and the injury of a swimmer caused by a whale were documented. Results indicate that in-water tourism activities significantly altered the time spent in each behavioural state by humpback whale in Vava'u. Mother-calf pairs decreased the proportion of time spent nurturing, while the time spent travelling increased two-fold when approached by swimmers. These findings indicate a potential energy expenditure increase for humpback whale mothers and their calves in response to swim-with tourism activities in Vava'u. Moreover, whales' behavioural responses can pose danger of injury to swimmers.},
  keywords = {behaviour,drones,impact,injury,swim-with-whales},
  file = {/Users/b3020111/Zotero/storage/6TU8UHKM/Fiori et al. - 2020 - Using Unmanned Aerial Vehicles (UAVs) to assess hu.pdf}
}

@misc{fisheries_finbase_2018,
  title = {{{FinBase Photo-Identification Database System}} | {{NOAA Fisheries}}},
  author = {Fisheries, {\relax NOAA}},
  year = {Thu, 08/02/2018 - 16:24},
  journal = {NOAA},
  urldate = {2020-02-12},
  abstract = {A database system consists of a collection of subdirectories (associated with image and file storage) and front- and back-end Microsoft Access databases},
  howpublished = {https://www.fisheries.noaa.gov/national/marine-mammal-protection/finbase-photo-identification-database-system},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/9JNLCLAR/finbase-photo-identification-database-system.html}
}

@article{foret_sharpness-aware_2020,
  title = {Sharpness-{{Aware Minimization}} for {{Efficiently Improving Generalization}}},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  year = {2020},
  month = dec,
  journal = {arXiv:2010.01412 [cs, stat]},
  eprint = {2010.01412},
  primaryclass = {cs, stat},
  urldate = {2021-01-04},
  abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization -- including a generalization bound that we prove here -- we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-\{10, 100\}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/8PEAU7MP/Foret et al. - 2020 - Sharpness-Aware Minimization for Efficiently Impro.pdf;/Users/b3020111/Zotero/storage/8GS7R49A/2010.html}
}

@article{forney_seasonal_1998,
  title = {Seasonal {{Patterns}} in the {{Abundance}} and {{Distribution}} of {{California Cetaceans}}, 1991\textendash 1992},
  author = {Forney, Karin A. and Barlow, Jay},
  year = {1998},
  journal = {Marine Mammal Science},
  volume = {14},
  number = {3},
  pages = {460--489},
  issn = {1748-7692},
  doi = {10.1111/j.1748-7692.1998.tb00737.x},
  urldate = {2021-01-07},
  abstract = {This study presents a detailed seasonal comparison of the abundance and distribution of cetaceans within 100-150 nmi (185-278 km) of the California coast during 1991 and 1992. The results of a shipboard line-transect survey conducted in July-November 1991 (``summer'') were compared to those from aerial line-transect surveys conducted in March-April 1991 and February-April 1992 (``winter''). Using a confidence-interval-based bootstrap procedure, abundance estimates for six of the eleven species included in the comparison exhibited significant ({$\alpha$}= 0.05) differences between the winter and summer surveys. Pacific white-sided dolphins (Lagenorhynchus obliquidens), Risso's dolphins (Grampus griseus), common dolphins (Delphinus spp.), and northern right whale dolphins (Lissodelphis borealis) were significantly more abundant in winter. The abundance of blue whales (Balaenoptera musculuss) and gray whales (Eschrichtius robustus) reflected well-documented migratory patterns. Fin whales (B. physalus) were significantly more abundant during summer. No significant differences in seasonal abundance were identified for Dall's porpoises (Phocoenoides dalli), bottlenose dolphins (Tursiops truncatus), killer whales (Orcinus orca), sperm whales (Physeter macrocephalus), or humpback whales (Megaptera novaeangliae). Significant north/south shifts in distribution were found for Dall's porpoises, common dolphins, and Pacific white-sided dolphins, and significant inshore/offshore differences were identified for northern right whale dolphins and humpback whales.},
  langid = {english},
  keywords = {abundance,aerial survey,bootstrap,California,cetacean,confidence-interval test,distribution,dolphin,line transect,North Pacific,porpoise,seasonality,ship survey,whale},
  file = {/Users/b3020111/Zotero/storage/WUHD9VM2/Forney and Barlow - 1998 - Seasonal Patterns in the Abundance and Distributio.pdf;/Users/b3020111/Zotero/storage/YMGNN47Y/j.1748-7692.1998.tb00737.html}
}

@techreport{franklin_migratory_2008,
  title = {Migratory Movements of Humpback Whales ({{Megaptera}} Novaeangliae) between Eastern {{Australia}} and the {{Balleny Islands}}, {{Antarctica}}, Confirmed by Photo-Identification},
  author = {Franklin, Trish and Smith, Franz and Gibbs, Nadine and Childerhouse, Simon and Paton, David and Franklin, Wally and Baker, Scott and Clapham, Phil},
  year = {2008},
  number = {SC/59/SH18},
  pages = {5},
  institution = {{International Whaling Commission}},
  abstract = {Using photo-identification, we report here migratory movements of three humpback whales (Megaptera novaeangliae) between the E1 breeding area (eastern Australia) and the Area V Antarctic feeding grounds. Comparisons between a Balleny Island fluke catalogue (11 individuals), and fluke catalogues from Hervey Bay (1556 individuals), Byron Bay (916 individuals), Ballina, (648 individuals), New Zealand (41 individuals), Fiji (3 individuals) and Samoa (2 individuals) yielded three matches between the Balleny Islands and Hervey Bay, Byron Bay and Ballina. Only three previous individual matches have been reported between the E1 breeding grounds and Antarctic Area V feeding grounds. Photo-identification has revealed a matrix of migratory interchange among Eastern Australian and Oceania breeding areas (E and F), and this method can significantly contribute to the understanding of linkages between and within breeding and feeding areas and of humpback whale population structure in the South Pacific and Southern Oceans.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/CNQ36VXA/Franklin et al. - Migratory movements of humpback whales (Megaptera .pdf}
}

@article{franklin_photo-identification_2020,
  title = {Photo-Identification of Individual {{Southern Hemisphere}} Humpback Whales ({{Megaptera}} Novaeangliae) Using All Available Natural Marks:: Managing the Potential for Misidentification},
  shorttitle = {Photo-Identification of Individual {{Southern Hemisphere}} Humpback Whales ({{Megaptera}} Novaeangliae) Using All Available Natural Marks},
  author = {Franklin, Wally and Franklin, Trish and Harrison, Peter and Brooks, Lyndon},
  year = {2020},
  month = oct,
  journal = {J. Cetacean Res. Manage.},
  volume = {21},
  number = {1},
  pages = {71--83},
  issn = {2312-2706},
  doi = {10.47536/jcrm.v21i1.186},
  urldate = {2020-10-15},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/E7JJPTSI/Franklin et al. - 2020 - Photo-identification of individual Southern Hemisp.pdf;/Users/b3020111/Zotero/storage/UKXRIFR6/186.html}
}

@article{fu_retinamask:_2019,
  title = {{{RetinaMask}}: {{Learning}} to Predict Masks Improves State-of-the-Art Single-Shot Detection for Free},
  author = {Fu, Cheng-Yang and Shvets, Mykhailo and Berg, Alexander C},
  year = {2019},
  journal = {arXiv preprint arXiv:1901.03353},
  eprint = {1901.03353},
  archiveprefix = {arxiv}
}

@inproceedings{fujita_fine-tuned_2020,
  title = {Fine-Tuned {{Surface Object Detection Applying Pre-trained Mask R-CNN Models}}},
  booktitle = {2020 {{International Conference}} on {{Computational Intelligence}} ({{ICCI}})},
  author = {Fujita, H. and Itagaki, M. and Ichikawa, K. and Hooi, Y. K. and Kawahara, K. and Sarlan, A.},
  year = {2020},
  month = oct,
  pages = {17--22},
  doi = {10.1109/ICCI51257.2020.9247666},
  abstract = {This study evaluates road surface object detection tasks using four Mask R-CNN models available on the Tensor-Flow Object Detection API. The models were pre-trained using COCO datasets and fine-tuned by 15,1SS segmented road surface annotation tags. Validation data set was used to obtain Average Precisions and Average Recalls. Result indicates a substantial false negatives or ``left judgement'' counts for linear cracks, joints, fillings, potholes, stains, shadows and patching with grid cracks classes. There were significant number of incorrectly predicted label instances. To improve the result, an alternative metric calculation method was tested. However, the results showed strong mutual interferences caused by misinterpretation of the scratches with other object classes.},
  keywords = {15-1SS segmented road surface annotation tags,application program interfaces,AR,COCO datasets,Computational modeling,condition monitoring,convolutional neural nets,cracks,Filling,geotechnical structures,image segmentation,learning (artificial intelligence),mAP,Mask R-CNN models,Measurement,object classes,object detection,Object detection,pre-trained mask R-CNN models,road object detection,road surface object detection tasks,roads,Roads,structural engineering computing,Surface cracks,surface object detection,Task analysis,tensor-flow object detection API,Tensor-Flow Object Detection API,validation data set},
  file = {/Users/b3020111/Zotero/storage/PMZAEVA5/Fujita et al. - 2020 - Fine-tuned Surface Object Detection Applying Pre-t.pdf;/Users/b3020111/Zotero/storage/IQXB96CU/9247666.html}
}

@misc{gal_dropout_2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  number = {arXiv:1506.02142},
  eprint = {1506.02142},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-06-24},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/EGTDIZCD/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@phdthesis{gal_uncertainty_2016,
  title = {Uncertainty in {{Deep Learning}}},
  author = {Gal, Yarin},
  year = {2016},
  langid = {english},
  school = {University of Cambridge},
  file = {/Users/b3020111/Zotero/storage/SRHZR3BV/Gal - Uncertainty in Deep Learning.pdf}
}

@article{galatius_lagenorhynchus_2016,
  title = {Lagenorhynchus Albirostris ({{Cetacea}}: {{Delphinidae}})},
  shorttitle = {Lagenorhynchus Albirostris ({{Cetacea}}},
  author = {Galatius, Anders and Kinze, Carl Christian},
  year = {2016},
  month = aug,
  journal = {Mammalian Species},
  volume = {48},
  number = {933},
  pages = {35--47},
  issn = {0076-3519},
  doi = {10.1093/mspecies/sew003},
  urldate = {2019-08-07},
  abstract = {Abstract.  Lagenorhynchus albirostris ( Gray, 1846a ) is a delphinid commonly called the white-beaked dolphin. A robustly built dolphin with black, white, and g},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ZBZNMHAX/Galatius and Kinze - 2016 - Lagenorhynchus albirostris (Cetacea Delphinidae).pdf;/Users/b3020111/Zotero/storage/6AIVQWB2/2583995.html}
}

@article{gao_towards_2021,
  title = {Towards {{Self-Supervision}} for {{Video Identification}} of {{Individual Holstein-Friesian Cattle}}: {{The Cows2021 Dataset}}},
  shorttitle = {Towards {{Self-Supervision}} for {{Video Identification}} of {{Individual Holstein-Friesian Cattle}}},
  author = {Gao, Jing and Burghardt, Tilo and Andrew, William and Dowsey, Andrew W. and Campbell, Neill W.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.01938 [cs]},
  eprint = {2105.01938},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {In this paper we publish the largest identity-annotated Holstein-Friesian cattle dataset Cows2021 and a first self-supervision framework for video identification of individual animals. The dataset contains 10,402 RGB images with labels for localisation and identity as well as 301 videos from the same herd. The data shows top-down in-barn imagery, which captures the breed's individually distinctive black and white coat pattern. Motivated by the labelling burden involved in constructing visual cattle identification systems, we propose exploiting the temporal coat pattern appearance across videos as a self-supervision signal for animal identity learning. Using an individual-agnostic cattle detector that yields oriented bounding-boxes, rotation-normalised tracklets of individuals are formed via tracking-by-detection and enriched via augmentations. This produces a `positive' sample set per tracklet, which is paired against a `negative' set sampled from random cattle of other videos. Frame-triplet contrastive learning is then employed to construct a metric latent space. The fitting of a Gaussian Mixture Model to this space yields a cattle identity classifier. Results show an accuracy of Top-1 57.0\% and Top-4: 76.9\% and an Adjusted Rand Index: 0.53 compared to the ground truth. Whilst supervised training surpasses this benchmark by a large margin, we conclude that self-supervision can nevertheless play a highly effective role in speeding up labelling efforts when initially constructing supervision information. We provide all data and full source code alongside an analysis and evaluation of the system.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/WRRK3WD9/Gao et al. - 2021 - Towards Self-Supervision for Video Identification .pdf;/Users/b3020111/Zotero/storage/UFHHZ9RA/2105.html}
}

@inproceedings{gavves_fine-grained_2013,
  title = {Fine-{{Grained Categorization}} by {{Alignments}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Gavves, E. and Fernando, B. and Snoek, C. G. M. and Smeulders, A. W. M. and Tuytelaars, T.},
  year = {2013},
  pages = {1713--1720},
  urldate = {2019-04-12},
  file = {/Users/b3020111/Zotero/storage/3FNJNV8Y/Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf;/Users/b3020111/Zotero/storage/R336N6YG/Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf;/Users/b3020111/Zotero/storage/DLKGGEFZ/Gavves_Fine-Grained_Categorization_by_2013_ICCV_paper.html}
}

@article{ge_yolox_2021,
  title = {{{YOLOX}}: {{Exceeding YOLO Series}} in 2021},
  shorttitle = {{{YOLOX}}},
  author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
  year = {2021},
  month = aug,
  journal = {arXiv:2107.08430 [cs.CV]},
  eprint = {2107.08430},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3\% AP on COCO, surpassing NanoDet by 1.8\% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3\% AP on COCO, outperforming the current best practice by 3.0\% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0\% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8\% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/V7UQIRRU/Ge et al. - 2021 - YOLOX Exceeding YOLO Series in 2021.pdf;/Users/b3020111/Zotero/storage/Q4ZJEHVB/2107.html}
}

@misc{georgetown_university_is_2018,
  title = {Is {{That}} `{{Jimmy Carter}}' or `{{Barbara Bush}}?' {{Google Designs}}, {{Professor Uses Artificial Intelligence}} to {{Track Wildlife}}},
  shorttitle = {Is {{That}} `{{Jimmy Carter}}' or `{{Barbara Bush}}?},
  author = {Georgetown University},
  year = {2018},
  month = oct,
  journal = {Georgetown University},
  urldate = {2020-02-12},
  abstract = {Janet Mann, professor of biology and psychology, collaborates with Google's artificial intelligence engineers on individual identification of wild dolphins through images.},
  howpublished = {https://www.georgetown.edu/news/is-that-jimmy-carter-or-barbara-bush-google-designs-professor-uses-artificial-intelligence-to-track-wildlife/},
  langid = {american},
  file = {/Users/b3020111/Zotero/storage/NB4JSIJQ/is-that-jimmy-carter-or-barbara-bush-google-designs-professor-uses-artificial-intelligence-to-t.html}
}

@article{gholamalinezhad_pooling_2020,
  title = {Pooling {{Methods}} in {{Deep Neural Networks}}, a {{Review}}},
  author = {Gholamalinezhad, Hossein and Khosravi, Hossein},
  year = {2020},
  journal = {arXiv:2009.07485 [cs.CV]},
  eprint = {2009.07485},
  primaryclass = {cs.CV},
  abstract = {Nowadays, Deep Neural Networks are among the main tools used in various sciences. Convolutional Neural Network is a special type of DNN consisting of several convolution layers, each followed by an activation function and a pooling layer. The pooling layer is an important layer that executes the down-sampling on the feature maps coming from the previous layer and produces new feature maps with a condensed resolution. This layer drastically reduces the spatial dimension of input. It serves two main purposes. The first is to reduce the number of parameters or weights, thus lessening the computational cost. The second is to control the overfitting of the network. An ideal pooling method is expected to extract only useful information and discard irrelevant details. There are a lot of methods for the implementation of pooling operation in Deep Neural Networks. In this paper, we reviewed some of the famous and useful pooling methods.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/PVC3SDJP/Gholamalinezhad and Khosravi - Pooling Methods in Deep Neural Networks, a Review.pdf}
}

@article{ghosh_facilitating_2022,
  title = {Facilitating {{Human-Wildlife Cohabitation}} through {{Conflict Prediction}}},
  author = {Ghosh, Susobhan and Varakantham, Pradeep and Bhatkhande, Aniket and Ahmad, Tamanna and Andheria, Anish and Li, Wenjun and Taneja, Aparna and Thakkar, Divy and Tambe, Milind},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {11},
  pages = {12496--12502},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i11.21518},
  urldate = {2022-12-09},
  abstract = {With increasing world population and expanded use of forests as cohabited regions, interactions and conflicts with wildlife are increasing, leading to large-scale loss of lives (animal and human) and livelihoods (economic). While community knowledge is valuable, forest officials and conservation organisations can greatly benefit from predictive analysis of human-wildlife conflict, leading to targeted interventions that can potentially help save lives and livelihoods. However, the problem of prediction is a complex socio-technical problem in the context of limited data in low-resource regions.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/BLZCVDEC/Ghosh et al. - 2022 - Facilitating Human-Wildlife Cohabitation through C.pdf}
}

@article{gibson_using_2020,
  title = {Using Social Media as a Cost-Effective Resource in the Photo-Identification of a Coastal Bottlenose Dolphin Community},
  author = {Gibson, Catherine Elizabeth and Williams, David and Dunlop, Rebecca and Beck, Suzanne},
  year = {2020},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {30},
  number = {8},
  pages = {1702--1710},
  issn = {1099-0755},
  doi = {10.1002/aqc.3356},
  urldate = {2021-01-07},
  abstract = {Bottlenose dolphins encountered around the Irish coast are considered part of a wide-ranging coastal community; however, knowledge on the significance of the north of Ireland for this species is limited by a lack of dedicated effort. Through social media, the opportunity now exists to gather large volumes of citizen science data in the form of high-quality images, potentially extending the spatial and temporal scope of photo-identification studies. The purpose of this study was to investigate social media as a data resource for photo-identification studies and to provide a preliminary assessment of bottlenose dolphins in the north of Ireland. Specifically, the study sought to examine the photo-identification data for spatial clustering. The study identified 54 well-marked individuals and provided evidence of potential year-round occurrence, with successful re-sightings throughout the study period (2007\textendash 2016). There was a geographic concentration of re-sightings along the north of Ireland, suggestive of interannual site fidelity. These results provide scientific rationale for strategically targeting the north of Ireland in future research on the Irish coastal community. For effective conservation of the bottlenose dolphin it is imperative that scientific research, and resultant management objectives, consider wide-ranging communities such as the Irish coastal community. Our research highlights data collection via social media as a cost-effective and scientifically valuable tool in the photo-identification of coastal cetaceans. We recommend that this method is used in research on low-density and wide-ranging coastal cetaceans.},
  copyright = {\textcopyright{} 2020 Crown Copyright. Aquatic Conservation: Marine and Freshwater Ecosystems published by John Wiley \& Sons, Ltd. This article is published with the permission of the Controller of HMSO and the Queen's Printer for Scotland.},
  langid = {english},
  keywords = {coastal,distribution,habitats directive,monitoring},
  file = {/Users/b3020111/Zotero/storage/XN2HLNEA/Gibson et al. - 2020 - Using social media as a cost-effective resource in.pdf;/Users/b3020111/Zotero/storage/QUXUHWLQ/aqc.html;/Users/b3020111/Zotero/storage/UFEFRKRZ/aqc.html}
}

@article{giles_responses_2020,
  title = {Responses of Bottlenose Dolphins ({{Tursiops}} Spp.) to Small Drones},
  author = {Giles, Anna B. and Butcher, Paul A. and Colefax, Andrew P. and Pagendam, Dan E. and Mayjor, Maddison and Kelaher, Brendan P.},
  year = {2020},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  issn = {1099-0755},
  doi = {10.1002/aqc.3440},
  urldate = {2021-01-05},
  abstract = {Recent advances in aerial drones offer new insights into the biology, ecology and behaviour of marine wildlife found on or near the ocean's surface. While opening up new opportunities for enhanced wildlife monitoring, the impacts of drone sampling and how it might influence interpretations of animal behaviour are only just beginning to be understood. The capacity of drones to record bottlenose dolphin (Tursiops spp.) behaviour was investigated, along with how the presence of a small drone at varying altitudes influences dolphin behaviour. Over 3 years and eight locations, 361 drone flights were completed between altitudes of 5 and 60 m above the ocean. Analyses showed that dolphins were increasingly likely to change behaviour with decreasing drone altitude. A positive correlation was also found between time spent hovering above a group of dolphins and the probability of recording a behavioural response. Dolphin group size also influenced the frequency of an observed behavioural change, displaying a positive correlation between behaviour change and group size. Overall, although drones have the potential to impact coastal dolphins when flown at low altitudes, they represent a useful tool for collecting ecological information on coastal dolphins owing to their convenience, low cost and capacity to observe behaviours underwater. To maximize benefits and minimize impacts, this study suggests that drones should be flown 30 m above coastal bottlenose dolphins.},
  copyright = {\textcopyright{} 2020 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {behaviour,coastal,disturbance,drone,mammals,new techniques,ocean,recreation,Tursiops aduncus,Tursiops truncatus},
  file = {/Users/b3020111/Zotero/storage/GQZC5TAB/Giles et al. - Responses of bottlenose dolphins (Tursiops spp.) t.pdf}
}

@inproceedings{gilotte_offline_2018,
  title = {Offline {{A}}/{{B Testing}} for {{Recommender Systems}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Gilotte, Alexandre and Calauz{\`e}nes, Cl{\'e}ment and Nedelec, Thomas and Abraham, Alexandre and Doll{\'e}, Simon},
  year = {2018},
  month = feb,
  pages = {198--206},
  publisher = {{ACM}},
  address = {{Marina Del Rey CA USA}},
  doi = {10.1145/3159652.3159687},
  urldate = {2022-09-28},
  abstract = {Online A/B testing evaluates the impact of a new technology by running it in a real production environment and testing its performance on a subset of the users of the platform. It is a well-known practice to run a preliminary offline evaluation on historical data to iterate faster on new ideas, and to detect poor policies in order to avoid losing money or breaking the system. For such offline evaluations, we are interested in methods that can compute offline an estimate of the potential uplift of performance generated by a new technology. Offline performance can be measured using estimators known as counterfactual or off-policy estimators. Traditional counterfactual estimators, such as capped importance sampling or normalised importance sampling, exhibit unsatisfying bias-variance compromises when experimenting on personalized product recommendation systems. To overcome this issue, we model the bias incurred by these estimators rather than bound it in the worst case, which leads us to propose a new counterfactual estimator. We provide a benchmark of the different estimators showing their correlation with business metrics observed by running online A/B tests on a large-scale commercial recommender system.},
  isbn = {978-1-4503-5581-0},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/KMDG26HH/Gilotte et al. - 2018 - Offline AB Testing for Recommender Systems.pdf}
}

@article{girshick_fast_2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = {2015},
  month = apr,
  journal = {arXiv:1504.08083 [cs]},
  eprint = {1504.08083},
  primaryclass = {cs},
  urldate = {2019-06-05},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/X6QD9372/Girshick - 2015 - Fast R-CNN.pdf;/Users/b3020111/Zotero/storage/ZL4KXHHF/1504.html}
}

@inproceedings{girshick_rich_2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  pages = {580--587},
  urldate = {2019-06-05},
  file = {/Users/b3020111/Zotero/storage/JGNEX6FD/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf;/Users/b3020111/Zotero/storage/9VHKQ9ZC/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html}
}

@misc{gomez_understanding_2019,
  title = {Understanding {{Ranking Loss}}, {{Contrastive Loss}}, {{Margin Loss}}, {{Triplet Loss}}, {{Hinge Loss}} and All Those Confusing Names},
  author = {G{\'o}mez, Ra{\'u}l},
  year = {2019},
  urldate = {2022-06-29},
  howpublished = {https://gombru.github.io/2019/04/03/ranking\_loss/},
  annotation = {https://gombru.github.io/2019/04/03/ranking\_loss/},
  file = {/Users/b3020111/Zotero/storage/5AI3VDSM/ranking_loss.html}
}

@article{goswami_application_2007,
  title = {Application of Photographic Capture\textendash Recapture Modelling to Estimate Demographic Parameters for Male {{Asian}} Elephants},
  author = {Goswami, Varun R. and Madhusudan, M. D. and Karanth, K. Ullas},
  year = {2007},
  journal = {Animal Conservation},
  volume = {10},
  number = {3},
  pages = {391--399},
  issn = {1469-1795},
  doi = {10.1111/j.1469-1795.2007.00124.x},
  urldate = {2021-06-11},
  abstract = {In addition to the threats of habitat loss and degradation, adult males of the Asian elephant Elephas maximus also face greater threats from ivory poaching and conflict with humans. To understand the impact of these threats, conservationists need robust estimates of abundance and vital rates specifically for the adult male segment of elephant populations. By integrating the identification of individual male elephants in a population from distinct morphology and natural markings, with modern capture\textendash recapture (CR) sampling designs, it is possible to estimate various demographic parameters that are otherwise difficult to obtain from this long-lived and wide-ranging megaherbivore. In this study, we developed systematic individual identification protocols and integrated them into CR sampling designs to obtain capture histories and thereby estimate the abundance of adult bull elephants in a globally important population in southern India. We validated these estimates against those obtained from an independent method combining line-transect density estimates with age\textendash sex composition data for elephants. The sampled population was open to gains and losses between sampling occasions. The abundance of adult males in the 176 km2 study area was {\^N}(S\^E{\^N}) = 134(14.20) and they comprised 14\% ({$\pm$}1\%) of the total elephant population. Time-specific abundance estimates for each sampling occasion showed a distinct increase in adult male numbers over the sampling period, explained by seasonal patterns of local migration. CR-based estimates for adult male abundance closely matched estimates from distance-based methods. Thus, while providing abundance data of comparable rigour and precision, photographic CR methods permit estimation of demographic parameters for the Asian elephant that are both urgently needed and difficult to obtain.},
  langid = {english},
  keywords = {abundance,India,individual identification,open population,population density},
  file = {/Users/b3020111/Zotero/storage/L4DAZIRY/Goswami et al. - 2007 - Application of photographic captureârecapture mode.pdf;/Users/b3020111/Zotero/storage/RRWNQUCZ/j.1469-1795.2007.00124.html}
}

@article{gray_drones_2019,
  title = {Drones and Convolutional Neural Networks Facilitate Automated and Accurate Cetacean Species Identification and Photogrammetry},
  author = {Gray, Patrick C. and Bierlich, Kevin C. and Mantell, Sydney A. and Friedlaender, Ari S. and Goldbogen, Jeremy A. and Johnston, David W.},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {9},
  pages = {1490--1500},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13246},
  urldate = {2021-01-05},
  abstract = {The flourishing application of drones within marine science provides more opportunity to conduct photogrammetric studies on large and varied populations of many different species. While these new platforms are increasing the size and availability of imagery datasets, established photogrammetry methods require considerable manual input, allowing individual bias in techniques to influence measurements, increasing error and magnifying the time required to apply these techniques. Here, we introduce the next generation of photogrammetry methods utilizing a convolutional neural network to demonstrate the potential of a deep learning-based photogrammetry system for automatic species identification and measurement. We then present the same data analysed using conventional techniques to validate our automatic methods. Our results compare favorably across both techniques, correctly predicting whale species with 98\% accuracy (57/58) for humpback whales, minke whales, and blue whales. Ninety percent of automated length measurements were within 5\% of manual measurements, providing sufficient resolution to inform morphometric studies and establish size classes of whales automatically. The results of this study indicate that deep learning techniques applied to survey programs that collect large archives of imagery may help researchers and managers move quickly past analytical bottlenecks and provide more time for abundance estimation, distributional research, and ecological assessments.},
  copyright = {\textcopyright{} 2019 The Authors. Methods in Ecology and Evolution \textcopyright{} 2019 British Ecological Society},
  langid = {english},
  keywords = {cetaceans,convolutional neural network,deep learning,drones,photogrammetry,population assessments,species identification,unoccupied aerial systems},
  file = {/Users/b3020111/Zotero/storage/Z9XPXIBS/Gray et al. - 2019 - Drones and convolutional neural networks facilitat.pdf;/Users/b3020111/Zotero/storage/6BFVDSXJ/2041-210X.html}
}

@article{griffin_caltech-256_2007,
  title = {Caltech-256 {{Object Category Dataset}}},
  author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
  year = {2007},
  month = mar,
  urldate = {2019-06-05},
  abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
  file = {/Users/b3020111/Zotero/storage/5GCVQRFP/Griffin et al. - 2007 - Caltech-256 Object Category Dataset.pdf;/Users/b3020111/Zotero/storage/6MHTRL5S/7694.html}
}

@misc{haimeh_haimehfinfindr_2020,
  title = {Haimeh/{{finFindR}}},
  author = {{haimeh}},
  year = {2020},
  month = feb,
  urldate = {2020-02-12},
  abstract = {An application for dorsal fin image recognition and cataloguing},
  keywords = {app,cpp,dolphin,r,recognition}
}

@article{hale_unsupervised_2012,
  title = {Unsupervised {{Threshold}} for {{Automatic Extraction}} of {{Dolphin Dorsal Fin Outlines}} from {{Digital Photographs}} in {{DARWIN}} ({{Digital Analysis}} and {{Recognition}} of {{Whale Images}} on a {{Network}})},
  author = {Hale, Scott A.},
  year = {2012},
  month = feb,
  journal = {arXiv:1202.4107 [cs]},
  eprint = {1202.4107},
  primaryclass = {cs},
  urldate = {2020-02-12},
  abstract = {At least two software packages---DARWIN, Eckerd College, and FinScan, Texas A\&M---exist to facilitate the identification of cetaceans---whales, dolphins, porpoises---based upon the naturally occurring features along the edges of their dorsal fins. Such identification is useful for biological studies of population, social interaction, migration, etc. The process whereby fin outlines are extracted in current fin-recognition software packages is manually intensive and represents a major user input bottleneck: it is both time consuming and visually fatiguing. This research aims to develop automated methods (employing unsupervised thresholding and morphological processing techniques) to extract cetacean dorsal fin outlines from digital photographs thereby reducing manual user input. Ideally, automatic outline generation will improve the overall user experience and improve the ability of the software to correctly identify cetaceans. Various transformations from color to gray space were examined to determine which produced a grayscale image in which a suitable threshold could be easily identified. To assist with unsupervised thresholding, a new metric was developed to evaluate the jaggedness of figures ("pixelarity") in an image after thresholding. The metric indicates how cleanly a threshold segments background and foreground elements and hence provides a good measure of the quality of a given threshold. This research results in successful extractions in roughly 93\% of images, and significantly reduces user-input time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,H.5.2,I.4.6},
  file = {/Users/b3020111/Zotero/storage/LN3QTPEQ/Hale - 2012 - Unsupervised Threshold for Automatic Extraction of.pdf;/Users/b3020111/Zotero/storage/HS82CUNS/1202.html}
}

@article{hammond_cetacean_2013,
  title = {Cetacean Abundance and Distribution in {{European Atlantic}} Shelf Waters to Inform Conservation and Management},
  author = {Hammond, Philip S. and Macleod, Kelly and Berggren, Per and Borchers, David L. and Burt, Louise and Ca{\~n}adas, Ana and Desportes, Genevi{\`e}ve and Donovan, Greg P. and Gilles, Anita and Gillespie, Douglas and Gordon, Jonathan and Hiby, Lex and Kuklik, Iwona and Leaper, Russell and Lehnert, Kristina and Leopold, Mardik and Lovell, Phil and {\O}ien, Nils and Paxton, Charles G. M. and Ridoux, Vincent and Rogan, Emer and Samarra, Filipa and Scheidat, Meike and Sequeira, Marina and Siebert, Ursula and Skov, Henrik and Swift, Ren{\'e} and Tasker, Mark L. and Teilmann, Jonas and Canneyt, Olivier Van and V{\'a}zquez, Jos{\'e} Antonio},
  year = {2013},
  month = aug,
  journal = {Biological Conservation},
  volume = {164},
  pages = {107--122},
  issn = {0006-3207},
  doi = {10.1016/j.biocon.2013.04.010},
  abstract = {The European Union (EU) Habitats Directive requires Member States to monitor and maintain at favourable conservation status those species identified to be in need of protection, including all cetaceans. In July 2005 we surveyed the entire EU Atlantic continental shelf to generate robust estimates of abundance for harbour porpoise and other cetacean species. The survey used line transect sampling methods and purpose built data collection equipment designed to minimise bias in estimates of abundance. Shipboard transects covered 19,725km in sea conditions {$\leqslant$}Beaufort 4 in an area of 1,005,743km2. Aerial transects covered 15,802km in good/moderate conditions ({$\leqslant$}Beaufort 3) in an area of 364,371km2. Thirteen cetacean species were recorded; abundance was estimated for harbour porpoise (375,358; CV=0.197), bottlenose dolphin (16,485; CV=0.422), white-beaked dolphin (16,536; CV=0.303), short-beaked common dolphin (56,221; CV=0.234) and minke whale (18,958; CV=0.347). Abundance in 2005 was similar to that estimated in July 1994 for harbour porpoise, white-beaked dolphin and minke whale in a comparable area. However, model-based density surfaces showed a marked difference in harbour porpoise distribution between 1994 and 2005. Our results allow EU Member States to discharge their responsibilities under the Habitats Directive and inform other international organisations concerning the assessment of conservation status of cetaceans and the impact of bycatch at a large spatial scale. The lack of evidence for a change in harbour porpoise abundance in EU waters as a whole does not exclude the possibility of an impact of bycatch in some areas. Monitoring bycatch and estimation of abundance continue to be essential.},
  keywords = {Bottlenose dolphin,Bycatch,Common dolphin,Conservation status,Habitats Directive,Harbour porpoise,Line transect sampling,Minke whale,North Sea,SCANS,White-beaked dolphin}
}

@article{hammond_estimating_2021,
  title = {Estimating the {{Abundance}} of {{Marine Mammal Populations}}},
  author = {Hammond, Philip S. and Francis, Tessa B. and Heinemann, Dennis and Long, Kristy J. and Moore, Jeffrey E. and Punt, Andr{\'e} E. and Reeves, Randall R. and Sep{\'u}lveda, Maritza and Sigur{\dh}sson, Gu{\dh}j{\'o}n M{\'a}r and Siple, Margaret C. and V{\'i}kingsson, G{\'i}sli and Wade, Paul R. and Williams, Rob and Zerbini, Alexandre N.},
  year = {2021},
  month = sep,
  journal = {Frontiers in Marine Science},
  volume = {8},
  pages = {735770},
  issn = {2296-7745},
  doi = {10.3389/fmars.2021.735770},
  urldate = {2023-03-23},
  abstract = {Motivated by the need to estimate the abundance of marine mammal populations to inform conservation assessments, especially relating to fishery bycatch, this paper provides background on abundance estimation and reviews the various methods available for pinnipeds, cetaceans and sirenians. We first give an ``entry-level'' introduction to abundance estimation, including fundamental concepts and the importance of recognizing sources of bias and obtaining a measure of precision. Each of the primary methods available to estimate abundance of marine mammals is then described, including data collection and analysis, common challenges in implementation, and the assumptions made, violation of which can lead to bias. The main method for estimating pinniped abundance is extrapolation of counts of animals (pups or all-ages) on land or ice to the whole population. Cetacean and sirenian abundance is primarily estimated from transect surveys conducted from ships, small boats or aircraft. If individuals of a species can be recognized from natural markings, mark-recapture analysis of photo-identification data can be used to estimate the number of animals using the study area. Throughout, we cite example studies that illustrate the methods described. To estimate the abundance of a marine mammal population, key issues include: defining the population to be estimated, considering candidate methods based on strengths and weaknesses in relation to a range of logistical and practical issues, being aware of the resources required to collect and analyze the data, and understanding the assumptions made. We conclude with a discussion of some practical issues, given the various challenges that arise during implementation.},
  file = {/Users/b3020111/Zotero/storage/DJWPJ9I6/Hammond et al. - 2021 - Estimating the Abundance of Marine Mammal Populati.pdf}
}

@article{hammond_individual_1990,
  title = {Individual Recognition of Cetaceans: Use of Photo-Identification and Other Techniques to Estimate Population Parameters: Incorporating the Proceedings of the Symposium and Workshop on Individual Recognition and the Estimation of Cetacean Population Parameters},
  author = {Hammond, Philip S. and Mizroch, Sally A. and Donovan, Greg P.},
  year = {1990},
  journal = {International Whaling Commission},
  volume = {12},
  file = {/Users/b3020111/Zotero/storage/S9XJLGL8/Hammond et al. - 1990 - Individual recognition of cetaceans use of photo-.pdf}
}

@article{hariharan_simultaneous_2014,
  title = {Simultaneous {{Detection}} and {{Segmentation}}},
  author = {Hariharan, Bharath and Arbel{\'a}ez, Pablo and Girshick, Ross and Malik, Jitendra},
  year = {2014},
  month = jul,
  journal = {arXiv:1407.1808 [cs]},
  eprint = {1407.1808},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, topdown figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16\% relative) over our baselines on SDS, a 5 point boost (10\% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/ZVZGIU2G/Hariharan et al. - 2014 - Simultaneous Detection and Segmentation.pdf}
}

@misc{harislqbal88_plotneuralnet_2018,
  title = {{{PlotNeuralNet}}},
  author = {Harislqbal88},
  year = {2018},
  howpublished = {Github}
}

@article{hartigan_algorithm_1979,
  title = {Algorithm {{AS}} 136: {{A K-Means Clustering Algorithm}}},
  shorttitle = {Algorithm {{AS}} 136},
  author = {Hartigan, J. A. and Wong, M. A.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {28},
  number = {1},
  eprint = {2346830},
  eprinttype = {jstor},
  pages = {100--108},
  issn = {0035-9254},
  doi = {10.2307/2346830},
  urldate = {2019-08-08}
}

@article{he_bag_2018,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.01187 [cs]},
  eprint = {1812.01187},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/8FH5QY53/He et al. - 2018 - Bag of Tricks for Image Classification with Convol.pdf;/Users/b3020111/Zotero/storage/M9KGD67C/1812.html}
}

@article{he_deep_2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,resnet50},
  file = {/Users/b3020111/Zotero/storage/7STEMDJW/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/b3020111/Zotero/storage/AMDEJUSK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/b3020111/Zotero/storage/U79QHJ2E/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/b3020111/Zotero/storage/QE5S4JJE/1512.html;/Users/b3020111/Zotero/storage/RMK4EIHR/1512.html}
}

@article{he_delving_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.01852 [cs]},
  eprint = {1502.01852},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/X5CNZA7E/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/Users/b3020111/Zotero/storage/FU2Y55LH/1502.html}
}

@article{he_mask_2017,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.06870 [cs]},
  eprint = {1703.06870},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/N7QKA2DD/He et al. - 2017 - Mask R-CNN.pdf;/Users/b3020111/Zotero/storage/ZZY6ZAVB/1703.html}
}

@incollection{hecht-nielsen_iii.3_1992,
  title = {{{III}}.3 - {{Theory}} of the {{Backpropagation Neural Network}}**{{Based}} on ``Nonindent'' by {{Robert Hecht-Nielsen}}, Which Appeared in {{Proceedings}} of the {{International Joint Conference}} on {{Neural Networks}} 1, 593\textendash 611, {{June}} 1989. \textcopyright{} 1989 {{IEEE}}.},
  booktitle = {Neural {{Networks}} for {{Perception}}},
  author = {{Hecht-nielsen}, {\relax ROBERT}},
  editor = {Wechsler, Harry},
  year = {1992},
  month = jan,
  pages = {65--93},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-741252-8.50010-8},
  urldate = {2019-08-08},
  abstract = {This chapter presents a survey of the elementary theory of the basic backpropagation neural network architecture, covering the areas of architectural design, performance measurement, function approximation capability, and learning. The survey includes a formulation of the backpropagation neural network architecture to make it a valid neural network and a proof that the backpropagation mean squared error function exists and is differentiable. Also included in the survey is a theorem showing that any L2 function can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. An appendix presents a speculative neurophysiological model illustrating the way in which the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of cerebral cortex. One of the crucial decisions in the design of the backpropagation architecture is the selection of a sigmoidal activation function.},
  isbn = {978-0-12-741252-8},
  file = {/Users/b3020111/Zotero/storage/EU4UZNPJ/B9780127412528500108.html}
}

@article{hermans_defense_2017,
  title = {In {{Defense}} of the {{Triplet Loss}} for {{Person Re-Identification}}},
  author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  year = {2017},
  month = nov,
  journal = {arXiv:1703.07737 [cs.CV]},
  eprint = {1703.07737},
  primaryclass = {cs.CV},
  urldate = {2022-07-01},
  abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person reidentification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/b3020111/Zotero/storage/ICHRVIWH/Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf}
}

@inproceedings{hillman_finscan_2002,
  title = {"{{Finscan}}", a Computer System for Photographic Identification of Marine Animals},
  booktitle = {Proceedings of the {{Second Joint}} 24th {{Annual Conference}} and the {{Annual Fall Meeting}} of the {{Biomedical Engineering Society}}] [{{Engineering}} in {{Medicine}} and {{Biology}}},
  author = {Hillman, G.R. and Kehtarnavaz, N. and Wursig, B. and Araabi, B. and Gailey, G. and Weller, D. and Mandava, S. and Tagare, H.},
  year = {2002},
  month = oct,
  volume = {2},
  pages = {1065-1066 vol.2},
  issn = {1094-687X},
  doi = {10.1109/IEMBS.2002.1106279},
  abstract = {A system has been developed for computer-assisted photographic identification of marine animals. The system creates and maintains a database of images of dorsal fins or flukes, and the user queries it by entering a new image acquired in the field. The system searches the database for similar images based on the notching pattern of the dorsal fin or fluke, and offers a list of database members ordered by similarity to the query image. A syntactic/semantic string representation method is used for matching and is compared with matching by direct alignment of edge patterns. The system has been tested with data sets of images of several biological species.},
  keywords = {Biomedical imaging,Catalogs,computer-assisted photographic identification,Dolphins,dorsal fins,Finscan,flukes,image database,Image databases,image recognition,marine animals,Marine animals,Medical diagnostic imaging,notching pattern,Pattern matching,pattern recognition,photographic identification,string matching,syntactic/semantic string representation method,Testing,Tin,visual databases,Whales,zoology},
  file = {/Users/b3020111/Zotero/storage/MGKTF6VT/Hillman et al. - 2002 - Finscan, a computer system for photographic iden.pdf;/Users/b3020111/Zotero/storage/Z7F868QQ/1106279.html}
}

@article{hinton_discovering_2011,
  title = {Discovering {{Binary Codes}} for {{Documents}} by {{Learning Deep Generative Models}}: {{Topics}} in {{Cognitive Science}}(2010)},
  shorttitle = {Discovering {{Binary Codes}} for {{Documents}} by {{Learning Deep Generative Models}}},
  author = {Hinton, Geoffrey and Salakhutdinov, Ruslan},
  year = {2011},
  month = jan,
  journal = {Topics in Cognitive Science},
  volume = {3},
  number = {1},
  pages = {74--91},
  issn = {17568757},
  doi = {10.1111/j.1756-8765.2010.01109.x},
  urldate = {2019-08-30},
  langid = {english}
}

@article{hinton_optimal_1983,
  title = {Optimal {{Perceptual Inference}}},
  author = {Hinton, G E and Sejnowski, T J},
  year = {1983},
  pages = {6},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/QTT5L8R4/Hinton and Sejnowski - Optimal Perceptual Inference.pdf}
}

@techreport{hobbs_bowhead_1982,
  title = {Bowhead {{Whale Radio Tagging Feasibility Study}} and {{Review}} of {{Large Cetacean Tagging}}},
  author = {Hobbs, Larry J},
  year = {1982},
  number = {NMFS F/NWC-21},
  pages = {74},
  institution = {{NOAA}},
  abstract = {This report reviews marking and tagging techniques, their feasibility, success, and history of employment on large cetaceans. Static tags, freeze branding, paint marking, natural marks, and sonic tags are discussed. Emphasis is placed on radio tags. Three radio tracking systems and four types of radio transmitter attachments currently available for large cetaceans are evaluated and discussed. Results of a feasibility study using a VHF radio tracking system on bowhead whales are presented. On 20 and 21 August 1981 radio tags were deployed on two bowhead whales (Balaena mysticetus) in the eastern Beaufort Sea (69\textordmasculine 54'N x 132\textdegree 12'W). From one whale, signals were received intermittently for 10 min, the other, for one and one-half hours. Reliable dive-surface profiles of tagged whales from these transmissions were not possible. However, dive-surface profiles are reported for a bowhead whale identifiable by natural marks. Efforts to relocate tagged whales from ship and three aerial receiving stations were unsuccessful. Aerial surveys were flown from 20 July through 12 September, initially to locate whales but ultimately to relocate and track tagged animals. Efforts to relocate tagged whales continued from 16 September through 13 October in collaboration with a BLM (Bureau of Land Management) bowhead survey team working in OCS (Outer Continental Shelf) lease-sale areas. A brief radio transmission was received during one of these surveys but the presence of a tagged whale was unconfirmed by either further transmission or visual relocation. A record of all species of marine mammals sighted on surveys is presented. The development of a satellite-linked transmitter and requirements for a successful satellite tracking program are discussed.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/93MFL9I3/Hobbs - Bowhead Whale Radio Tagging Feasibility Study and .pdf}
}

@article{hochreiter_long_1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2022-09-27},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/FL7W4M4J/download.pdf}
}

@article{hochreiter_long_1997-1,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2022-09-27},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english}
}

@article{hoffer_deep_2018,
  title = {Deep Metric Learning Using {{Triplet}} Network},
  author = {Hoffer, Elad and Ailon, Nir},
  year = {2018},
  month = dec,
  journal = {arXiv:1412.6622 [cs, stat]},
  eprint = {1412.6622},
  primaryclass = {cs, stat},
  urldate = {2021-01-15},
  abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/X82R8HSL/Hoffer and Ailon - 2018 - Deep metric learning using Triplet network.pdf;/Users/b3020111/Zotero/storage/9DDC5E5B/1412.html}
}

@article{hoffer_deep_2018-1,
  title = {Deep Metric Learning Using {{Triplet}} Network},
  author = {Hoffer, Elad and Ailon, Nir},
  year = {2018},
  month = dec,
  journal = {arXiv:1412.6622 [cs, stat]},
  eprint = {1412.6622},
  primaryclass = {cs, stat},
  urldate = {2021-01-15},
  abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/G3XWSXXY/Hoffer and Ailon - 2018 - Deep metric learning using Triplet network.pdf}
}

@article{hoffer_deep_2018-2,
  title = {Deep Metric Learning Using {{Triplet}} Network},
  author = {Hoffer, Elad and Ailon, Nir},
  year = {2018},
  month = dec,
  journal = {arXiv:1412.6622 [cs, stat]},
  eprint = {1412.6622},
  primaryclass = {cs, stat},
  urldate = {2021-01-15},
  abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/583CKDEH/Hoffer and Ailon - 2018 - Deep metric learning using Triplet network.pdf}
}

@article{hoffer_deep_2018-3,
  title = {Deep Metric Learning Using {{Triplet}} Network},
  author = {Hoffer, Elad and Ailon, Nir},
  year = {2018},
  month = dec,
  journal = {arXiv:1412.6622 [cs, stat]},
  eprint = {1412.6622},
  primaryclass = {cs, stat},
  urldate = {2021-01-15},
  abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/FG484B49/Hoffer and Ailon - 2018 - Deep metric learning using Triplet network.pdf}
}

@article{holmberg_estimating_2009,
  title = {Estimating Population Size, Structure, and Residency Time for Whale Sharks {{Rhincodon}} Typus through Collaborative Photo-Identification},
  author = {Holmberg, Jason and Norman, Bradley and Arzoumanian, Zaven},
  year = {2009},
  month = apr,
  journal = {Endangered Species Research},
  volume = {7},
  number = {1},
  pages = {39--53},
  issn = {1863-5407, 1613-4796},
  doi = {10.3354/esr00186},
  urldate = {2021-01-07},
  abstract = {Capture-mark-recapture (CMR) data from Ningaloo Marine Park (NMP) in Western Australia have recently been used to study the population dynamics of the local whale shark aggregation. Because nascent research efforts at other aggregation points look to NMP as a model, further analysis of existing modeling approaches is important. We have expanded upon previous studies of NMP whale sharks by estimating CMR survival and recruitment rates as functions of average total length (TL). Our analysis suggests a decline in reported values of TL coincident with marginally increasing abundance among sharks sighted in more than one year (`returning') from 1995 to 2008. We found a positive, average returning recruitment rate ({$\lambda$}) of 1.07 yr\textendash 1 (0.99 to 1.15, 95\% CI); smaller individuals contributed in larger numbers to recruitment, allowing for population growth accompanied by a decline in median size. We subsequently explored intraseasonal population dynamics with the Open Robust Design (ORD) model structure. Our best-fit model estimated modestly increasing annual abundances between 107 (95\% CI = 90 to 124) and 159 (95\% CI = 127 to 190) for 2004 to 2007, suggesting a short-term increase in total annual abundance. The ORD also estimated an average residency time of 33 d (95\% CI = 31 to 39) and biweekly entry profiles into the study area. Overall, our techniques demonstrate how large aggregations of the species can be modeled to better understand short- and long-term population trends. These results also show the direct scientific benefit from the development of an online, collaborative data management system to increase collection of sighting data for a rare species in conjunction with ecotourism activity.},
  langid = {english},
  keywords = {Mark-recapture,Open Robust Design,Population biology,Rhincodon typus,Survivorship,Transience,Whale shark},
  file = {/Users/b3020111/Zotero/storage/9C3QCLEB/Holmberg et al. - 2009 - Estimating population size, structure, and residen.pdf;/Users/b3020111/Zotero/storage/KUFM9XAI/p39-53.html}
}

@article{hong_trashcan_2020,
  title = {{{TrashCan}}: {{A Semantically-Segmented Dataset}} towards {{Visual Detection}} of {{Marine Debris}}},
  shorttitle = {{{TrashCan}}},
  author = {Hong, Jungseok and Fulton, Michael and Sattar, Junaed},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.08097 [cs]},
  eprint = {2007.08097},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {This paper presents TrashCan, a large dataset comprised of images of underwater trash collected from a variety of sources, annotated both using bounding boxes and segmentation labels, for development of robust detectors of marine debris. The dataset has two versions, TrashCan-Material and TrashCan-Instance, corresponding to different object class configurations. The eventual goal is to develop efficient and accurate trash detection methods suitable for onboard robot deployment. Along with information about the construction and sourcing of the TrashCan dataset, we present initial results of instance segmentation from Mask R-CNN and object detection from Faster R-CNN. These do not represent the best possible detection results but provides an initial baseline for future work in instance segmentation and object detection on the TrashCan dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/b3020111/Zotero/storage/46FENFGR/Hong et al. - 2020 - TrashCan A Semantically-Segmented Dataset towards.pdf;/Users/b3020111/Zotero/storage/SUQTT2NA/2007.html}
}

@misc{hron_variational_2018,
  title = {Variational {{Bayesian}} Dropout: Pitfalls and Fixes},
  shorttitle = {Variational {{Bayesian}} Dropout},
  author = {Hron, Jiri and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
  year = {2018},
  month = jul,
  number = {arXiv:1807.01969},
  eprint = {1807.01969},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-06-24},
  abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKLoptimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/HGYUVC5B/Hron et al. - 2018 - Variational Bayesian dropout pitfalls and fixes.pdf}
}

@inproceedings{hsu_dolphin_2018,
  title = {Dolphin {{Recognition}} with {{Adaptive Hybrid Saliency Detection}} for {{Deep Learning Based}} on {{DenseNet Recognition}}},
  booktitle = {2018 {{IEEE Asia Pacific Conference}} on {{Circuits}} and {{Systems}} ({{APCCAS}})},
  author = {Hsu, Hung-Wei and Lee, Yih-Cherng and Ding, Jian-Jiun and Chang, Ronald Y.},
  year = {2018},
  month = oct,
  pages = {455--458},
  doi = {10.1109/APCCAS.2018.8605718},
  abstract = {Dolphin identification is important for wildlife conservation. Since identifying dolphins from thousands of images manually takes tremendous time, it is important to develop an automatic dolphin identification algorithm. In this paper, a high accurate deep learning based dolphin identification algorithm is proposed. We presented an advanced approach, called hybrid saliency method, for feature extraction and efficiently integrate several well-known techniques to make dolphins distinguishable. With the proposed techniques, we can avoid the background part (e.g. the sea water) to affect the identification results, which is usually a problem of most convolutional neural network based methods. Simulations show that the proposed algorithm can well identify a dolphin in most cases and it can achieve the accuracy rate of 85\% even if there are 40 dolphins to be distinguished.},
  keywords = {acoustic signal detection,adaptive hybrid saliency detection,automatic dolphin identification algorithm,biology computing,computer vision,convolutional neural networks,DenseNet recognition,dolphin recognition,Dolphins,feature extraction,feedforward neural nets,high accurate deep learning,hybrid saliency method,identification results,Image color analysis,image recognition,image sensors,learning (artificial intelligence),marine vertebrate,photo-identification,photography,Prediction algorithms,saliency map,Sea surface,Testing,Training,tremendous time,Whales,wildlife conservation,zoology},
  file = {/Users/b3020111/Zotero/storage/D3RUMQE2/Hsu et al. - 2018 - Dolphin Recognition with Adaptive Hybrid Saliency .pdf}
}

@article{hu_istr_2021,
  title = {{{ISTR}}: {{End-to-End Instance Segmentation}} with {{Transformers}}},
  shorttitle = {{{ISTR}}},
  author = {Hu, Jie and Cao, Liujuan and Lu, Yao and Zhang, ShengChuan and Wang, Yan and Li, Ke and Huang, Feiyue and Shao, Ling and Ji, Rongrong},
  year = {2021},
  month = may,
  journal = {arXiv:2105.00637 [cs.CV]},
  eprint = {2105.00637},
  primaryclass = {cs.CV},
  urldate = {2022-09-27},
  abstract = {End-to-end paradigms significantly improve the accuracy of various deep-learning-based computer vision models. To this end, tasks like object detection have been upgraded by replacing non-end-to-end components, such as removing non-maximum suppression by training with a set loss based on bipartite matching. However, such an upgrade is not applicable to instance segmentation, due to its significantly higher output dimensions compared to object detection. In this paper, we propose an instance segmentation Transformer, termed ISTR, which is the first end-to-end framework of its kind. ISTR predicts low-dimensional mask embeddings, and matches them with ground truth mask embeddings for the set loss. Besides, ISTR concurrently conducts detection and segmentation with a recurrent refinement strategy, which provides a new way to achieve instance segmentation compared to the existing top-down and bottom-up frameworks. Benefiting from the proposed endto-end mechanism, ISTR demonstrates state-of-the-art performance even with approximation-based suboptimal embeddings. Specifically, ISTR obtains a 46.8/38.6 box/mask AP using ResNet50-FPN, and a 48.1/39.9 box/mask AP using ResNet101-FPN, on the MS COCO dataset. Quantitative and qualitative results reveal the promising potential of ISTR as a solid baseline for instance-level recognition. Code has been made available at: https://github. com/hujiecpp/ISTR.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/5SXXUR6W/Hu et al. - 2021 - ISTR End-to-End Instance Segmentation with Transf.pdf}
}

@article{huang_densely_2016,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.06993 [cs]},
  eprint = {1608.06993},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/K3U93XP6/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf;/Users/b3020111/Zotero/storage/WIUXMLVY/1608.html}
}

@article{huang_faster_2019,
  title = {Faster {{R-CNN}} for Marine Organisms Detection and Recognition Using Data Augmentation},
  author = {Huang, Hai and Zhou, Hao and Yang, Xu and Zhang, Lu and Qi, Lu and Zang, Ai-Yun},
  year = {2019},
  month = apr,
  journal = {Neurocomputing},
  volume = {337},
  pages = {372--384},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.01.084},
  urldate = {2020-12-03},
  abstract = {Recently, Faster Region-based Convolutional Neural Network (Faster R-CNN) has achieved marvelous accomplishment in object detection and recognition. In this paper, Faster R-CNN is applied to marine organisms detection and recognition. However, the training of Faster R-CNN requires a mass of labeled samples which are difficult to obtain for marine organisms. Therefore, three data augmentation methods dedicated to underwater-imaging are proposed. Specifically, the inverse process of underwater image restoration is used to simulate different marine turbulence environments. Perspective transformation is proposed to simulate different views of camera shooting. Illumination synthesis is used to simulate different marine uneven illuminating environments. The performance of each data augmentation method, together with previous frequently used data augmentation methods are evaluated by Faster R-CNN on the real-world underwater dataset, which validate the effectiveness of the proposed methods for marine organisms detection and recognition.},
  langid = {english},
  keywords = {Data augmentation,Detection and recognition,Faster R-CNN,Marine organisms,Underwater-imaging},
  file = {/Users/b3020111/Zotero/storage/X9ZURVZG/Huang et al. - 2019 - Faster R-CNN for marine organisms detection and re.pdf}
}

@inproceedings{huang_sbsgan_2019,
  title = {{{SBSGAN}}: {{Suppression}} of {{Inter-Domain Background Shift}} for {{Person Re-Identification}}},
  shorttitle = {{{SBSGAN}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Huang, Yan and Wu, Qiang and Xu, Jingsong and Zhong, Yi},
  year = {2019},
  month = oct,
  pages = {9526--9535},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00962},
  urldate = {2022-09-27},
  abstract = {Cross-domain person re-identification (re-ID) is challenging due to the bias between training and testing domains. We observe that if backgrounds in the training and testing datasets are very different, it dramatically introduces difficulties to extract robust pedestrian features, and thus compromises the cross-domain person re-ID performance. In this paper, we formulate such problems as a background shift problem. A Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to generate images with suppressed backgrounds. Unlike simply removing backgrounds using binary masks, SBSGAN allows the generator to decide whether pixels should be preserved or suppressed to reduce segmentation errors caused by noisy foreground masks. Additionally, we take ID-related cues, such as vehicles and companions into consideration. With high-quality generated images, a Densely Associated 2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection (ISDC) modules to strengthen the complementarity of the generated data and ID-related cues. The experiments show that the proposed method achieves competitive performance on three re-ID datasets, i.e., Market1501, DukeMTMC-reID, and CUHK03, under the crossdomain person re-ID scenario.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ARDSN2HD/Huang et al. - 2019 - SBSGAN Suppression of Inter-Domain Background Shi.pdf}
}

@article{huang_snapshot_2017,
  title = {Snapshot {{Ensembles}}: {{Train}} 1, Get {{M}} for Free},
  shorttitle = {Snapshot {{Ensembles}}},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  year = {2017},
  month = mar,
  journal = {arXiv:1704.00109 [cs]},
  eprint = {1704.00109},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/RJ7LY6XI/Huang et al. - 2017 - Snapshot Ensembles Train 1, get M for free.pdf;/Users/b3020111/Zotero/storage/LQY9MDXI/1704.html}
}

@article{huang_speed/accuracy_2016,
  title = {Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors},
  author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.10012 [cs]},
  eprint = {1611.10012},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/TPUHAW9Q/Huang et al. - 2016 - Speedaccuracy trade-offs for modern convolutional.pdf;/Users/b3020111/Zotero/storage/6PSRKK4Z/1611.html}
}

@inproceedings{huang_speedaccuracy_2017,
  title = {Speed/{{Accuracy Trade-Offs}} for {{Modern Convolutional Object Detectors}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
  year = {2017},
  month = jul,
  pages = {3296--3297},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.351},
  urldate = {2021-01-13},
  abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as ``meta-architectures'' and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/TCVIHTYN/Huang et al. - 2017 - SpeedAccuracy Trade-Offs for Modern Convolutional.pdf}
}

@article{hughes_automated_2017,
  title = {Automated {{Visual Fin Identification}} of {{Individual Great White Sharks}}},
  author = {Hughes, Benjamin and Burghardt, Tilo},
  year = {2017},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {122},
  number = {3},
  pages = {542--557},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-016-0961-y},
  urldate = {2019-03-14},
  abstract = {This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images. To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics. The approach put forward appreciates shark fins as textureless, flexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we first introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial, scale-space selective fingerprinting can successfully encode fin individuality. We then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global `fin space'. Exploiting this domain, we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/R5KW3Y26/Hughes and Burghardt - 2017 - Automated Visual Fin Identification of Individual .pdf}
}

@book{institute_of_electrical_and_electronics_engineers_2009_2009,
  title = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}: {{CVPR}} 2009 ; {{Miami}} [{{Beach}}], {{Florida}}, {{USA}}, 20 - 25 {{June}} 2009},
  shorttitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  editor = {{Institute of Electrical {and} Electronics Engineers} and IEEE Computer Society},
  year = {2009},
  publisher = {{IEEE}},
  address = {{Piscataway, NJ}},
  isbn = {978-1-4244-3992-8 978-1-4244-3991-1},
  langid = {english}
}

@article{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.03167 [cs]},
  eprint = {1502.03167},
  primaryclass = {cs},
  urldate = {2019-06-04},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/JN2GN9LU/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/Users/b3020111/Zotero/storage/PXJLUU3K/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/Users/b3020111/Zotero/storage/LMBHZT8H/1502.html;/Users/b3020111/Zotero/storage/T6VEIVFW/1502.html}
}

@book{iucn_iucn_2012,
  title = {The {{IUCN}} Red List of Threatened Species},
  author = {IUCN},
  year = {2012}
}

@inproceedings{jackson_camera_2021,
  title = {Camera {{Bias}} in a {{Fine Grained Classification Task}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Jackson, Philip T. and Bonner, Stephen and Jia, Ning and Holder, Christopher and Stonehouse, Jon and Obara, Boguslaw},
  year = {2021},
  month = jul,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Shenzhen, China}},
  doi = {10.1109/IJCNN52387.2021.9534097},
  urldate = {2023-03-16},
  abstract = {We show that correlations between the camera used to acquire an image and the class label of that image can be exploited by convolutional neural networks (CNN), resulting in a model that ``cheats'' at an image classification task by recognizing which camera took the image and inferring the class label from the camera. We show that models trained on a dataset with camera / label correlations do not generalize well to images in which those correlations are absent, nor to images from unencountered cameras. Furthermore, we investigate which visual features they are exploiting for camera recognition. Our experiments present evidence against the importance of global color statistics, lens deformation and chromatic aberration, and in favor of high frequency features, which may be introduced by image processing algorithms built into the cameras.},
  isbn = {978-1-66543-900-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/GM5AT8H9/Jackson et al. - 2021 - Camera Bias in a Fine Grained Classification Task.pdf}
}

@inproceedings{jackson_phenotypic_2019,
  title = {Phenotypic {{Profiling}} of {{High Throughput Imaging Screens}} with {{Generic Deep Convolutional Features}}},
  booktitle = {2019 16th {{International Conference}} on {{Machine Vision Applications}} ({{MVA}})},
  author = {Jackson, Philip T. and Wang, Yinhai and Knight, Sinead and Chen, Hongming and Dorval, Thierry and Brown, Martin and Bendtsen, Claus and Obara, Boguslaw},
  year = {2019},
  month = may,
  pages = {1--4},
  doi = {10.23919/MVA.2019.8757871},
  abstract = {While deep learning has seen many recent applications to drug discovery, most have focused on predicting activity or toxicity directly from chemical structure. Phenotypic changes exhibited in cellular images are also indications of the mechanism of action (MoA) of chemical compounds. In this paper, we show how pre-trained convolutional image features can be used to assist scientists in discovering interesting chemical clusters for further investigation. Our method reduces the dimensionality of raw fluorescent stained images from a high throughput imaging (HTI) screen, producing an embedding space that groups together images with similar cellular phenotypes. Running standard unsupervised clustering on this embedding space yields a set of distinct phenotypic clusters. This allows scientists to further select and focus on interesting clusters for downstream analyses. We validate the consistency of our embedding space qualitatively with t-sne visualizations, and quantitatively by measuring embedding variance among images that are known to be similar. Results suggested the usefulness of our proposed workflow using deep learning and clustering and it can lead to robust HTI screening and compound triage.},
  keywords = {Bridges,Data models,Gallium nitride,Generative adversarial networks,Generators,Neural networks,Social networking (online)},
  file = {/Users/b3020111/Zotero/storage/2GJI3HU5/Jackson et al. - 2019 - Phenotypic Profiling of High Throughput Imaging Sc.pdf}
}

@article{jaderberg_population_2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.09846 [cs]},
  eprint = {1711.09846},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \textbackslash emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/b3020111/Zotero/storage/7MFTI52F/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf;/Users/b3020111/Zotero/storage/BIR9BCMK/1711.html}
}

@misc{jocher_githubcomultralyticsyolov5treev70_2022,
  title = {Github.Com/Ultralytics/Yolov5/Tree/v7.0 - {{YOLOv5 SOTA Realtime Instance Segmentation}}},
  shorttitle = {Ultralytics/Yolov5},
  author = {Jocher, Glenn and Ayush Chaurasia and Stoken, Alex and Borovec, Jirka and NanoCode012 and Yonghye Kwon and Kalen Michael and TaoXie and Jiacong Fang and Imyhxy and Lorna and æ¾é¸å¤«(Zeng Yifu) and Wong, Colin and Abhiram V and Montes, Diego and Zhiqiang Wang and Fati, Cristi and Jebastin Nadar and Laughing and UnglvKitDe and Sonck, Victor and Tkianai and YxNONG and Skalski, Piotr and Hogan, Adam and Dhruv Nair and Strobel, Max and Jain, Mrinal},
  year = {2022},
  month = nov,
  doi = {10.5281/ZENODO.7347926},
  urldate = {2023-04-27},
  abstract = {\&lt;div align="center"\&gt; \&lt;a align="center" href="https://ultralytics.com/yolov5" target="\_blank"\&gt; \&lt;img width="850" src="https://github.com/ultralytics/assets/blob/master/yolov5/v70/splash.png"\&gt;\&lt;/a\&gt; \&lt;/div\&gt; \&lt;br\&gt; Our new YOLOv5 v7.0 instance segmentation models are the fastest and most accurate in the world, beating all current SOTA benchmarks. We've made them super simple to train, validate and deploy. See full details in our Release Notes and visit our YOLOv5 Segmentation Colab Notebook for quickstart tutorials. \&lt;div align="center"\&gt; \&lt;a align="center" href="https://ultralytics.com/yolov5" target="\_blank"\&gt; \&lt;img width="800" src="https://user-images.githubusercontent.com/26833433/203348073-9b85607b-03e2-48e1-a6ba-fe1c1c31749c.png"\&gt;\&lt;/a\&gt; \&lt;/div\&gt; \&lt;br\&gt; Our primary goal with this release is to introduce super simple YOLOv5 segmentation workflows just like our existing object detection models. The new v7.0 YOLOv5-seg models below are just a start, we will continue to improve these going forward together with our existing detection and classification models. We'd love your feedback and contributions on this effort! This release incorporates {$<$}strong{$>$}280 PRs{$<$}/strong{$>$} from {$<$}strong{$>$}41 contributors{$<$}/strong{$>$} since our last release in August 2022. Important Updates {$<$}strong{$>$}Segmentation Models {$\medwhitestar$} NEW{$<$}/strong{$>$}: SOTA YOLOv5-seg COCO-pretrained segmentation models are now available for the first time (https://github.com/ultralytics/yolov5/pull/9052 by @glenn-jocher, @AyushExel and @Laughing-q) {$<$}strong{$>$}Paddle Paddle Export{$<$}/strong{$>$}: Export any YOLOv5 model (cls, seg, det) to Paddle format with python export.py --include paddle (https://github.com/ultralytics/yolov5/pull/9459 by @glenn-jocher) {$<$}strong{$>$}YOLOv5 AutoCache{$<$}/strong{$>$}: Use {$<$}code{$>$}python train.py --cache ram{$<$}/code{$>$} will now scan available memory and compare against predicted dataset RAM usage. This reduces risk in caching and should help improve adoption of the dataset caching feature, which can significantly speed up training. (https://github.com/ultralytics/yolov5/pull/10027 by @glenn-jocher) {$<$}strong{$>$}Comet Logging and Visualization Integration:{$<$}/strong{$>$} Free forever, Comet lets you save YOLOv5 models, resume training, and interactively visualise and debug predictions. (https://github.com/ultralytics/yolov5/pull/9232 by @DN6) New Segmentation Checkpoints We trained YOLOv5 segmentations models on COCO for 300 epochs at image size 640 using A100 GPUs. We exported all models to ONNX FP32 for CPU speed tests and to TensorRT FP16 for GPU speed tests. We ran all speed tests on Google Colab Pro notebooks for easy reproducibility. Model size\&lt;br\&gt;\&lt;sup\&gt;(pixels) mAP\&lt;sup\&gt;box\&lt;br\&gt;50-95 mAP\&lt;sup\&gt;mask\&lt;br\&gt;50-95 Train time\&lt;br\&gt;\&lt;sup\&gt;300 epochs\&lt;br\&gt;A100 (hours) Speed\&lt;br\&gt;\&lt;sup\&gt;ONNX CPU\&lt;br\&gt;(ms) Speed\&lt;br\&gt;\&lt;sup\&gt;TRT A100\&lt;br\&gt;(ms) params\&lt;br\&gt;\&lt;sup\&gt;(M) FLOPs\&lt;br\&gt;\&lt;sup\&gt;@640 (B) YOLOv5n-seg 640 27.6 23.4 80:17 {$<$}strong{$>$}62.7{$<$}/strong{$>$} {$<$}strong{$>$}1.2{$<$}/strong{$>$} {$<$}strong{$>$}2.0{$<$}/strong{$>$} {$<$}strong{$>$}7.1{$<$}/strong{$>$} YOLOv5s-seg 640 37.6 31.7 88:16 173.3 1.4 7.6 26.4 YOLOv5m-seg 640 45.0 37.1 108:36 427.0 2.2 22.0 70.8 YOLOv5l-seg 640 49.0 39.9 66:43 (2x) 857.4 2.9 47.9 147.7 YOLOv5x-seg 640 {$<$}strong{$>$}50.7{$<$}/strong{$>$} {$<$}strong{$>$}41.4{$<$}/strong{$>$} 62:56 (3x) 1579.2 4.5 88.8 265.7 All checkpoints are trained to 300 epochs with SGD optimizer with {$<$}code{$>$}lr0=0.01{$<$}/code{$>$} and {$<$}code{$>$}weight\_decay=5e-5{$<$}/code{$>$} at image size 640 and all default settings.\&lt;br\&gt;Runs logged to https://wandb.ai/glenn-jocher/YOLOv5\_v70\_official {$<$}strong{$>$}Accuracy{$<$}/strong{$>$} values are for single-model single-scale on COCO dataset.\&lt;br\&gt;Reproduce by {$<$}code{$>$}python segment/val.py --data coco.yaml --weights yolov5s-seg.pt{$<$}/code{$>$} {$<$}strong{$>$}Speed{$<$}/strong{$>$} averaged over 100 inference images using a Colab Pro A100 High-RAM instance. Values indicate inference speed only (NMS adds about 1ms per image). \&lt;br\&gt;Reproduce by {$<$}code{$>$}python segment/val.py --data coco.yaml --weights yolov5s-seg.pt --batch 1{$<$}/code{$>$} {$<$}strong{$>$}Export{$<$}/strong{$>$} to ONNX at FP32 and TensorRT at FP16 done with {$<$}code{$>$}export.py{$<$}/code{$>$}. \&lt;br\&gt;Reproduce by {$<$}code{$>$}python export.py --weights yolov5s-seg.pt --include engine --device 0 --half{$<$}/code{$>$} New Segmentation Usage Examples Train YOLOv5 segmentation training supports auto-download COCO128-seg segmentation dataset with {$<$}code{$>$}--data coco128-seg.yaml{$<$}/code{$>$} argument and manual download of COCO-segments dataset with {$<$}code{$>$}bash data/scripts/get\_coco.sh --train --val --segments{$<$}/code{$>$} and then {$<$}code{$>$}python train.py --data coco.yaml{$<$}/code{$>$}. &lt;code class="lang-bash"&gt;# Single-GPU python segment/train.py --model yolov5s-seg.pt --data coco128-seg.yaml --epochs 5 --img 640 # Multi-GPU DDP python -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --model yolov5s-seg.pt --data coco128-seg.yaml --epochs 5 --img 640 --device 0,1,2,3 &lt;/code&gt; Val Validate YOLOv5m-seg accuracy on ImageNet-1k dataset: &lt;code class="lang-bash"&gt;bash data/scripts/get_coco.sh --val --segments # download COCO val segments split (780MB, 5000 images) python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640 # validate &lt;/code&gt; Predict Use pretrained YOLOv5m-seg to predict bus.jpg: &lt;code class="lang-bash"&gt;python segment/predict.py --weights yolov5m-seg.pt --data data/images/bus.jpg &lt;/code&gt; &lt;code class="lang-python"&gt;model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m-seg.pt') # load from PyTorch Hub (WARNING: inference not yet supported) &lt;/code&gt; Export Export YOLOv5s-seg model to ONNX and TensorRT: &lt;code class="lang-bash"&gt;python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0 &lt;/code&gt; Changelog Changes between {$<$}strong{$>$}previous release and this release{$<$}/strong{$>$}: https://github.com/ultralytics/yolov5/compare/v6.2...v7.0 Changes {$<$}strong{$>$}since this release{$<$}/strong{$>$}: https://github.com/ultralytics/yolov5/compare/v7.0...HEAD \&lt;details\&gt; \&lt;summary\&gt;ð ï¸ New Features and Bug Fixes (280)\&lt;/summary\&gt; * Improve classification comments by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/8997 * Update `attempt\_download(release='v6.2')` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/8998 * Update README\_cn.md by @KieraMengru0907 in https://github.com/ultralytics/yolov5/pull/9001 * Update dataset `names` from array to dictionary by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9000 * [segment]: Allow inference on dirs and videos by @AyushExel in https://github.com/ultralytics/yolov5/pull/9003 * DockerHub tag update Usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9005 * Add weight `decay` to argparser by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9006 * Add glob quotes to detect.py usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9007 * Fix TorchScript JSON string key bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9015 * EMA FP32 assert classification bug fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9016 * Faster pre-processing for gray image input by @cher-liang in https://github.com/ultralytics/yolov5/pull/9009 * Improved `Profile()` inference timing by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9024 * `torch.empty()` for speed improvements by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9025 * Remove unused `time\_sync` import by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9026 * Add PyTorch Hub classification CI checks by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9027 * Attach transforms to model by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9028 * Default --data `imagenette160` training (fastest) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9033 * VOC `names` dictionary fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9034 * Update train.py `import val as validate` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9037 * AutoBatch protect from negative batch sizes by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9048 * Temporarily remove `macos-latest` from CI by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9049 * Add `--save-hybrid` mAP warning by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9050 * Refactor for simplification by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9054 * Refactor for simplification 2 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9055 * zero-mAP fix return `.detach()` to EMA by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9056 * zero-mAP fix 3 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9058 * Daemon `plot\_labels()` for faster start by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9057 * TensorBoard fix in tutorial.ipynb by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9064 * zero-mAP fix remove `torch.empty()` forward pass in `.train()` mode by @0zppd in https://github.com/ultralytics/yolov5/pull/9068 * Rename 'labels' to 'instances' by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9066 * Threaded TensorBoard graph logging by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9070 * De-thread TensorBoard graph logging by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9071 * Two dimensional `size=(h,w)` AutoShape support by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9072 * Remove unused Timeout import by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9073 * Improved Usage example docstrings by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9075 * Install `torch` latest stable by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9092 * New `@try\_export` decorator by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9096 * Add optional `transforms` argument to LoadStreams() by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9105 * Streaming Classification support by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9106 * Fix numpy to torch cls streaming bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9112 * Infer Loggers project name by @AyushExel in https://github.com/ultralytics/yolov5/pull/9117 * Add CSV logging to GenericLogger by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9128 * New TryExcept decorator by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9154 * Fixed segment offsets by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9155 * New YOLOv5 v6.2 splash images by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9142 * Rename onnx\_dynamic -\&gt; dynamic by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9168 * Inline `\_make\_grid()` meshgrid by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9170 * Comment EMA assert by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9173 * Fix confidence threshold for ClearML debug images by @HighMans in https://github.com/ultralytics/yolov5/pull/9174 * Update Dockerfile-cpu by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9184 * Update Dockerfile-cpu to libpython3-dev by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9185 * Update Dockerfile-arm64 to libpython3-dev by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9187 * Fix AutoAnchor MPS bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9188 * Skip AMP check on MPS by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9189 * ClearML's set\_report\_period's time is defined in minutes not seconds. by @HighMans in https://github.com/ultralytics/yolov5/pull/9186 * Add `check\_git\_status(..., branch='master')` argument by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9199 * `check\_font()` on notebook init by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9200 * Comment `protobuf` in requirements.txt by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9207 * `check\_font()` fstring update by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9208 * AutoBatch protect from extreme batch sizes by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9209 * Default AutoBatch 0.8 fraction by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9212 * Delete rebase.yml by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9202 * Duplicate segment verification fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9225 * New `LetterBox(size)` `CenterCrop(size)`, `ToTensor()` transforms (\#9213) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9213 * Add ClassificationModel TF export assert by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9226 * Remove usage of `pathlib.Path.unlink(missing\_ok=...)` by @ymerkli in https://github.com/ultralytics/yolov5/pull/9227 * Add support for `*.pfm` images by @spacewalk01 in https://github.com/ultralytics/yolov5/pull/9230 * Python check warning emoji by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9238 * Add `url\_getsize()` function by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9247 * Update dataloaders.py by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9250 * Refactor Loggers : Move code outside train.py by @AyushExel in https://github.com/ultralytics/yolov5/pull/9241 * Update general.py by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9252 * Add LoadImages.\_cv2\_rotate() by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9249 * Move `cudnn.benchmarks(True)` to LoadStreams by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9258 * `cudnn.benchmark = True` on Seed 0 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9259 * Update `TryExcept(msg='...')`` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9261 * Make sure best.pt model file is preserved ClearML by @thepycoder in https://github.com/ultralytics/yolov5/pull/9265 * DetectMultiBackend improvements by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9269 * Update DetectMultiBackend for tuple outputs by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9274 * Update DetectMultiBackend for tuple outputs 2 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9275 * Update benchmarks CI with `--hard-fail` min metric floor by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9276 * Add new `--vid-stride` inference parameter for videos by @VELCpro in https://github.com/ultralytics/yolov5/pull/9256 * [pre-commit.ci] pre-commit suggestions by @pre-commit-ci in https://github.com/ultralytics/yolov5/pull/9295 * Replace deprecated `np.int` with `int` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9307 * Comet Logging and Visualization Integration by @DN6 in https://github.com/ultralytics/yolov5/pull/9232 * Comet changes by @DN6 in https://github.com/ultralytics/yolov5/pull/9328 * Train.py line 486 typo fix by @robinned in https://github.com/ultralytics/yolov5/pull/9330 * Add dilated conv support by @YellowAndGreen in https://github.com/ultralytics/yolov5/pull/9347 * Update `check\_requirements()` single install by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9353 * Update `check\_requirements(args, cmds='')` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9355 * Update `check\_requirements()` multiple string by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9356 * Add PaddlePaddle export and inference by @kisaragychihaya in https://github.com/ultralytics/yolov5/pull/9240 * PaddlePaddle Usage examples by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9358 * labels.jpg names fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9361 * Exclude `ipython` from hubconf.py `check\_requirements()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9362 * `torch.jit.trace()` fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9363 * AMP Check fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9367 * Remove duplicate line in setup.cfg by @zldrobit in https://github.com/ultralytics/yolov5/pull/9380 * Remove `.train()` mode exports by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9429 * Continue on Docker arm64 failure by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9430 * Continue on Docker failure (all backends) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9432 * Continue on Docker fail (all backends) fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9433 * YOLOv5 segmentation model support by @AyushExel in https://github.com/ultralytics/yolov5/pull/9052 * Fix val.py zero-TP bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9431 * New model.yaml `activation:` field by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9371 * Fix tick labels for background FN/FP by @hotohoto in https://github.com/ultralytics/yolov5/pull/9414 * Fix TensorRT exports to ONNX opset 12 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9441 * AutoShape explicit arguments fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9443 * Update Detections() insta},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@misc{kaggle_humpback_2018,
  title = {Humpback {{Whale Identification Challenge}}},
  author = {Kaggle},
  year = {2018},
  urldate = {2020-02-12},
  abstract = {Can you identify a whale by the picture of its fluke?},
  howpublished = {https://kaggle.com/c/whale-categorization-playground},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/PNAXMZVK/whale-categorization-playground.html}
}

@inproceedings{karnowski_dolphin_2015,
  title = {Dolphin {{Detection}} and {{Tracking}}},
  booktitle = {2015 {{IEEE Winter Applications}} and {{Computer Vision Workshops}}},
  author = {Karnowski, Jeremy and Hutchins, Edwin and Johnson, Christine},
  year = {2015},
  month = jan,
  pages = {51--56},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACVW.2015.10},
  urldate = {2019-01-14},
  isbn = {978-0-7695-5469-3}
}

@misc{karpathy_peek_2017,
  title = {A Peek at Trends in Machine Learning},
  author = {Karpathy, Andrej},
  year = {2017},
  journal = {A peek at trends in machine learning},
  urldate = {2021-06-29},
  howpublished = {https://karpathy.medium.com/a-peek-at-trends-in-machine-learning-ab8a1085a106},
  langid = {english}
}

@article{katija_fathomnet_2022,
  title = {{{FathomNet}}: {{A}} Global Image Database for Enabling Artificial Intelligence in the Ocean},
  shorttitle = {{{FathomNet}}},
  author = {Katija, Kakani and Orenstein, Eric and Schlining, Brian and Lundsten, Lonny and Barnard, Kevin and Sainz, Giovanna and Boulais, Oceane and Cromwell, Megan and Butler, Erin and Woodward, Benjamin and Bell, Katherine L. C.},
  year = {2022},
  month = sep,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {15914},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-19939-2},
  urldate = {2022-09-28},
  abstract = {Abstract             The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. As baselines are sought by the research community, the volume and rate of this required data collection rapidly outpaces our abilities to process and analyze them. Recent advances in machine learning enables fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data standardization, insufficient formatting, and demand for large, labeled datasets. To address this need, we built FathomNet, an open-source image database that standardizes and aggregates expertly curated labeled data. FathomNet has been seeded with existing iconic and non-iconic imagery of marine animals, underwater equipment, debris, and other concepts, and allows for future contributions from distributed data sources. We demonstrate how FathomNet data can be used to train and deploy models on other institutional video to reduce annotation effort, and enable automated tracking of underwater concepts when integrated with robotic vehicles. As FathomNet continues to grow and incorporate more labeled data from the community, we can accelerate the processing of visual data to achieve a healthy and sustainable global ocean.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/MAV7HC6K/Katija et al. - 2022 - FathomNet A global image database for enabling ar.pdf}
}

@article{kay_fishnet_2021,
  title = {The {{Fishnet Open Images Database}}: {{A Dataset}} for {{Fish Detection}} and {{Fine-Grained Categorization}} in {{Fisheries}}},
  shorttitle = {The {{Fishnet Open Images Database}}},
  author = {Kay, Justin and Merrifield, Matt},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.09178 [cs]},
  eprint = {2106.09178},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Camera-based electronic monitoring (EM) systems are increasingly being deployed onboard commercial fishing vessels to collect essential data for fisheries management and regulation. These systems generate large quantities of video data which must be reviewed on land by human experts. Computer vision can assist this process by automatically detecting and classifying fish species, however the lack of existing public data in this domain has hindered progress. To address this, we present the Fishnet Open Images Database, a large dataset of EM imagery for fish detection and fine-grained categorization onboard commercial fishing vessels. The dataset consists of 86,029 images containing 34 object classes, making it the largest and most diverse public dataset of fisheries EM imagery to-date. It includes many of the characteristic challenges of EM data: visual similarity between species, skewed class distributions, harsh weather conditions, and chaotic crew activity. We evaluate the performance of existing detection and classification algorithms and demonstrate that the dataset can serve as a challenging benchmark for development of computer vision algorithms in fisheries. The dataset is available at https://www.fishnet.ai/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/QT8P2FAJ/Kay and Merrifield - 2021 - The Fishnet Open Images Database A Dataset for Fi.pdf;/Users/b3020111/Zotero/storage/ETZBZKWY/2106.html}
}

@article{keen_catrlog_2021,
  title = {{{catRlog}}: A Photo-Identification Project Management System Based in {{R}}},
  shorttitle = {{{catRlog}}},
  author = {Keen, Eric M. and Wren, Julia and O'Mahony, {\'E}adin and Wray, Janie},
  year = {2021},
  month = aug,
  journal = {Mammalian Biology},
  issn = {1616-5047, 1618-1476},
  doi = {10.1007/s42991-021-00158-7},
  urldate = {2021-08-09},
  abstract = {Photo-identification (photo-ID) databases can comprise versatile troves of information for well-studied animal populations and, when organized well and curated carefully, can be readily applied to a wide range of research questions, such as population abundance estimates, meta-population connectivity and social network structure. To bring the potential impact of photo-ID data within reach of a greater number of research groups, we introduce an R-based photo-ID project management system, named `catRlog'. As a computer directory with custom apps embedded throughout, catRlog serves as a workflow organizer that simplifies, streamlines, and improves the quality of photo-ID data processing. The system can be utilized by research teams in a number of ways, ranging from automated formatting and printing of a photo-ID catalog, to photoID matching, thereby creating and expanding a historical catalog, to processing of identification data to generate datasets necessary for site fidelity, mark\textendash recapture, and social association analyses. As an R-based tool, the apps are open-source, cross-platform, readily customizable, and easily updated. catRlog has been tested using photo-ID databases of humpback whales (Megaptera novaeangliae) and fin whales (Balaenoptera physalus) within a mainland fjord system of Pacific Canada, but is a generalized system useful for almost any photo-ID project of any species in any habitat. A detailed user's manual and example dataset are provided.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/UR77HTVV/Keen et al. - 2021 - catRlog a photo-identification project management.pdf}
}

@misc{kendall_what_2017,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  year = {2017},
  month = oct,
  number = {arXiv:1703.04977},
  eprint = {1703.04977},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-06-24},
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model \textendash{} uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/2GJCFCMG/Kendall and Gal - 2017 - What Uncertainties Do We Need in Bayesian Deep Lea.pdf}
}

@article{khosla_novel_2011,
  title = {Novel {{Dataset}} for {{Fine-Grained Image Categorization}}: {{Stanford Dogs}}},
  author = {Khosla, Aditya and Jayadevaprakash, Nityananda and Yao, Bangpeng and Li, Fei-Fei},
  year = {2011},
  journal = {Proc. CVPR workshop on fine-grained visual categorization (FGVC)},
  pages = {2},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/Z6NWN3YS/Khosla et al. - Novel Dataset for Fine-Grained Image Categorizatio.pdf}
}

@incollection{kim_convolutional_2017,
  title = {Convolutional {{Neural Network}}},
  booktitle = {{{MATLAB Deep Learning}}: {{With Machine Learning}}, {{Neural Networks}} and {{Artificial Intelligence}}},
  author = {Kim, Phil},
  year = {2017},
  pages = {121--147},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-2845-6_6},
  abstract = {The importance of the deep neural network lies in the fact that it opened the door to the complicated non-linear model and systematic approach for the hierarchical processing of knowledge.},
  isbn = {978-1-4842-2845-6}
}

@article{kingma_adam:_2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/B6RJQ8WY/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/Users/b3020111/Zotero/storage/7UDECC6K/1412.html}
}

@article{kirillov_segment_2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2023},
  month = apr,
  journal = {arXiv:2304.02643 [cs.CV]},
  eprint = {2304.02643},
  primaryclass = {cs.CV},
  urldate = {2023-04-18},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive \textendash{} often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/I72Z2HPS/Kirillov et al. - 2023 - Segment Anything.pdf}
}

@article{konovalov_individual_nodate,
  title = {Individual {{Minke Whale Recognition Using Deep Learning Convolutional Neural Networks}}},
  author = {Konovalov, Dmitry A and Hillcoat, Suzanne and Williams, Genevieve and Birtles, R Alastair and Gardiner, Naomi and Curnock, Matthew I},
  journal = {Journal of Geoscience and Environment Protection},
  pages = {12},
  abstract = {The only known predictable aggregation of dwarf minke whales (Balaenoptera acutorostrata subsp.) occurs in the Australian offshore waters of the northern Great Barrier Reef in May-August each year. The identification of individual whales is required for research on the whales' population characteristics and for monitoring the potential impacts of tourism activities, including commercial swims with the whales. At present, it is not cost-effective for researchers to manually process and analyze the tens of thousands of underwater images collated after each observation/tourist season, and a large data base of historical non-identified imagery exists. This study reports the first proof of concept for recognizing individual dwarf minke whales using the Deep Learning Convolutional Neural Networks (CNN).The ``off-the-shelf'' Image net-trained VGG16 CNN was used as the feature-encoder of the per-pixel sematic segmentation Automatic Minke Whale Recognizer (AMWR). The most frequently photographed whale in a sample of 76 individual whales (MW1020) was identified in 179 images out of the total 1320 images provided. Training and image augmentation procedures were developed to compensate for the small number of available images. The trained AMWR achieved 93\% prediction accuracy on the testing subset of 36 positive/MW1020 and 228 negative/not-MW1020 images, where each negative image contained at least one of the other 75 whales. Furthermore on the test subset, AMWR achieved 74\% precision, 80\% recall, and 4\% false-positive rate, making the presented approach comparable or better to other state-of-the-art individual animal recognition results.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/6BYIW2GN/Konovalov et al. - Individual Minke Whale Recognition Using Deep Lear.pdf}
}

@article{korotcov_comparison_2017,
  title = {Comparison of {{Deep Learning With Multiple Machine Learning Methods}} and {{Metrics Using Diverse Drug Discovery Data Sets}}},
  author = {Korotcov, Alexandru and Tkachenko, Valery and Russo, Daniel P. and Ekins, Sean},
  year = {2017},
  month = dec,
  journal = {Molecular Pharmaceutics},
  volume = {14},
  number = {12},
  pages = {4462--4475},
  issn = {1543-8384, 1543-8392},
  doi = {10.1021/acs.molpharmaceut.7b00578},
  urldate = {2021-06-11},
  abstract = {Machine learning methods have been applied to many data sets in pharmaceutical research for several decades. The relative ease and availability of fingerprint type molecular descriptors paired with Bayesian methods resulted in the widespread use of this approach for a diverse array of end points relevant to drug discovery. Deep learning is the latest machine learning algorithm attracting attention for many of pharmaceutical applications from docking to virtual screening. Deep learning is based on an artificial neural network with multiple hidden layers and has found considerable traction for many artificial intelligence applications. We have previously suggested the need for a comparison of different machine learning methods with deep learning across an array of varying data sets that is applicable to pharmaceutical research. End points relevant to pharmaceutical research include absorption, distribution, metabolism, excretion, and toxicity (ADME/Tox) properties, as well as activity against pathogens and drug discovery data sets. In this study, we have used data sets for solubility, probe-likeness, hERG, KCNQ1, bubonic plague, Chagas, tuberculosis, and malaria to compare different machine learning methods using FCFP6 fingerprints. These data sets represent whole cell screens, individual proteins, physicochemical properties as well as a data set with a complex end point. Our aim was to assess whether deep learning offered any improvement in testing when assessed using an array of metrics including AUC, F1 score, Cohen's kappa, Matthews correlation coefficient and others. Based on ranked normalized scores for the metrics or data sets Deep Neural Networks (DNN) ranked higher than SVM, which in turn was ranked higher than all the other machine learning methods. Visualizing these properties for training and test sets using radar type plots indicates when models are inferior or perhaps over trained. These results also suggest the need for assessing deep learning further using multiple metrics with much larger scale comparisons, prospective testing as well as assessment of different fingerprints and DNN architectures beyond those used.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/CBDSNBTU/Korotcov et al. - 2017 - Comparison of Deep Learning With Multiple Machine .pdf}
}

@incollection{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-06-05},
  file = {/Users/b3020111/Zotero/storage/BHYY9P53/4824-imagenet-classification-with-deep-convolutional-neural-networ.html}
}

@techreport{krizhevsky_learning_2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2009},
  pages = {60},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/MLP5PYV9/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@article{krizhevsky_learning_nodate,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  pages = {60},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/QBH3HT93/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@article{krogh_simple_1991,
  title = {A {{Simple Weight Decay Can Improve Generalization}}},
  author = {Krogh, Anders and Hertz, John A},
  year = {1991},
  journal = {Advances in neural information processing systems},
  volume = {4},
  pages = {8},
  abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/6PHZS6IB/Krogh and Hertz - A Simple Weight Decay Can Improve Generalization.pdf}
}

@article{kulits_elephantbook_2021,
  title = {{{ElephantBook}}: {{A Semi-Automated Human-in-the-Loop System}} for {{Elephant Re-Identification}}},
  shorttitle = {{{ElephantBook}}},
  author = {Kulits, Peter and Wall, Jake and Bedetti, Anka and Henley, Michelle and Beery, Sara},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.15083 [cs]},
  eprint = {2106.15083},
  primaryclass = {cs},
  doi = {10.1145/3460112.3471947},
  urldate = {2021-07-21},
  abstract = {African elephants are vital to their ecosystems, but their populations are threatened by a rise in human-elephant conflict and poaching. Monitoring population dynamics is essential in conservation efforts; however, tracking elephants is a difficult task, usually relying on the invasive and sometimes dangerous placement of GPS collars. Although there have been many recent successes in the use of computer vision techniques for automated identification of other species, identification of elephants is extremely difficult and typically requires expertise as well as familiarity with elephants in the population. We have built and deployed a web-based platform and database for human-in-the-loop re-identification of elephants combining manual attribute labeling and state-of-the-art computer vision algorithms, known as ElephantBook. Our system is currently in use at the Mara Elephant Project, helping monitor the protected and at-risk population of elephants in the Greater Maasai Mara ecosystem. ElephantBook makes elephant re-identification usable by non-experts and scalable for use by multiple conservation NGOs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/F4MVKLBW/Kulits et al. - 2021 - ElephantBook A Semi-Automated Human-in-the-Loop S.pdf}
}

@article{langtimm_survival_2004,
  title = {Survival {{Estimates}} for {{Florida Manatees}} from the {{Photo-Identification}} of {{Individuals}}},
  author = {Langtimm, Catherine A. and Beck, Cathy A. and Edwards, Holly H. and Fick-Child, Kristin J. and Ackerman, Bruce B. and Barton, Sheri L. and Hartley, Wayne C.},
  year = {2004},
  journal = {Marine Mammal Science},
  volume = {20},
  number = {3},
  pages = {438--463},
  issn = {1748-7692},
  doi = {10.1111/j.1748-7692.2004.tb01171.x},
  urldate = {2021-01-07},
  abstract = {We estimated adult survival probabilities for the endangered Florida manatee (Trichechus manatus latirostris) in four regional populations using photoidentification data and open-population capture-recapture statistical models. The mean annual adult survival probability over the most recent 10-yr period of available estimates was as follows: Northwest - 0.956 (SE 0.007), Upper St. Johns River - 0.960 (0.011), Atlantic Coast - 0.937 (0.008), and Southwest - 0.908 (0.019). Estimates of temporal variance independent of sampling error, calculated from the survival estimates, indicated constant survival in the Upper St. Johns River, true temporal variability in the Northwest and Atlantic Coast, and large sampling variability obscuring estimates for the Southwest. Calf and subadult survival probabilities were estimated for the Upper St. Johns River from the only available data for known-aged individuals: 0.810 (95\% CI 0.727\textendash 0.873) for 1st year calves, 0.915 (0.827\textendash 0.960) for 2nd year calves, and 0.969 (0.946\textendash 0.982) for manatee 3 yr or older. These estimates of survival probabilities and temporal variance, in conjunction with estimates of reproduction probabilities from photoidentification data can be used to model manatee population dynamics, estimate population growth rates, and provide an integrated measure of regional status.},
  langid = {english},
  keywords = {capture-recapture,manatee,photo-identification,Program MARK,sighting probabilities,survival probabilities,temporal variance,Trichechus manatus latirostris},
  file = {/Users/b3020111/Zotero/storage/TQXXMDQF/Langtimm et al. - 2004 - Survival Estimates for Florida Manatees from the P.pdf;/Users/b3020111/Zotero/storage/37W9NLBC/j.1748-7692.2004.tb01171.html}
}

@article{laptev_time-series_2017,
  title = {Time-Series {{Extreme Event Forecasting}} with {{Neural Networks}} at {{Uber}}},
  author = {Laptev, Nikolay and Yosinski, Jason and Li, Li Erran and Smyl, Slawek},
  year = {2017},
  pages = {5},
  abstract = {Accurate time-series forecasting during high variance segments (e.g., holidays), is critical for anomaly detection, optimal resource allocation, budget planning and other related tasks. At Uber accurate prediction for completed trips during special events can lead to a more efficient driver allocation resulting in a decreased wait time for the riders.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/2928BAWW/Laptev et al. - Time-series Extreme Event Forecasting with Neural .pdf}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2019-08-08},
  langid = {english}
}

@inproceedings{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  booktitle = {Proceedings of the {{IEEE}}},
  author = {Lecun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year = {1998},
  pages = {2278--2324},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.},
  file = {/Users/b3020111/Zotero/storage/EJZX7HLS/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/b3020111/Zotero/storage/MUKSRDLZ/summary.html}
}

@article{lee_backbone_2020,
  title = {Backbone {{Alignment}} and {{Cascade Tiny Object Detecting Techniques}} for {{Dolphin Detection}} and {{Classification}}},
  author = {Lee, Yih-Cherng and Hsu, Hung-Wei and Ding, Jian-Jiun and Hou, Wen and Chou, Lien-Shiang and Chang, Ronald Y.},
  year = {2020},
  journal = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  volume = {advpub},
  doi = {10.1587/transfun.2020EAP1054},
  abstract = {Automatic tracking and classification are essential for studying the behaviors of wild animals. Owing to dynamic far-shooting photos, the occlusion problem, protective coloration, the background noise is irregular interference for designing a computerized algorithm for reducing human labeling resources. Moreover, wild dolphin images are hard-acquired by on-the-spot investigations, which takes a lot of waiting time and hardly sets the fixed camera to automatic monitoring dolphins on the ocean in several days. It is challenging tasks to detect well and classify a dolphin from polluted photos by a single famous deep learning method in a small dataset. Therefore, in this study, we propose a generic Cascade Small Object Detection (CSOD) algorithm for dolphin detection to handle small object problems and develop visualization to backbone based classification (V2BC) for removing noise, highlighting features of dolphin and classifying the name of dolphin. The architecture of CSOD consists of the P-net and the F-net. The P-net uses the crude Yolov3 detector to be a core network to predict all the regions of interest (ROIs) at lower resolution images. Then, the F-net, which is more robust, is applied to capture the ROIs from high-resolution photos to solve single detector problems. Moreover, a visualization to backbone based classification (V2BC) method focuses on extracting significant regions of occluded dolphin and design significant post-processing by referencing the backbone of dolphins to facilitate for classification. Compared to the state of the art methods, including faster-rcnn, yolov3 detection and Alexnet, the Vgg, and the Resnet classification. All experiments show that the proposed algorithm based on CSOD and V2BC has an excellent performance in dolphin detection and classification. Consequently, compared to the related works of classification, the accuracy of the proposed designation is over 14\% higher. Moreover, our proposed CSOD detection system has 42\% higher performance than that of the original Yolov3 architecture.},
  keywords = {deep learning,image classification,object detection,object segmentation,rotation measurement},
  file = {/Users/b3020111/Zotero/storage/IP34FZ8L/Lee et al. - 2020 - Backbone Alignment and Cascade Tiny Object Detecti.pdf;/Users/b3020111/Zotero/storage/RT8I68XZ/en.html}
}

@inproceedings{lee_difference_2015,
  title = {Difference {{Target Propagation}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
  editor = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, V{\'i}tor and Soares, Carlos and Gama, Jo{\~a}o and Jorge, Al{\'i}pio},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {498--515},
  publisher = {{Springer International Publishing}},
  abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
  isbn = {978-3-319-23528-8},
  langid = {english},
  keywords = {Deep Neural Network,Hide Layer,Hide Unit,Target Propagation,Test Error},
  file = {/Users/b3020111/Zotero/storage/25S9ZBW5/Lee et al. - 2015 - Difference Target Propagation.pdf}
}

@article{lee_potato_2020,
  title = {Potato {{Detection}} and {{Segmentation Based}} on {{Mask R-CNN}}},
  author = {Lee, Hyeon-Seung and Shin, Beom-Soo},
  year = {2020},
  month = oct,
  journal = {Journal of Biosystems Engineering},
  issn = {2234-1862},
  doi = {10.1007/s42853-020-00063-w},
  urldate = {2020-11-13},
  abstract = {Potatoes are similar in color and size to soil and its clods. They are mostly irregular in the shape as well. Therefore, it is not easy to distinguish potatoes from the soil surface background only with machine vision. This study applied Mask R-CNN, one of the object recognition technologies using deep learning to detect potatoes. The size of object in pixel was obtained on individual potato, and they will be used to predict the yield of potatoes.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/D677D2ZD/Lee and Shin - 2020 - Potato Detection and Segmentation Based on Mask R-.pdf}
}

@inproceedings{lee_weakly_2022,
  title = {Weakly {{Supervised Semantic Segmentation Using Out-of-Distribution Data}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lee, Jungbeom and Oh, Seong Joon and Yun, Sangdoo and Choe, Junsuk and Kim, Eunji and Yoon, Sungroh},
  year = {2022},
  pages = {10},
  address = {{New Orleans, LA, USA}},
  abstract = {Weakly supervised semantic segmentation (WSSS) methods are often built on pixel-level localization maps obtained from a classifier. However, training on class labels only, classifiers suffer from the spurious correlation between foreground and background cues (e.g. train and rail), fundamentally bounding the performance of WSSS. There have been previous endeavors to address this issue with additional supervision. We propose a novel source of information to distinguish foreground from the background: Out-of-Distribution (OoD) data, or images devoid of foreground object classes. In particular, we utilize the hard OoDs that the classifier is likely to make false-positive predictions. These samples typically carry key visual features on the background (e.g. rail) that the classifiers often confuse as foreground (e.g. train), so these cues let classifiers correctly suppress spurious background cues. Acquiring such hard OoDs does not require an extensive amount of annotation efforts; it only incurs a few additional image-level labeling costs on top of the original efforts to collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs. W-OoD achieves state-of-the-art performance on Pascal VOC 2012. The code is available at: https://github.com/naver-ai/w-ood.},
  langid = {english},
  keywords = {Suggested,Suggested: Steve},
  file = {/Users/b3020111/Zotero/storage/UHY82TMG/Lee et al. - Weakly Supervised Semantic Segmentation Using Out-.pdf}
}

@article{li_exploring_2022,
  title = {Exploring {{Plain Vision Transformer Backbones}} for {{Object Detection}}},
  author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  year = {2022},
  journal = {arXiv:2203.16527 [cs.CV]},
  eprint = {2203.16527},
  primaryclass = {cs.CV},
  pages = {21},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/4DKXQNAT/Li et al. - Exploring Plain Vision Transformer Backbones for O.pdf}
}

@article{li_mask_2022,
  title = {Mask {{DINO}}: {{Towards A Unified Transformer-based Framework}} for {{Object Detection}} and {{Segmentation}}},
  shorttitle = {Mask {{DINO}}},
  author = {Li, Feng and Zhang, Hao and {xu}, Huaizhe and Liu, Shilong and Zhang, Lei and Ni, Lionel M. and Shum, Heung-Yeung},
  year = {2022},
  month = dec,
  journal = {arXiv:2206.02777 [cs.CV]},
  eprint = {2206.02777},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dotproduct a high-resolution pixel embedding map to predict a set of binary masks. Some key components in DINO are extended for segmentation through a shared architecture and training process. Mask DINO is simple, efficient, and scalable, and it can benefit from joint large-scale detection and segmentation datasets. Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K) among models under one billion parameters. Code is available at https://github.com/IDEAResearch/MaskDINO.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/CZQMQP2W/Li et al. - 2022 - Mask DINO Towards A Unified Transformer-based Fra.pdf}
}

@article{li_yolov6_2022,
  title = {{{YOLOv6}}: {{A Single-Stage Object Detection Framework}} for {{Industrial Applications}}},
  shorttitle = {{{YOLOv6}}},
  author = {Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and Li, Yiduo and Zhang, Bo and Liang, Yufei and Zhou, Linyuan and Xu, Xiaoming and Chu, Xiangxiang and Wei, Xiaoming and Wei, Xiaolin},
  year = {2022},
  month = sep,
  journal = {arXiv:2209.02976 [cs.CV]},
  eprint = {2209.02976},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {For years, the YOLO series has been the de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application. Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academia. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization, and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9\% AP on the COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5\% AP at 495 FPS, outperforming other mainstream detectors at the same scale\textasciitilde (YOLOv5-S, YOLOX-S, and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3\% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5\%/52.3\%) than other detectors with a similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at https://github.com/meituan/YOLOv6.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/EDBGWJ42/Li et al. - 2022 - YOLOv6 A Single-Stage Object Detection Framework .pdf;/Users/b3020111/Zotero/storage/MSFA4C3C/2209.html}
}

@article{li_yolov6_2023,
  title = {{{YOLOv6}} v3.0: {{A Full-Scale Reloading}}},
  shorttitle = {{{YOLOv6}} v3.0},
  author = {Li, Chuyi and Li, Lulu and Geng, Yifei and Jiang, Hongliang and Cheng, Meng and Zhang, Bo and Ke, Zaidan and Xu, Xiaoming and Chu, Xiangxiang},
  year = {2023},
  month = jan,
  journal = {arXiv:2301.05586 [cs.CV]},
  eprint = {2301.05586},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {The YOLO community has been in high spirits since our first two releases! By the advent of Chinese New Year 2023, which sees the Year of the Rabbit, we refurnish YOLOv6 with numerous novel enhancements on the network architecture and the training scheme. This release is identified as YOLOv6 v3.0. For a glimpse of performance, our YOLOv6-N hits 37.5\% AP on the COCO dataset at a throughput of 1187 FPS tested with an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 45.0\% AP at 484 FPS, outperforming other mainstream detectors at the same scale (YOLOv5-S, YOLOv8-S, YOLOX-S and PPYOLOE-S). Whereas, YOLOv6-M/L also achieve better accuracy performance (50.0\%/52.8\% respectively) than other detectors at a similar inference speed. Additionally, with an extended backbone and neck design, our YOLOv6-L6 achieves the state-of-the-art accuracy in real-time. Extensive experiments are carefully conducted to validate the effectiveness of each improving component. Our code is made available at https://github.com/meituan/YOLOv6.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/Q38554LK/Li et al. - 2023 - YOLOv6 v3.0 A Full-Scale Reloading.pdf}
}

@misc{liang_googles_2018,
  title = {Google's {{AI Helps Researcher ID Dolphins}}},
  author = {Liang, John},
  year = {2018},
  month = oct,
  journal = {DeeperBlue.com},
  urldate = {2020-02-12},
  abstract = {Google's artificial intelligence engineers are collaborating with a university researcher to identify dolphins in the wild.},
  howpublished = {https://www.deeperblue.com/googles-ai-helps-researcher-id-dolphins/},
  langid = {american},
  file = {/Users/b3020111/Zotero/storage/GJ9FC95C/googles-ai-helps-researcher-id-dolphins.html}
}

@inproceedings{liao_how_2016,
  title = {How Important Is Weight Symmetry in Backpropagation?},
  booktitle = {Thirtieth {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
  year = {2016}
}

@article{lillicrap_random_2014,
  title = {Random Feedback Weights Support Learning in Deep Neural Networks},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.0247 [cs, q-bio]},
  eprint = {1411.0247},
  primaryclass = {cs, q-bio},
  urldate = {2019-08-08},
  abstract = {The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/b3020111/Zotero/storage/G42ZBI5X/Lillicrap et al. - 2014 - Random feedback weights support learning in deep n.pdf;/Users/b3020111/Zotero/storage/9JX7GZX9/1411.html}
}

@inproceedings{lim_automated_2018,
  title = {Automated {{Interpretation}} of {{Seafloor Visual Maps Obtained Using Underwater Robots}}},
  booktitle = {2018 {{OCEANS-MTS}}/{{IEEE Kobe Techno-Oceans}} ({{OTO}})},
  author = {Lim, Jin Wei and {Pr{\"u}gel-Bennett}, Adam and Thornton, Blair},
  year = {2018},
  pages = {1--8},
  publisher = {{IEEE}},
  isbn = {1-5386-1654-8}
}

@article{lin_microsoft_2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2014},
  month = may,
  journal = {arXiv:1405.0312 [cs]},
  eprint = {1405.0312},
  primaryclass = {cs},
  urldate = {2019-02-11},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/V4BHGBCE/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf;/Users/b3020111/Zotero/storage/4S98T37Y/1405.html}
}

@article{linnainmaa_representation_1970,
  title = {The Representation of the Cumulative Rounding Error of an Algorithm as a {{Taylor}} Expansion of the Local Rounding Errors},
  author = {Linnainmaa, Seppo},
  year = {1970},
  journal = {Master's Thesis (in Finnish), Univ. Helsinki},
  pages = {6--7}
}

@article{liu_convnet_2022,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  year = {2022},
  month = mar,
  journal = {arXiv:2201.03545 [cs.CV]},
  eprint = {2201.03545},
  primaryclass = {cs.CV},
  urldate = {2022-09-29},
  abstract = {The ``Roaring 20s'' of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually ``modernize'' a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/53FQSKM4/Liu et al. - 2022 - A ConvNet for the 2020s.pdf}
}

@article{liu_convnet_2022-1,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  year = {2022},
  month = mar,
  journal = {arXiv:2201.03545 [cs.CV]},
  eprint = {2201.03545},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {The ``Roaring 20s'' of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually ``modernize'' a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/PQKUIIS3/Liu et al. - 2022 - A ConvNet for the 2020s.pdf}
}

@inproceedings{liu_dog_2012,
  title = {Dog Breed Classification Using Part Localization},
  booktitle = {European Conference on Computer Vision},
  author = {Liu, Jiongxin and Kanazawa, Angjoo and Jacobs, David and Belhumeur, Peter},
  year = {2012},
  pages = {172--185},
  publisher = {{Springer}}
}

@article{liu_mobile_2017,
  title = {Mobile {{Video Object Detection}} with {{Temporally-Aware Feature Maps}}},
  author = {Liu, Mason and Zhu, Menglong},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.06368 [cs]},
  eprint = {1711.06368},
  primaryclass = {cs},
  urldate = {2019-03-06},
  abstract = {This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/GBUBLB8I/Liu and Zhu - 2017 - Mobile Video Object Detection with Temporally-Awar.pdf;/Users/b3020111/Zotero/storage/JYZEHKYN/Liu and Zhu - 2017 - Mobile Video Object Detection with Temporally-Awar.pdf;/Users/b3020111/Zotero/storage/9Y23LQU2/1711.html;/Users/b3020111/Zotero/storage/T9XUGQYL/1711.html}
}

@inproceedings{liu_segmentation_2018,
  title = {Segmentation of {{Lung Nodule}} in {{CT Images Based}} on {{Mask R-CNN}}},
  booktitle = {2018 9th {{International Conference}} on {{Awareness Science}} and {{Technology}} ({{iCAST}})},
  author = {Liu, M. and Dong, J. and Dong, X. and Yu, H. and Qi, L.},
  year = {2018},
  month = sep,
  pages = {1--6},
  issn = {2325-5994},
  doi = {10.1109/ICAwST.2018.8517248},
  abstract = {Due to the low-quality of CT images, the lack of annotated data, and the complex shapes of lung nodules, existing methods for lung nodules detection only predict the center of the nodule, whereas the nodule size is a very important diagnostic criteria but is neglected. In this paper, we employed the powerful object detection neural network ``Mask R-CNN'' for lung nodule segmentation, which provides contour information. Because of the imbalance between positive and negative samples, we trained classification networks based on block. We selected the classification network with the hightest accuracy. The selected classification network was used as the backbone of the image segmentation network-Mask R-CNN, which performs excellently on natural images. Lastly, Mask R-CNN model trained on the COCO data set was fine-tuned to segment pulmonary nodules. The model was tested on the LIDC-IDRI dataset.},
  keywords = {annotated data,cancer,Cancer,COCO dataset,Computed tomography,computerised tomography,Convolutional neural networks,CT images,deep learning,diagnostic criteria,feature extraction,image segmentation,Image segmentation,image segmentation network,LIDC-IDRI,LIDC-IDRI dataset,low-quality,lung,Lung,lung nodule segmentation,lung nodules detection,Mask R-CNN,mask R-CNN model,medical image processing,nodule size,object detection,object detection neural network,recurrent neural nets,segment pulmonary nodules,selected classification network,Solid modeling,Task analysis},
  file = {/Users/b3020111/Zotero/storage/E9JV7XL9/Liu et al. - 2018 - Segmentation of Lung Nodule in CT Images Based on .pdf;/Users/b3020111/Zotero/storage/NYE66QZN/8517248.html}
}

@article{liu_ssd:_2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  journal = {arXiv:1512.02325 [cs]},
  volume = {9905},
  eprint = {1512.02325},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  urldate = {2019-01-08},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300\textbackslash times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500\textbackslash times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/W27GTLB9/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/Users/b3020111/Zotero/storage/ISCZ9YLM/1512.html}
}

@article{liu_swin_2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  journal = {arXiv:2103.14030 [cs.CV]},
  eprint = {2103.14030},
  primaryclass = {cs.CV},
  urldate = {2022-09-27},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/9HL3ALUX/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@article{liu_swin_2022,
  title = {Swin {{Transformer V2}}: {{Scaling Up Capacity}} and {{Resolution}}},
  shorttitle = {Swin {{Transformer V2}}},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
  year = {2022},
  month = apr,
  journal = {arXiv:2111.09883 [cs.CV]},
  eprint = {2111.09883},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pretraining method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\texttimes 1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at https://github.com/ microsoft/Swin-Transformer.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/3N9WGPR8/Liu et al. - 2022 - Swin Transformer V2 Scaling Up Capacity and Resol.pdf}
}

@article{lockyer_observations_1990,
  title = {Some Observations on Wound Healing and Persistence of Scars in {{Tursiops}} Truncatus},
  author = {Lockyer, {\relax CH} and Morris, {\relax RJ}},
  year = {1990},
  journal = {Reports of the International Whaling Commission},
  number = {12},
  pages = {113--118},
  file = {/Users/b3020111/Zotero/storage/ZXL857J5/Lockyer and Morris - Some observations on wound healing and persistence.pdf}
}

@article{long_fully_2014,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.4038 [cs]},
  eprint = {1411.4038},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [4] to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/B4VVGTY3/Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf}
}

@article{loshchilov_sgdr:_2016,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.03983 [cs, math]},
  eprint = {1608.03983},
  primaryclass = {cs, math},
  urldate = {2019-01-08},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/b3020111/Zotero/storage/43KPKN54/Loshchilov and Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf;/Users/b3020111/Zotero/storage/ZGNZ6WS6/1608.html}
}

@inproceedings{lowe_object_1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D. G.},
  year = {1999},
  month = sep,
  volume = {2},
  pages = {1150-1157 vol.2},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.},
  keywords = {3D projection,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,computational geometry,Computer science,Electrical capacitance tomography,feature extraction,Filters,image matching,Image recognition,inferior temporal cortex,Layout,least squares approximations,Lighting,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,Neurons,object recognition,Object recognition,primate vision,Programmable logic arrays,Reactive power,robust object recognition,staged filtering approach,unknown model parameters},
  file = {/Users/b3020111/Zotero/storage/2Y5PQRM3/Lowe - 1999 - Object recognition from local scale-invariant feat.pdf;/Users/b3020111/Zotero/storage/JVZEP5XF/790410.html}
}

@inproceedings{luo_artificial_2005,
  title = {Artificial Neural Network Computation on Graphic Process Unit},
  booktitle = {Proceedings. 2005 {{IEEE International Joint Conference}} on {{Neural Networks}}, 2005.},
  author = {Luo, Zhongwen and Liu, Hongzhi and Wu, Xincai},
  year = {2005},
  month = jul,
  volume = {1},
  pages = {622-626 vol. 1},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2005.1555903},
  abstract = {Artificial neural network (ANN) is widely used in pattern recognition related area. In some case, the computational load is very heavy, in other case, real time process is required. So there is a need to apply a parallel algorithm on it, and usually the computation for ANN is inherently parallel. In this paper, graphic hardware is used to speed up the computation of ANN. In recent years, graphic processing unit (GPU) grows faster than CPU. Graphic hardware venders provide programmability on GPU. In this paper, application of commodity available GPU for two kinds of ANN models was explored. One is the self-organizing maps (SOM); the other is multi layer perceptron (MLP). The computation result shows that ANN computing on GPU is much faster than on standard CPU when the neural network is large. And some design rules for improve the efficiency on GPU are given.},
  keywords = {Artificial neural networks,Central Processing Unit,Computer networks,Concurrent computing,Graphics,Hardware,Neural networks,Parallel algorithms,Pattern recognition,Self organizing feature maps},
  file = {/Users/b3020111/Zotero/storage/NG4QBAD9/Luo et al. - 2005 - Artificial neural network computation on graphic p.pdf;/Users/b3020111/Zotero/storage/6Y5GARG7/1555903.html}
}

@article{luo_multiple_2017,
  title = {Multiple {{Object Tracking}}: {{A Literature Review}}},
  shorttitle = {Multiple {{Object Tracking}}},
  author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Zhao, Xiaowei and Kim, Tae-Kyun},
  year = {2017},
  month = may,
  journal = {arXiv:1409.7618 [cs]},
  eprint = {1409.7618},
  primaryclass = {cs},
  urldate = {2020-10-09},
  abstract = {Multiple Object Tracking (MOT) is an important computer vision problem which has gained increasing attention due to its academic and commercial potential. Although different kinds of approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in a multiple object tracking system, including formulation, categorization, key principles, evaluation of an MOT are discussed. 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks. 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative comparisons. We also point to some interesting discoveries by analyzing these results. 4) We provide a discussion about issues of MOT research, as well as some interesting directions which could possibly become potential research effort in the future.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.8},
  file = {/Users/b3020111/Zotero/storage/SKL85HAK/Luo et al. - 2017 - Multiple Object Tracking A Literature Review.pdf}
}

@article{maglietta_dolfin_2018,
  title = {{{DolFin}}: An Innovative Digital Platform for Studying {{Risso}}'s Dolphins in the {{Northern Ionian Sea}} ({{North-eastern Central Mediterranean}})},
  shorttitle = {{{DolFin}}},
  author = {Maglietta, Rosalia and Ren{\`o}, Vito and Cipriano, Giulia and Fanizza, Carmelo and Milella, Annalisa and Stella, Ettore and Carlucci, Roberto},
  year = {2018},
  month = nov,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {17185},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-35492-3},
  urldate = {2021-01-08},
  abstract = {The Risso's dolphin is a widely distributed species, found in deep temperate and tropical waters. Estimates of its abundance are available in a few regions, details of its distribution are lacking, and its status in the Mediterranean Sea is ranked as Data Deficient by the IUCN Red List. In this paper, a synergy between bio-ecological analysis and innovative strategies has been applied to construct a digital platform, DolFin. It contains a collection of sighting data and geo-referred photos of Grampus griseus, acquired from 2013 to 2016 in the Gulf of Taranto (Northern Ionian Sea, North-eastern Central Mediterranean Sea), and the first automated tool for Smart Photo Identification of the Risso's dolphin (SPIR). This approach provides the capability to collect and analyse significant amounts of data acquired over wide areas and extended periods of time. This effort establishes the baseline for future large-scale studies, essential to providing further information on the distribution of G. griseus. Our data and analysis results corroborate the hypothesis of a resident Risso's dolphin population in the Gulf of Taranto, showing site fidelity in a relatively restricted area characterized by a steep slope to around 800\,m in depth, north of the Taranto Valley canyon system.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/8UNY6B6D/Maglietta et al. - 2018 - DolFin an innovative digital platform for studyin.pdf;/Users/b3020111/Zotero/storage/EYNRUXBB/s41598-018-35492-3.html}
}

@inproceedings{maharani_improving_2020,
  title = {Improving the {{Capability}} of {{Real-Time Face Masked Recognition}} Using {{Cosine Distance}}},
  booktitle = {2020 6th {{International Conference}} on {{Interactive Digital Media}} ({{ICIDM}})},
  author = {Maharani, Devira Anggi and Machbub, Carmadi and Rusmin, Pranoto Hidaya and Yulianti, Lenni},
  year = {2020},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ICIDM51048.2020.9339677},
  abstract = {During the pandemic, people around the world are expected to wear masks. So far, the ability to recognize the identity of a person who wears a mask is still a challenge. Face recognition is widely used in schools, hospitals, and companies as an attendance system, even as a criminal watchlist examination system. Thus, face recognition implementation is difficult to obtain the identity of people who are wearing masks, and moreover, computer systems might fail to detect the faces. This study used Haar-cascade Face detection and MobileNet while proposing the addition of the cosine distance method. This method compares the middle position of face detection results within the previous frame and the current. The proposed system can generate a person's name and identification number while wearing a mask. The system is designed to utilize multi-threading by comparing the transfer learning methods of VGG16 and Triplet Loss FaceNet for face mask recognition with an accuracy rate of 100\% and 82.20\%. Real-time implementation speed resulted in 4 FPS and 22 FPS and successfully added cosine distance to generate a person's ID number.},
  keywords = {cosine distance,Face detection,face mask recognition,Face recognition,FaceNet,Media,MobileNet,multi-threading,Pandemics,real-time,Real-time systems,Streaming media,Transfer learning,VGG16},
  file = {/Users/b3020111/Zotero/storage/R2F88KYE/Maharani et al. - 2020 - Improving the Capability of Real-Time Face Masked .pdf;/Users/b3020111/Zotero/storage/5U2HZKUX/stamp.html}
}

@article{maji_fine-grained_2013,
  title = {Fine-{{Grained Visual Classification}} of {{Aircraft}}},
  author = {Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
  year = {2013},
  month = jun,
  journal = {arXiv:1306.5151 [cs]},
  eprint = {1306.5151},
  primaryclass = {cs},
  urldate = {2021-01-04},
  abstract = {This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/2DKUXG54/Maji et al. - 2013 - Fine-Grained Visual Classification of Aircraft.pdf;/Users/b3020111/Zotero/storage/79ZBE6K7/1306.html}
}

@article{maji_large_2011,
  title = {Large {{Scale Image Annotations}} on {{Amazon Mechanical Turk}}},
  author = {Maji, Subhransu},
  year = {2011},
  journal = {EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2011-79},
  pages = {12},
  abstract = {We describe our experience with collecting roughly 250, 000 image annotations on Amazon Mechanical Turk (AMT). The annotations we collected range from location of keypoints and figure ground masks of various object categories, 3D pose estimates of head and torsos of people in images and attributes like gender, race, type of hair, etc. We describe the setup and strategies we adopted to automatically approve and reject the annotations, which becomes important for large scale annotations. These annotations were used to train algorithms for detection, segmentation, pose estimation, action recognition and attribute recognition of people in images.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/M25KH8JX/Maji - Large Scale Image Annotations on Amazon Mechanical.pdf}
}

@article{mann_behavioral_1999,
  title = {Behavioral {{Sampling Methods}} for {{Cetaceans}}: {{A Review}} and {{Critique}}},
  shorttitle = {Behavioral {{Sampling Methods}} for {{Cetaceans}}},
  author = {Mann, Janet},
  year = {1999},
  journal = {Marine Mammal Science},
  volume = {15},
  number = {1},
  pages = {102--122},
  issn = {1748-7692},
  doi = {10.1111/j.1748-7692.1999.tb00784.x},
  urldate = {2022-04-19},
  abstract = {Behavioral scientists have developed methods for sampling behavior in order to reduce observational biases and to facilitate comparisons between studies. A review of 74 cetacean behavioral field studies published from 1989 to 1995 in Marine Mammal Science and The Canadian Journal of Zoology suggests that cetacean researchers have not made optimal use of available methodology. The survey revealed that a large proportion of studies did not use reliable sampling methods. Ad libitum sampling was used most often (59\%). When anecdotal studies were excluded, 45\% of 53 behavioral studies used ad libitum as the predominant method. Other sampling methods were continuous, onezero, incident, point, sequence, or scan sampling. Recommendations for sampling methods are made, depending on identifiability of animals, group sizes, dive durations, and change in group membership.},
  langid = {english},
  keywords = {behavior,Cetacea,ethology,methods,observations,quantitative,sampling,vocalization},
  file = {/Users/b3020111/Zotero/storage/EC4GBPBL/Mann - 1999 - Behavioral Sampling Methods for Cetaceans A Revie.pdf;/Users/b3020111/Zotero/storage/7KJB3RFS/j.1748-7692.1999.tb00784.html}
}

@book{mann_cetacean_2000,
  title = {Cetacean {{Societies}}: {{Field Studies}} of {{Dolphins}} and {{Whales}}},
  author = {Mann, Janet and Connor, Richard C. and Tyack, Peter and Whitehead, Hal},
  year = {2000},
  publisher = {{University of Chicago Press}},
  urldate = {2021-01-07},
  file = {/Users/b3020111/Zotero/storage/HK8GCY8J/books.html}
}

@misc{mann_dolphin_2019,
  title = {Dolphin {{Matching Using Google Cloud Vision}} ({{DMUGCV}}), Available at {{https://drive.google.com/file/d/1nLXJiSjGCgvjW54UTd46EU64oJcKS1f\_/}}},
  author = {Mann, Janet},
  year = {2019},
  journal = {Google Docs},
  urldate = {2020-02-12},
  howpublished = {https://drive.google.com/file/d/1nLXJiSjGCgvjW54UTd46EU64oJcKS1f\_/view?usp=sharing\&usp=embed\_facebook},
  file = {/Users/b3020111/Zotero/storage/UHEITJLN/view.html}
}

@article{mariani_analysis_2016,
  title = {Analysis of the Natural Markings of {{Risso}}'s Dolphins ({{Grampus}} Griseus) in the Central {{Mediterranean Sea}}},
  author = {Mariani, Monica and Miragliuolo, Angelo and Mussi, Barbara and Russo, Giovanni F. and Ardizzone, Giandomenico and Pace, Daniela S.},
  year = {2016},
  month = dec,
  journal = {Journal of Mammalogy},
  volume = {97},
  number = {6},
  pages = {1512--1524},
  issn = {0022-2372},
  doi = {10.1093/jmammal/gyw109},
  urldate = {2021-01-08},
  abstract = {Risso's dolphins are known for the persistency of their natural markings, possibly due to the loss of pigment during the healing process of skin wounds. Nonetheless, the actual longevity and reliability of each mark type has never been assessed. In this paper, we used photographs to investigate the etiology of skin marks in the species, analyze their distribution and temporal variability, and discuss implications for photo identification. Nineteen mark types were described on the dorsal fin of Risso's dolphin, including 2 new to the literature: the snake-like mark and the protruding fat. Longevity of skin marks ranged from 6 years for the protruding fat to several decades for scrapes and dots. Persistent and reliable marks were notch, tooth-rake, and thick single and parallel scrapes. Mark change was sufficiently low that all mark types could be used for photo identification, provided that backlit or underexposed images were discarded as photographs taken under suboptimal light conditions proved to be unreliable. Finally, mark distribution and variability were unequal between age classes; juveniles were less marked and showed a higher rate of mark change than older individuals so that, even if they possessed enough notches to be classified as reliably marked, they could be confidently matched over a time interval of up to 3 years.},
  file = {/Users/b3020111/Zotero/storage/DB8ZZ7TK/Mariani et al. - 2016 - Analysis of the natural markings of Rissoâs dolphi.pdf;/Users/b3020111/Zotero/storage/6XHTNIJJ/2628015.html}
}

@article{mcculloch_logical_1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  journal = {The bulletin of mathematical biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  urldate = {2019-06-04},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  langid = {english},
  keywords = {beginnings,Excitatory Synapse,Inhibitory Synapse,Nervous Activity,Spatial Summation,Temporal Summation},
  file = {/Users/b3020111/Zotero/storage/DN2VLF7L/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf}
}

@inproceedings{melekhov_siamese_2016,
  title = {Siamese Network Features for Image Matching},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Melekhov, Iaroslav and Kannala, Juho and Rahtu, Esa},
  year = {2016},
  month = dec,
  pages = {378--383},
  doi = {10.1109/ICPR.2016.7899663},
  abstract = {Finding matching images across large datasets plays a key role in many computer vision applications such as structure-from-motion (SfM), multi-view 3D reconstruction, image retrieval, and image-based localisation. In this paper, we propose finding matching and non-matching pairs of images by representing them with neural network based feature vectors, whose similarity is measured by Euclidean distance. The feature vectors are obtained with convolutional neural networks which are learnt from labeled examples of matching and non-matching image pairs by using a contrastive loss function in a Siamese network architecture. Previously Siamese architecture has been utilised in facial image verification and in matching local image patches, but not yet in generic image retrieval or whole-image matching. Our experimental results show that the proposed features improve matching performance compared to baseline features obtained with networks which are trained for image classification task. The features generalize well and improve matching of images of new landmarks which are not seen at training time. This is despite the fact that the labeling of matching and non-matching pairs is imperfect in our training data. The results are promising considering image retrieval applications, and there is potential for further improvement by utilising more training image pairs with more accurate ground truth labels.},
  keywords = {Euclidean distance,Image matching,Image retrieval,Network architecture,Neural networks,Training},
  file = {/Users/b3020111/Zotero/storage/5E2B4825/Melekhov et al. - 2016 - Siamese network features for image matching.pdf;/Users/b3020111/Zotero/storage/JWCYPTMC/7899663.html}
}

@article{miragliuolo_rissos_2004,
  title = {Risso's Dolphin Harassment by Pleasure Boaters off the Island of {{Ischia}}, Central {{Mediterranean Sea}}},
  author = {Miragliuolo, A and Mussi, B and Bearzi, G},
  year = {2004},
  journal = {European Research on Cetaceans},
  volume = {15},
  pages = {168--171},
  file = {/Users/b3020111/Zotero/storage/PS5LI8ZP/Miragliuolo et al. - 2004 - Rissoâs dolphin harassment by pleasure boaters off.pdf}
}

@misc{moindrot_triplet_2018,
  title = {Triplet {{Loss}} and {{Online Triplet Mining}} in {{TensorFlow}}},
  author = {Moindrot, Olivier},
  year = {2018},
  month = mar,
  journal = {Olivier Moindrot blog},
  urldate = {2022-07-01},
  abstract = {Triplet loss is known to be difficult to implement, especially if you add the constraints of TensorFlow.},
  howpublished = {https://omoindrot.github.io/triplet-loss},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/LT5XARTT/triplet-loss.html}
}

@article{moore_marine_2008,
  title = {Marine Mammals as Ecosystem Sentinels},
  author = {Moore, Sue E.},
  year = {2008},
  month = jun,
  journal = {Journal of Mammalogy},
  volume = {89},
  number = {3},
  pages = {534--540},
  issn = {0022-2372, 1545-1542},
  doi = {10.1644/07-MAMM-S-312R1.1},
  urldate = {2019-08-07},
  abstract = {The earth's climate is changing, possibly at an unprecedented rate. Overall, the planet is warming, sea ice and glaciers are in retreat, sea level is rising, and pollutants are accumulating in the environment and within organisms. These clear physical changes undoubtedly affect marine ecosystems. Species dependent on sea ice, such as the polar bear (Ursus maritimus) and the ringed seal (Phoca hispida), provide the clearest examples of sensitivity to climate change. Responses of cetaceans to climate change are more difficult to discern, but in the eastern North Pacific evidence is emerging that gray whales (Eschrichtius robustus) are delaying their southbound migration, expanding their feeding range along the migration route and northward to Arctic waters, and even remaining in polar waters over winter\textemdash all indications that North Pacific and Arctic ecosystems are in transition. To use marine mammals as sentinels of ecosystem change, we must expand our existing research strategies to encompass the decadal and ocean-basin temporal and spatial scales consistent with their natural histories.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/K3JB8DTI/Moore - 2008 - Marine mammals as ecosystem sentinels.pdf}
}

@article{morteo_phenotypic_2017,
  title = {Phenotypic Variation in Dorsal Fin Morphology of Coastal Bottlenose Dolphins ({{Tursiops}} Truncatus) off {{Mexico}}},
  author = {Morteo, Eduardo and {Rocha-Olivares}, Axay{\'a}catl and Morteo, Rodrigo and Weller, David W.},
  year = {2017},
  month = jun,
  journal = {PeerJ},
  volume = {5},
  pages = {e3415},
  publisher = {{PeerJ Inc.}},
  issn = {2167-8359},
  doi = {10.7717/peerj.3415},
  urldate = {2021-01-25},
  abstract = {Geographic variation in external morphology is thought to reflect an interplay between genotype and the environment. Morphological variation has been well-described for a number of cetacean species, including the bottlenose dolphin (Tursiops truncatus). In this study we analyzed dorsal fin morphometric variation in coastal bottlenose dolphins to search for geographic patterns at different spatial scales. A total of 533 dorsal fin images from 19 available photo-identification catalogs across the three Mexican oceanic regions (Pacific Ocean n = 6, Gulf of California n = 6 and, Gulf of Mexico n = 7) were used in the analysis. Eleven fin shape measurements were analyzed to evaluate fin polymorphism through multivariate tests. Principal Component Analysis on log-transformed standardized ratios explained 94\% of the variance. Canonical Discriminant Function Analysis on factor scores showed separation among most study areas (p {$<$} 0.05) with exception of the Gulf of Mexico where a strong morphometric cline was found. Possible explanations for the observed differences are related to environmental, biological and evolutionary processes. Shape distinction between dorsal fins from the Pacific and those from the Gulf of California were consistent with previously reported differences in skull morphometrics and genetics. Although the functional advantages of dorsal fin shape remains to be assessed, it is not unlikely that over a wide range of environments, fin shape may represent a trade-off among thermoregulatory capacity, hydrodynamic performance and the swimming/hunting behavior of the species.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/3BJUVQ67/Morteo et al. - 2017 - Phenotypic variation in dorsal fin morphology of c.pdf;/Users/b3020111/Zotero/storage/87YE9ZSP/3415.html}
}

@inproceedings{muller_identifying_2019,
  title = {Identifying {{Mislabeled Instances}} in {{Classification Datasets}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {M{\"u}ller, Nicolas M. and Markert, Karla},
  year = {2019},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2019.8851920},
  abstract = {A key requirement for supervised machine learning is labeled training data, which is created by annotating unlabeled data with the appropriate class. Because this process can in many cases not be done by machines, labeling needs to be performed by human domain experts. This process tends to be expensive both in time and money, and is prone to errors. Additionally, reviewing an entire labeled dataset manually is often prohibitively costly, so many real world datasets contain mislabeled instances.To address this issue, we present in this paper a non-parametric end-to-end pipeline to find mislabeled instances in numerical, image and natural language datasets. We evaluate our system quantitatively by adding a small number of label noise to 29 datasets, and show that we find mislabeled instances with an average precision of more than 0.84 when reviewing our system's top 1\% recommendation. We then apply our system to publicly available datasets and find mislabeled instances in CIFAR-100, Fashion-MNIST, and others. Finally, we publish the code and an applicable implementation of our approach.},
  keywords = {Labeling,Machine learning,Neural networks,Pipelines,Tools,Training,Training data},
  file = {/Users/b3020111/Zotero/storage/THCMDT9E/MÃ¼ller and Markert - 2019 - Identifying Mislabeled Instances in Classification.pdf;/Users/b3020111/Zotero/storage/R83IFBIF/stamp.html}
}

@inproceedings{nagar_concept_2020,
  title = {Concept {{Drift Detection}} for {{Multivariate Data Streams}} and {{Temporal Segmentation}} of {{Daylong Egocentric Videos}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Nagar, Pravin and Khemka, Mansi and Arora, Chetan},
  year = {2020},
  month = oct,
  pages = {1065--1074},
  publisher = {{ACM}},
  address = {{Seattle WA USA}},
  doi = {10.1145/3394171.3413713},
  urldate = {2022-07-15},
  abstract = {The long and unconstrained nature of egocentric videos makes it imperative to use temporal segmentation as an important preprocessing step for many higher-level inference tasks. Activities of the wearer in an egocentric video typically span over hours and are often separated by slow, gradual changes. Furthermore, the change of camera viewpoint due to the wearer's head motion causes frequent and extreme, but, spurious scene changes. The continuous nature of boundaries makes it difficult to apply traditional Markov Random Field (MRF) pipelines relying on temporal discontinuity, whereas deep Long Short Term Memory (LSTM) networks gather context only upto a few hundred frames, rendering them ineffective for egocentric videos. In this paper, we present a novel unsupervised temporal segmentation technique especially suited for day-long egocentric videos. We formulate the problem as detecting concept drift in a time-varying, non i.i.d. sequence of frames. Statistically bounded thresholds are calculated to detect concept drift between two temporally adjacent multivariate data segments with different underlying distributions while establishing guarantees on false positives. Since the derived threshold indicates confidence in the prediction, it can also be used to control the granularity of the output segmentation. Using our technique, we report significantly improved state of the art f-measure for daylong egocentric video datasets, as well as photostream datasets derived {${_\ast}$}The work was done during the internship with Prof. Chetan Arora.},
  isbn = {978-1-4503-7988-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/CIH49NK2/Nagar et al. - 2020 - Concept Drift Detection for Multivariate Data Stre.pdf}
}

@article{nasir_dwelling_2022,
  title = {Dwelling {{Type Classification}} for {{Disaster Risk Assessment Using Satellite Imagery}}},
  author = {Nasir, Md and Sederholm, Tina and Sharma, Anshu and Mallu, Sundeep Reddy and Ghatage, Sumedh Ranjan and Dodhia, Rahul and Ferres, Juan Lavista},
  year = {2022},
  month = nov,
  journal = {arXiv:2211.11636 [cs.LG]},
  eprint = {2211.11636},
  primaryclass = {cs.LG},
  urldate = {2022-12-09},
  abstract = {Vulnerability and risk assessment of neighborhoods is essential for effective disaster preparedness. Existing traditional systems, due to dependency on time-consuming and cost-intensive field surveying, do not provide a scalable way to decipher warnings and assess the precise extent of the risk at a hyper-local level. In this work, machine learning was used to automate the process of identifying dwellings and their type to build a potentially more effective disaster vulnerability assessment system. First, satellite imageries of low-income settlements and vulnerable areas in India were used to identify 7 different dwelling types. Specifically, we formulated the dwelling type classification as a semantic segmentation task and trained a U-net based neural network model, namely TernausNet, with the data we collected. Then a risk score assessment model was employed, using the determined dwelling type along with an inundation model of the regions. The entire pipeline was deployed to multiple locations prior to natural hazards in India in 2020. Post hoc ground-truth data from those regions was collected to validate the efficacy of this model which showed promising performance. This work can aid disaster response organizations and communities at risk by providing household-level risk information that can inform preemptive actions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/DRNDKMUV/Nasir et al. - 2022 - Dwelling Type Classification for Disaster Risk Ass.pdf}
}

@article{neven_instance_2019,
  title = {Instance {{Segmentation}} by {{Jointly Optimizing Spatial Embeddings}} and {{Clustering Bandwidth}}},
  author = {Neven, Davy and De Brabandere, Bert and Proesmans, Marc and Van Gool, Luc},
  year = {2019},
  month = aug,
  journal = {arXiv:1906.11109 [cs]},
  eprint = {1906.11109},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5\textbackslash\% improvement over Mask R-CNN) at more than 10 fps on 2MP images. Code will be available at https://github.com/davyneven/SpatialEmbeddings .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/CPXAEZ6F/Neven et al. - 2019 - Instance Segmentation by Jointly Optimizing Spatia.pdf;/Users/b3020111/Zotero/storage/HFUVFTJX/1906.html}
}

@article{neven_instance_2019-1,
  title = {Instance {{Segmentation}} by {{Jointly Optimizing Spatial Embeddings}} and {{Clustering Bandwidth}}},
  author = {Neven, Davy and De Brabandere, Bert and Proesmans, Marc and Van Gool, Luc},
  year = {2019},
  month = aug,
  journal = {arXiv:1906.11109 [cs]},
  eprint = {1906.11109},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5\textbackslash\% improvement over Mask R-CNN) at more than 10 fps on 2MP images. Code will be available at https://github.com/davyneven/SpatialEmbeddings .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/VN5L9724/Neven et al. - 2019 - Instance Segmentation by Jointly Optimizing Spatia.pdf;/Users/b3020111/Zotero/storage/84GF37UV/1906.html}
}

@article{neyshabur_exploring_2017,
  title = {Exploring {{Generalization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
  year = {2017},
  journal = {arXiv:1706.08947 [cs.LG]},
  eprint = {1706.08947},
  primaryclass = {cs.LG},
  pages = {10},
  abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/P85W287G/Neyshabur et al. - Exploring Generalization in Deep Learning.pdf}
}

@article{ng_facebook_2019,
  title = {Facebook {{FAIR}}'s {{WMT19 News Translation Task Submission}}},
  author = {Ng, Nathan and Yee, Kyra and Baevski, Alexei and Ott, Myle and Auli, Michael and Edunov, Sergey},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.06616 [cs.CL]},
  eprint = {1907.06616},
  primaryclass = {cs.CL},
  urldate = {2022-09-27},
  abstract = {This paper describes Facebook FAIR's submission to the WMT19 shared news translation task. We participate in two language pairs and four language directions, English {$\leftrightarrow$} German and English {$\leftrightarrow$} Russian. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit which rely on sampled backtranslations. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the human evaluation campaign. On En\textrightarrow De, our system significantly outperforms other systems as well as human translations. This system improves upon our WMT'18 submission by 4.5 BLEU points.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/b3020111/Zotero/storage/39G2Z7HL/Ng et al. - 2019 - Facebook FAIR's WMT19 News Translation Task Submis.pdf}
}

@inproceedings{nguyen_background_2016,
  title = {Background Removal for Improving Saliency-Based Person Re-Identification},
  booktitle = {2016 {{Eighth International Conference}} on {{Knowledge}} and {{Systems Engineering}} ({{KSE}})},
  author = {Nguyen, Thuy Binh and Van Phu, Pham and Le, Thi-Lan and Vo Le, Cuong},
  year = {2016},
  month = oct,
  pages = {339--344},
  doi = {10.1109/KSE.2016.7758077},
  abstract = {This paper presents background removal methods to increase the accuracy of saliency-based person re-identification. After evaluating the current global salience algorithm, we found that wrong matching appears when (1) images of different people have a similar or the same background and/or (2) salience on the backgrounds of various images are similar. To prove the maximum theoretical accuracy of the global saliency method when using background removal, we use a manual method with support of an interactive segmentation tool. Another method is to use an ellipse to localize a human body region. This method is a preliminary step for confirming a possibility of applying an automatic technique. The human body region is automatically determined by utilizing a local salience method named GBVS with an adaptive threshold for every image in VIPeR dataset. Preliminary results show that when applying the three background removal methods, the accuracy at rank 1 of CMC curve increases from 20.00\% to 27.18\%, 24.81\% and 23.80\% for interactive segmentation, elliptical window and adaptive local salience methods, respectively.},
  keywords = {Cameras,Feature extraction,Image segmentation,Lighting,Measurement,Probes,Robustness},
  file = {/Users/b3020111/Zotero/storage/2QCF5XX2/Nguyen et al. - 2016 - Background removal for improving saliency-based pe.pdf;/Users/b3020111/Zotero/storage/HFRHZFHJ/7758077.html}
}

@inproceedings{nguyen_background_2016-1,
  title = {Background Removal for Improving Saliency-Based Person Re-Identification},
  booktitle = {2016 {{Eighth International Conference}} on {{Knowledge}} and {{Systems Engineering}} ({{KSE}})},
  author = {Nguyen, Thuy Binh and Van Phu, Pham and Le, Thi-Lan and Vo Le, Cuong},
  year = {2016},
  month = oct,
  pages = {339--344},
  doi = {10.1109/KSE.2016.7758077},
  abstract = {This paper presents background removal methods to increase the accuracy of saliency-based person re-identification. After evaluating the current global salience algorithm, we found that wrong matching appears when (1) images of different people have a similar or the same background and/or (2) salience on the backgrounds of various images are similar. To prove the maximum theoretical accuracy of the global saliency method when using background removal, we use a manual method with support of an interactive segmentation tool. Another method is to use an ellipse to localize a human body region. This method is a preliminary step for confirming a possibility of applying an automatic technique. The human body region is automatically determined by utilizing a local salience method named GBVS with an adaptive threshold for every image in VIPeR dataset. Preliminary results show that when applying the three background removal methods, the accuracy at rank 1 of CMC curve increases from 20.00\% to 27.18\%, 24.81\% and 23.80\% for interactive segmentation, elliptical window and adaptive local salience methods, respectively.},
  keywords = {Cameras,Feature extraction,Image segmentation,Lighting,Measurement,Probes,Robustness},
  file = {/Users/b3020111/Zotero/storage/53JJ8DGR/Nguyen et al. - 2016 - Background removal for improving saliency-based pe.pdf;/Users/b3020111/Zotero/storage/PEAQBRJD/7758077.html}
}

@inproceedings{nguyen_hand_2018,
  title = {Hand Segmentation under Different Viewpoints by Combination of {{Mask R-CNN}} with Tracking},
  booktitle = {2018 5th {{Asian Conference}} on {{Defense Technology}} ({{ACDT}})},
  author = {Nguyen, D. and Le, T. and Tran, T. and Vu, H. and Le, T. and Doan, H.},
  year = {2018},
  month = oct,
  pages = {14--20},
  doi = {10.1109/ACDT.2018.8593130},
  abstract = {This paper presents a new method for hand segmentation from images and video. The method based mainly on an advanced technique for instance segmentation (Mask RCNN) which has been shown very efficient in segmentation task on COCO dataset. However, Mask R-CNN has some limitations. It works on still images, so cannot explore temporal information of the object of interest such as dynamic hand gestures. Second Mask R-CNN usually fails to detect object suffered from motion blur at low resolution as hand. Our proposed method improves Mask R-CNN by integrating a Mean Shift tracker that tracks hands in consecutive frames and removes false alarms. We have also trained another model of Mask R-CNN on cropped regions extended from hand centers to obtain a better accuracy of segmentation. We have evaluated both methods on a self-constructed multi-view dataset of hand gestures and show how robust these methods are to view point changes. Experimental results showed that our method achieved better performance than the original Mask R-CNN under different viewpoints.},
  keywords = {computer vision,deep learning,Detectors,dynamic hand gestures,feature extraction,Feature extraction,feedforward neural nets,gesture recognition,hand centers,hand segmentation,image motion analysis,Image resolution,image segmentation,Image segmentation,instance segmentation,learning (artificial intelligence),Mask RCNN,mean shift tracker,motion blur,neural network,Neural networks,object detection,segmentation task,Skin,tracking,Tracking,video signal processing}
}

@article{nie_attention_2020,
  title = {Attention {{Mask R-CNN}} for {{Ship Detection}} and {{Segmentation From Remote Sensing Images}}},
  author = {Nie, X. and Duan, M. and Ding, H. and Hu, B. and Wong, E. K.},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {9325--9334},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2964540},
  abstract = {In recent years, ship detection in satellite remote sensing images has become an important research topic. Most existing methods detect ships by using a rectangular bounding box but do not perform segmentation down to the pixel level. This paper proposes a ship detection and segmentation method based on an improved Mask R-CNN model. Our proposed method can accurately detect and segment ships at the pixel level. By adding a bottom-up structure to the FPN structure of Mask R-CNN, the path between the lower layers and the topmost layer is shortened, allowing the lower layer features to be more effectively utilized at the top layer. In the bottom-up structure, we use channel-wise attention to assign weights in each channel and use the spatial attention mechanism to assign a corresponding weight at each pixel in the feature maps. This allows the feature maps to respond better to the target's features. Using our method, the detection and segmentation mAPs increased from 70.6\% and 62.0\% to 76.1\% and 65.8\%, respectively.},
  keywords = {channel-wise attention,Computer vision,convolutional neural nets,Deep learning,Feature extraction,feature maps,geophysical image processing,image classification,image segmentation,Image segmentation,improved mask R-CNN model,lower layer features,Marine vehicles,object detection,Object detection,object segmentation,pixel level,remote sensing,Remote sensing,satellite remote sensing images,Satellites,segmentation method,ship detection,ships},
  file = {/Users/b3020111/Zotero/storage/ITVM7QQJ/Nie et al. - 2020 - Attention Mask R-CNN for Ship Detection and Segmen.pdf}
}

@inproceedings{nie_inshore_2018,
  title = {Inshore {{Ship Detection Based}} on {{Mask R-CNN}}},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Nie, S. and Jiang, Z. and Zhang, H. and Cai, B. and Yao, Y.},
  year = {2018},
  month = jul,
  pages = {693--696},
  issn = {2153-7003},
  doi = {10.1109/IGARSS.2018.8519123},
  abstract = {Inshore ship detection is a popular research domain for optical remote sensing image understanding with many applications in harbor management. However, recent approaches on inshore ship detection depend heavily on hand-crafted features, which need a complicated procedure. In this paper, we propose a new method to achieve inshore ship detection based on Mask R-CNN. We introduce Soft-Non-Maximum Suppression (Soft-NMS) into our framework to improve the robustness to nearby inshore ships. Both battleships and merchantships can be detected in our framework. Furthermore, our framework can also obtain the binary masks of inshore ships. Experimental results on a dataset collected from Google Earth have quantitatively and qualitatively demonstrated the effectiveness of our approach.},
  keywords = {binary masks,feature extraction,Feature extraction,geophysical image processing,Google Earth,hand-crafted features,harbor management,Image segmentation,inshore ship,inshore ship detection,Marine vehicles,mask R-CNN,Mask R-CNN,nearby inshore ships,object detection,Object detection,oceanographic techniques,optical remote sensing image understanding,Proposals,remote sensing,Remote sensing,remote sensing images,Shape,ships,soft-NMS,soft-nonmaximum suppression},
  file = {/Users/b3020111/Zotero/storage/8429LIIC/8519123.html}
}

@inproceedings{nita_cnn-based_2020,
  title = {{{CNN-based}} Object Detection and Segmentation for Maritime Domain Awareness},
  booktitle = {Artificial {{Intelligence}} and {{Machine Learning}} in {{Defense Applications II}}},
  author = {Nita, Cornelia and Vandewal, Marijke},
  editor = {Dijk, Judith},
  year = {2020},
  month = sep,
  pages = {4},
  publisher = {{SPIE}},
  address = {{Online Only, United Kingdom}},
  doi = {10.1117/12.2573287},
  urldate = {2020-10-08},
  abstract = {Deep learning algorithms have been proven to be a powerful tool in image and video processing for security and surveillance operations. In a maritime environment, the fusion of electro-optical sensor data with human intelligence plays an important role to counter the security issues. For instance, the situational awareness can be enhanced through an automated system that generates reports on ship identity and signature together with detecting the changes on naval vessels activity. To date, various studies have been set out to explore the performance of deep neural networks using a ship signature database. In the current study, we investigate the Mask R-CNN method to address not only the naval vessel detection using bounding boxes, but also obtaining their segmentation masks. We train and validate the model on data captured by an on-board camera covering the visible spectral band under various weather and light conditions. The experimental results show that Mask RCNN provides high confidence scores on challenging scenarios with a mean average precision of 86.4\%. However, the precision of the segmentation mask is slightly deteriorated when the ships are adjacent to the border of the captured scene. Moreover, the network tested on thermal images indicates a decrease in detection and segmentation performance since the training data distribution is not representative enough.},
  isbn = {978-1-5106-3899-0 978-1-5106-3900-3},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/8FQZ7AEN/Nita and Vandewal - 2020 - CNN-based object detection and segmentation for ma.pdf;/Users/b3020111/Zotero/storage/B5G87CVY/Nita and Vandewal - 2020 - CNN-based object detection and segmentation for ma.pdf}
}

@misc{noauthor_016pdf_nodate,
  title = {016.Pdf},
  journal = {Google Docs},
  urldate = {2022-05-03},
  howpublished = {https://drive.google.com/file/d/172bFnpbbkYNu8k2li4H7dkSWyMJq7hiz/view?usp=sharing\&usp=embed\_facebook},
  file = {/Users/b3020111/Zotero/storage/8HHKVJTB/view.html}
}

@misc{noauthor_automatic_nodate,
  title = {Automatic Knee Meniscus Tear Detection and Orientation Classification with {{Mask-RCNN}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.diii.2019.03.002},
  urldate = {2021-01-20},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S2211568419300580?token=11F554F3D180A3E57FD831CAB889070670C0F98FAE70AC86E0764841D02AA8154A3E0544ED851068237286B12A966ABF},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/GXN8FI62/S2211568419300580.html}
}

@book{noauthor_computer_2014,
  title = {Computer Vision - {{ECCV}} 2014: 13th {{European Conference}}, {{Zurich}}, {{Switzerland}}, {{September}} 6-12, 2014, {{Proceedings}}, Part i},
  shorttitle = {Computer Vision - {{ECCV}} 2014},
  year = {2014},
  series = {Lecture Notes in Computer Science},
  edition = {1st edition},
  number = {8689},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-3-319-10589-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/VADUBVSN/2014 - Computer vision - ECCV 2014 13th European Confere.pdf}
}

@book{noauthor_fishery_1971,
  title = {Fishery {{Bulletin}}},
  year = {1971},
  publisher = {{National Marine Fisheries Service}},
  langid = {english}
}

@misc{noauthor_hierarchical_nodate,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}: {{Journal}} of the {{American Statistical Association}}: {{Vol}} 58, {{No}} 301},
  urldate = {2020-02-12},
  howpublished = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
  file = {/Users/b3020111/Zotero/storage/UI4B2BBW/01621459.1963.html}
}

@article{noauthor_iaaam_2018,
  title = {{{IAAAM}} 2019},
  year = {2018},
  month = may,
  journal = {VIN.com},
  annotation = {Context Object: ctx\_ver=Z39.88-2004\&rft\_val\_fmt=info\%3Aofi\%2Ffmt\%3Akev\%3Amtx\%3Ajournal\&rft\_id=https://www.vin.com/doc/?id\%3D9032708\&rft.atitle=IAAAM+2019\&rft.jtitle=VIN.com\&rft.date=2018-05-19},
  file = {/Users/b3020111/Zotero/storage/VRGQFI6U/defaultadv1.html}
}

@misc{noauthor_integrating_nodate,
  title = {Integrating Multiple Data Sources to Assess the Distribution and Abundance of Bottlenose Dolphins {{Tursiops}} Truncatus in {{Scottish}} Waters - {{Cheney}} - 2013 - {{Mammal Review}} - {{Wiley Online Library}}},
  urldate = {2021-01-07},
  howpublished = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1365-2907.2011.00208.x},
  file = {/Users/b3020111/Zotero/storage/38ARPEVT/Integrating multiple data sources to assess the di.pdf;/Users/b3020111/Zotero/storage/3F3VS4I9/j.1365-2907.2011.00208.html}
}

@misc{noauthor_integrating_nodate-1,
  title = {Integrating Multiple Data Sources to Assess the Distribution and Abundance of Bottlenose Dolphins {{Tursiops}} Truncatus in {{Scottish}} Waters - {{Cheney}} - 2013 - {{Mammal Review}} - {{Wiley Online Library}}},
  urldate = {2021-01-07},
  howpublished = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1365-2907.2011.00208.x}
}

@misc{noauthor_marine_2009,
  title = {Marine and {{Coastal Access Act}}},
  year = {2009},
  month = nov,
  number = {2009 c 23}
}

@book{noauthor_notitle_nodate,
  type = {Book}
}

@misc{noauthor_photo-identification_2015,
  title = {Photo-Identification},
  year = {2015},
  month = jun,
  journal = {Cetacea Association},
  urldate = {2021-01-07},
  abstract = {PHOTO-IDENTIFICATION AT ASSOCIACI\'O CET\`ACEA Associaci\'o Cet\`acea carries out the Photo-identification Project: Whales and dolphins along the Catalan coast since 2014. The main goal of the project is t\ldots},
  langid = {american}
}

@misc{noauthor_photo-identification_nodate,
  title = {Photo-Identification | {{Cetacea Association}}},
  urldate = {2021-01-07},
  howpublished = {http://www.associaciocetacea.org/en/research/photo-identification/},
  file = {/Users/b3020111/Zotero/storage/SNCNV7D2/photo-identification.html}
}

@misc{noauthor_sift-based_nodate,
  title = {A {{SIFT-based}} Software System for the Photo-Identification of the {{Risso}}'s Dolphin | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ecoinf.2019.01.006},
  urldate = {2021-01-07},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S1574954118301377?token=2C1F301064C2B1DAE9FC3E42617BDC8F3A754A22E9CBAC1EBDA069CDE0EF1E361F59A8D187F071A928E979317304C6B1},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/34XNP6FW/S1574954118301377.html}
}

@misc{noauthor_wildbookorgibeis-curvrank-module_2020,
  title = {{{WildbookOrg}}/Ibeis-Curvrank-Module},
  year = {2020},
  month = jan,
  urldate = {2020-02-12},
  abstract = {Python module that wraps https://github.com/hjweide/dolphin-identification},
  copyright = {Apache-2.0},
  howpublished = {Wildbook Organization}
}

@inproceedings{noh_learning_2015,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  year = {2015},
  month = dec,
  pages = {1520--1528},
  publisher = {{IEEE}},
  address = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.178},
  urldate = {2023-04-20},
  abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/6CDH5SHL/Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmen.pdf}
}

@inproceedings{nokland_direct_2016,
  title = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {N{\o}kland, Arild},
  year = {2016},
  pages = {1037--1045}
}

@article{norouzzadeh_automatically_2018,
  title = {Automatically Identifying, Counting, and Describing Wild Animals in Camera-Trap Images with Deep Learning},
  author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
  year = {2018},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {25},
  pages = {E5716-E5725},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1719367115},
  urldate = {2021-01-05},
  abstract = {Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into ``big data'' sciences. Motion-sensor ``camera traps'' enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {$>$}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {$>$}8.4 y (i.e., {$>$}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
  chapter = {PNAS Plus},
  copyright = {Copyright \textcopyright{} 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {29871948},
  keywords = {artificial intelligence,camera-trap images,deep learning,deep neural networks,wildlife ecology},
  file = {/Users/b3020111/Zotero/storage/EESET394/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describin.pdf;/Users/b3020111/Zotero/storage/4M7ZDYL5/E5716.html}
}

@article{norouzzadeh_deep_2019,
  title = {A Deep Active Learning System for Species Identification and Counting in Camera Trap Images},
  author = {Norouzzadeh, Mohammad Sadegh and Morris, Dan and Beery, Sara and Joshi, Neel and Jojic, Nebojsa and Clune, Jeff},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.09716 [cs, eess, stat]},
  eprint = {1910.09716},
  primaryclass = {cs, eess, stat},
  urldate = {2021-01-05},
  abstract = {Biodiversity conservation depends on accurate, up-to-date information about wildlife population distributions. Motion-activated cameras, also known as camera traps, are a critical tool for population surveys, as they are cheap and non-intrusive. However, extracting useful information from camera trap images is a cumbersome process: a typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical information is often lost due to resource limitations, and critical conservation questions may be answered too slowly to support decision-making. Computer vision is poised to dramatically increase efficiency in image-based biodiversity surveys, and recent studies have harnessed deep learning techniques for automatic information extraction from camera trap images. However, the accuracy of results depends on the amount, quality, and diversity of the data available to train models, and the literature has focused on projects with millions of relevant, labeled training images. Many camera trap projects do not have a large set of labeled images and hence cannot benefit from existing machine learning techniques. Furthermore, even projects that do have labeled data from similar ecosystems have struggled to adopt deep learning methods because image classification models overfit to specific image backgrounds (i.e., camera locations). In this paper, we focus not on automating the labeling of camera trap images, but on accelerating this process. We combine the power of machine intelligence and human intelligence to build a scalable, fast, and accurate active learning system to minimize the manual work required to identify and count animals in camera trap images. Our proposed scheme can match the state of the art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labeling effort by over 99.5\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/9JRZNE8U/Norouzzadeh et al. - 2019 - A deep active learning system for species identifi.pdf;/Users/b3020111/Zotero/storage/CKHKIVY8/1910.html}
}

@article{norris_tagging_1970,
  title = {A {{Tagging Method}} for {{Small Cetaceans}}},
  author = {Norris, Kenneth S. and Pryor, Karen W.},
  year = {1970},
  month = aug,
  journal = {Journal of Mammalogy},
  volume = {51},
  number = {3},
  pages = {609},
  issn = {00222372},
  doi = {10.2307/1378401},
  urldate = {2022-09-20},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/K6ZVVB47/Norris and Pryor - 1970 - A Tagging Method for Small Cetaceans.pdf}
}

@article{osband_risk_2016,
  title = {Risk versus {{Uncertainty}} in {{Deep Learning}}: {{Bayes}}, {{Bootstrap}} and the {{Dangers}} of {{Dropout}}},
  author = {Osband, Ian},
  year = {2016},
  pages = {5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/AY8P28IW/Osband - Risk versus Uncertainty in Deep Learning Bayes, B.pdf}
}

@article{oswald_tool_2007,
  title = {A Tool for Real-Time Acoustic Species Identification of Delphinid Whistles},
  author = {Oswald, Julie N. and Rankin, Shannon and Barlow, Jay and Lammers, Marc O.},
  year = {2007},
  month = jul,
  journal = {The Journal of the Acoustical Society of America},
  volume = {122},
  number = {1},
  pages = {587--595},
  issn = {0001-4966},
  doi = {10.1121/1.2743157},
  urldate = {2019-01-08},
  langid = {english}
}

@article{pan_survey_2010,
  title = {A Survey on Transfer Learning},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  journal = {IEEE Transactions on knowledge and data engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359}
}

@article{pan_towards_2020,
  title = {Towards Zero-Shot Learning Generalization via a Cosine Distance Loss},
  author = {Pan, Chongyu and Huang, Jian and Hao, Jianguo and Gong, Jianxing},
  year = {2020},
  month = mar,
  journal = {Neurocomputing},
  volume = {381},
  pages = {167--176},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.11.011},
  urldate = {2023-04-14},
  langid = {english}
}

@inproceedings{pantazis_focus_2021,
  title = {Focus on the {{Positives}}: {{Self-Supervised Learning}} for {{Biodiversity Monitoring}}},
  shorttitle = {Focus on the {{Positives}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Pantazis, Omiros and Brostow, Gabriel J. and Jones, Kate E. and Aodha, Oisin Mac},
  year = {2021},
  month = oct,
  pages = {10563--10572},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.01041},
  urldate = {2023-03-16},
  abstract = {We address the problem of learning self-supervised representations from unlabeled image collections. Unlike existing approaches that attempt to learn useful features by maximizing similarity between augmented versions of each input image or by speculatively picking negative samples, we instead also make use of the natural variation that occurs in image collections that are captured using static monitoring cameras. To achieve this, we exploit readily available context data that encodes information such as the spatial and temporal relationships between the input images. We are able to learn representations that are surprisingly effective for downstream supervised classification, by first identifying high probability positive pairs at training time, i.e. those images that are likely to depict the same visual concept. For the critical task of global biodiversity monitoring, this results in image features that can be adapted to challenging visual species classification tasks with limited human supervision. We present results on four different camera trap image collections, across three different families of self-supervised learning methods, and show that careful image selection at training time results in superior performance compared to existing baselines such as conventional self-supervised training and transfer learning.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/5RVWDTIY/Pantazis et al. - 2021 - Focus on the Positives Self-Supervised Learning f.pdf}
}

@misc{pantazis_svl-adapter_2022,
  title = {{{SVL-Adapter}}: {{Self-Supervised Adapter}} for {{Vision-Language Pretrained Models}}},
  shorttitle = {{{SVL-Adapter}}},
  author = {Pantazis, Omiros and Brostow, Gabriel and Jones, Kate and Mac Aodha, Oisin},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03794},
  eprint = {2210.03794},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-16},
  abstract = {Vision-language models such as CLIP are pretrained on large volumes of internet sourced image and text pairs, and have been shown to sometimes exhibit impressive zero- and low-shot image classification performance. However, due to their size, finetuning these models on new datasets can be prohibitively expensive, both in terms of the supervision and compute required. To combat this, a series of light-weight adaptation methods have been proposed to efficiently adapt such models when limited supervision is available. In this work, we show that while effective on internet-style datasets, even those remedies under-deliver on classification tasks with images that differ significantly from those commonly found online. To address this issue, we present a new approach called SVL-Adapter that combines the complementary strengths of both vision-language pretraining and self-supervised representation learning. We report an average classification accuracy improvement of 10\% in the low-shot setting when compared to existing methods, on a set of challenging visual classification tasks. Further, we present a fully automatic way of selecting an important blending hyperparameter for our model that does not require any held-out labeled validation data. Code for our project is available here: https://github.com/omipan/svl\_adapter.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/6PA6WW6E/Pantazis et al. - 2022 - SVL-Adapter Self-Supervised Adapter for Vision-La.pdf}
}

@inproceedings{parkhi_cats_2012,
  title = {Cats and Dogs},
  booktitle = {2012 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
  year = {2012},
  pages = {3498--3505},
  publisher = {{IEEE}},
  isbn = {1-4673-1228-2}
}

@misc{participation_marine_nodate,
  type = {Text},
  title = {Marine and {{Coastal Access Act}} 2009},
  author = {Participation, Expert},
  publisher = {{Statute Law Database}},
  urldate = {2022-04-14},
  abstract = {An Act to make provision in relation to marine functions and activities; to make provision about migratory and freshwater fish; to make provision for and in connection with the establishment of an English coastal walking route and of rights of access to land near the English coast; to enable the making of Assembly Measures in relation to Welsh coastal routes for recreational journeys and rights of access to land near the Welsh coast; to make further provision in relation to Natural England and the Countryside Council for Wales; to make provision in relation to works which are detrimental to navigation; to amend the Harbours Act 1964; and for connected purposes.},
  howpublished = {https://www.legislation.gov.uk/ukpga/2009/23/contents},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/5NMXT9GK/contents.html}
}

@article{paszke_automatic_2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  journal = {Proceedings of the 31st Conference on Neural Information Processing Systems},
  pages = {4},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch \textemdash{} a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/C7IV49XY/Paszke et al. - Automatic differentiation in PyTorch.pdf}
}

@inproceedings{paterakis_deep_2017,
  title = {Deep Learning versus Traditional Machine Learning Methods for Aggregated Energy Demand Prediction},
  booktitle = {2017 {{IEEE PES Innovative Smart Grid Technologies Conference Europe}} ({{ISGT-Europe}})},
  author = {Paterakis, Nikolaos G. and Mocanu, Elena and Gibescu, Madeleine and Stappers, Bart and {van Alst}, Walter},
  year = {2017},
  month = sep,
  pages = {1--6},
  doi = {10.1109/ISGTEurope.2017.8260289},
  abstract = {In this paper the more advanced, in comparison with traditional machine learning approaches, deep learning methods are explored with the purpose of accurately predicting the aggregated energy consumption. Despite the fact that a wide range of machine learning methods have been applied to probabilistic energy prediction, the deep learning ones certainly represent the state-of-the-art artificial intelligence methods with remarkable success in a spectrum of practical applications. In particular, the use of Multi Layer Perceptrons, recently enhanced with deep learning capabilities, is proposed. Furthermore, its performance is compared with the most commonly used machine learning methods, such as Support Vector Machines, Gaussian Processes, Regression Trees, Ensemble Boosting and Linear Regression. The analysis of the day-ahead energy prediction demonstrates that different prediction methods present significantly different levels of accuracy in the case of a challenging dataset that comprises an interesting mix of consumers, wind and solar generation. The results show that Multi Layer Perceptrons outperform all the eight methods used as a benchmark in this study.},
  keywords = {Buildings,deep learning,energy consumption,Energy consumption,energy prediction,forecasting,Learning systems,machine learning,Machine learning,Support vector machines,Wind forecasting},
  file = {/Users/b3020111/Zotero/storage/DJIISKAA/Paterakis et al. - 2017 - Deep learning versus traditional machine learning .pdf;/Users/b3020111/Zotero/storage/QYRITA2D/8260289.html}
}

@inproceedings{patterson_fine-grained_2005,
  title = {Fine-Grained Activity Recognition by Aggregating Abstract Object Usage},
  booktitle = {Ninth {{IEEE International Symposium}} on {{Wearable Computers}} ({{ISWC}}'05)},
  author = {Patterson, D. J. and Fox, D. and Kautz, H. and Philipose, M.},
  year = {2005},
  month = oct,
  pages = {44--51},
  doi = {10.1109/ISWC.2005.22},
  abstract = {In this paper we present results related to achieving finegrained activity recognition for context-aware computing applications. We examine the advantages and challenges of reasoning with globally unique object instances detected by an RFID glove. We present a sequence of increasingly powerful probabilistic graphical models for activity recognition. We show the advantages of adding additional complexity and conclude with a model that can reason tractably about aggregated object instances and gracefully generalizes from object instances to their classes by using abstraction smoothing. We apply these models to data collected from a morning household routine.},
  keywords = {abstract object usage,abstraction smoothing,Character recognition,computer vision,context-aware computing RFID glove,fine-grained activity recognition,Inference algorithms,inference mechanisms,Machine vision,mobile computing,Multimodal sensors,Object detection,planning (artificial intelligence),probabilistic graphical model,radiofrequency identification,Radiofrequency identification,Robustness,Sensor phenomena and characterization,Wearable computers,Wearable sensors},
  file = {/Users/b3020111/Zotero/storage/IIMGYJ6S/Patterson et al. - 2005 - Fine-grained activity recognition by aggregating a.pdf;/Users/b3020111/Zotero/storage/DSTJVV2V/1550785.html}
}

@techreport{payne_long_1986,
  title = {Long Term Behavioral Studies of the Southern Right Whale ({{Eubalaena}} Australis)},
  author = {Payne, Roger},
  year = {1986},
  number = {10},
  pages = {161--167},
  institution = {{International Whaling Commission}},
  file = {/Users/b3020111/Zotero/storage/AV3L6GIQ/Payne - 1986 - Long term behavioral studies of the southern right.pdf}
}

@article{pedersen_finding_2023,
  title = {Finding {{Nemo}}'s {{Giant Cousin}}: {{Keypoint Matching}} for {{Robust Re-Identification}} of {{Giant Sunfish}}},
  shorttitle = {Finding {{Nemo}}'s {{Giant Cousin}}},
  author = {Pedersen, Malte and Nyegaard, Marianne and Moeslund, Thomas B.},
  year = {2023},
  month = apr,
  journal = {Journal of Marine Science and Engineering},
  volume = {11},
  number = {5},
  pages = {889},
  issn = {2077-1312},
  doi = {10.3390/jmse11050889},
  urldate = {2023-04-26},
  abstract = {The Giant Sunfish (Mola alexandrini) has unique patterns on its body, which allow for individual identification. By continuously gathering and matching images, it is possible to monitor and track individuals across location and time. However, matching images manually is a tedious and time-consuming task. To automate the process, we propose a pipeline based on finding and matching keypoints between image pairs. We evaluate our pipeline with four different keypoint descriptors, namely ORB, SIFT, RootSIFT, and SuperPoint, and demonstrate that the number of matching keypoints between a pair of images is a strong indicator for the likelihood that they contain the same individual. The best results are obtained with RootSIFT, which achieves an mAP of 75.91\% on our test dataset (TinyMola+) without training or fine-tuning any parts of the pipeline. Furthermore, we show that the pipeline generalizes to other domains, such as re-identification of seals and cows. Lastly, we discuss the impracticality of a ranking-based output for real-life tasks and propose an alternative approach by viewing re-identification as a binary classification. We show that the pipeline can be easily modified with minimal fine-tuning to provide a binary output with a precision of 98\% and recall of 44\% on the TinyMola+ dataset, which basically eliminates the need for time-consuming manual verification on nearly half the dataset.},
  langid = {english}
}

@book{perrin_encyclopedia_2009,
  title = {Encyclopedia of Marine Mammals},
  author = {Perrin, W and W{\"u}rsig, B. and Thewissen, {\relax JGM}},
  year = {2009},
  publisher = {{Academic Press}},
  isbn = {978-0-12-373553-9}
}

@inproceedings{pobar_detection_2019,
  title = {Detection of the Leading Player in Handball Scenes Using {{Mask R-CNN}} and {{STIPS}}},
  booktitle = {Eleventh {{International Conference}} on {{Machine Vision}} ({{ICMV}} 2018)},
  author = {Pobar, M. and {Iva{\v s}i{\'c}-Kos}, Marina},
  year = {2019},
  month = mar,
  volume = {11041},
  pages = {110411V},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2522668},
  urldate = {2020-12-03},
  abstract = {In team sports scenes, recorded during training and lessons, it is common to have many players on the court, each with his own ball performing different actions. Our goal is to detect all players in the handball court and determine the leading player who performs the given handball technique such as a shooting at the goal, catching a ball or dribbling. This is a very challenging task for which, apart from an accurate object detector that is able to deal with cluttered scenes with many objects, partially occluded and with bad illumination, additional information is needed to determine the leading player. Therefore, we propose a leading player detector method combining the Mask R-CNN object detector and spatiotemporal interest points, referred to as MR-CNN+STIPs. The performance of the proposed leading player detector is evaluated on a custom sports video dataset acquired during handball training lessons. The performance of the detector in different conditions will be discussed.},
  file = {/Users/b3020111/Zotero/storage/N52I27HR/12.2522668.html}
}

@article{pomeroy_assessing_2015,
  title = {Assessing Use of and Reaction to Unmanned Aerial Systems in Gray and Harbor Seals during Breeding and Molt in the {{UK}}},
  author = {Pomeroy, P. and O'Connor, L. and Davies, P.},
  year = {2015},
  month = sep,
  journal = {Journal of Unmanned Vehicle Systems},
  publisher = {{NRC Research Press  http://www.nrcresearchpress.com}},
  doi = {10.1139/juvs-2015-0013},
  urldate = {2021-01-05},
  abstract = {Wildlife biology applications of unmanned aerial systems (UAS) are extensive. Survey, identification, and measurement using UAS equipped with appropriate sensors can now be added to the suite of te...},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/DWJ8RHP4/Pomeroy et al. - 2015 - Assessing use of and reaction to unmanned aerial s.pdf;/Users/b3020111/Zotero/storage/JQDX97F3/juvs-2015-0013.html;/Users/b3020111/Zotero/storage/RUVYTIRS/juvs-2015-0013.html}
}

@inproceedings{prangemeier_attention-based_2020,
  title = {Attention-{{Based Transformers}} for {{Instance Segmentation}} of {{Cells}} in {{Microstructures}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Prangemeier, Tim and Reich, Christoph and Koeppl, Heinz},
  year = {2020},
  month = dec,
  pages = {700--707},
  doi = {10.1109/BIBM49941.2020.9313305},
  abstract = {Detecting and segmenting object instances is a common task in biomedical applications. Examples range from detecting lesions on functional magnetic resonance images, to the detection of tumours in histopathological images and extracting quantitative single-cell information from microscopy imagery, where cell segmentation is a major bottleneck. Attention-based transformers are state-of-the-art in a range of deep learning fields. They have recently been proposed for segmentation tasks where they are beginning to outperform other methods. We present a novel attention-based cell detection transformer (CellDETR) for direct end-to-end instance segmentation. While the segmentation performance is on par with a state-of-the-art instance segmentation method, Cell-DETR is simpler and faster. We showcase the method's contribution in a the typical use case of segmenting yeast in microstructured environments, commonly employed in systems or synthetic biology. For the specific use case, the proposed method surpasses the state-of-the-art tools for semantic segmentation and additionally predicts the individual object instances. The fast and accurate instance segmentation performance increases the experimental information yield for a posteriori data processing and makes online monitoring of experiments and closed-loop optimal experimental design feasible. Code and data sample is available at https://git.rwth-aachen.de/ bcs/projects/cell-detr.git.},
  keywords = {attention,Cells (biology),Computer architecture,Decoding,deep learning,Head,Image segmentation,instance segmentation,microfluidics,Microscopy,Microstructure,single-cell analysis,synthetic biology,transformers},
  file = {/Users/b3020111/Zotero/storage/JSMKIHW4/Prangemeier et al. - 2020 - Attention-Based Transformers for Instance Segmenta.pdf;/Users/b3020111/Zotero/storage/S33DZEY3/9313305.html}
}

@article{qian_momentum_1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  year = {1999},
  month = jan,
  journal = {Neural Networks},
  volume = {12},
  number = {1},
  pages = {145--151},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00116-6},
  urldate = {2019-08-08},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence},
  file = {/Users/b3020111/Zotero/storage/C5KJ3QL6/S0893608098001166.html}
}

@article{qiao_cattle_2019,
  title = {Cattle Segmentation and Contour Extraction Based on {{Mask R-CNN}} for Precision Livestock Farming},
  author = {Qiao, Yongliang and Truman, Matthew and Sukkarieh, Salah},
  year = {2019},
  month = oct,
  journal = {Computers and Electronics in Agriculture},
  volume = {165},
  pages = {104958},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2019.104958},
  urldate = {2020-12-03},
  abstract = {In precision livestock farming, computer vision based approaches have been widely used to obtain individual cattle health and welfare information such as body condition score, live weight, activity behaviours. For this, precisely segmenting each cattle image from its background is a prerequisite, which is an important step towards obtaining real-time individual cattle information. In this paper, an instance segmentation approach based on a Mask R-CNN deep learning framework is proposed to solve cattle instance segmentation and contour extraction problems in a real feedlot environment. The proposed approach consists of the following steps: key frame extraction (detect the huge cattle motion frames), image enhancement (reduce the illumination and shadow influence), cattle segmentation and body contour extraction. We trained and tested the proposed approach on a challenging cattle image dataset. According to the experimental results, the proposed approach can render fairly desirable cattle segmentation performance with 0.92 Mean Pixel Accuracy (MPA) and achieve contour extraction with an Average Distance Error (ADE) of 33.56 pixel, which is better than that of the state-of-the-art SharpMask and DeepMask instance segmentation methods.},
  langid = {english},
  keywords = {Cattle contour,Deep learning,Instance segmentation,Mask R-CNN,Precision livestock farming},
  file = {/Users/b3020111/Zotero/storage/PH5XZFR2/Qiao et al. - 2019 - Cattle segmentation and contour extraction based o.pdf;/Users/b3020111/Zotero/storage/ZPUZEVSF/S0168169919304077.html}
}

@inproceedings{quinonez_using_2019,
  title = {Using {{Convolutional Neural Networks}} to {{Recognition}} of {{Dolphin Images}}},
  booktitle = {Trends and {{Applications}} in {{Software Engineering}}},
  author = {Qui{\~n}onez, Yadira and Zatarain, Oscar and Lizarraga, Carmen and Peraza, Juan},
  editor = {Mejia, Jezreel and Mu{\~n}oz, Mirna and Rocha, Alvaro and Pe{\~n}a, Adriana and {P{\'e}rez-Cisneros}, Marco},
  year = {2019},
  pages = {236--245},
  publisher = {{Springer International Publishing}},
  abstract = {Classification of specific objects through Convolutional Neural Networks (CNN) has become an interesting research line in the area from information processing and machine learning, main idea is training a image dataset to perform the classifying a given pattern. In this work, a new dataset with 2504 images was introduced, the method used to train the networks was transfer learning to recognition of dolphin images. For this purpose, two models were used: Inception V3 and Inception ResNet V2 to train on TensorFlow platform with different images, corresponding to the four main classes: dolphin, dolphin\_pod, open\_sea, and seabirds. The paper ends with a critical discussion of the experimental results.},
  isbn = {978-3-030-01171-0},
  file = {/Users/b3020111/Zotero/storage/R54YVDT3/QuiÃ±onez et al. - 2019 - Using Convolutional Neural Networks to Recognition.pdf}
}

@article{ramos_bottlenose_2018,
  title = {Bottlenose {{Dolphins}} and {{Antillean Manatees Respond}} to {{Small Multi-Rotor Unmanned Aerial Systems}}},
  author = {Ramos, Eric A. and Maloney, Brigid and Magnasco, Marcelo O. and Reiss, Diana},
  year = {2018},
  journal = {Frontiers in Marine Science},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {2296-7745},
  doi = {10.3389/fmars.2018.00316},
  urldate = {2021-01-05},
  abstract = {Unmanned aerial systems (UAS) are powerful tools for research and monitoring of wildlife. However, the effects of these systems on most marine mammals are largely unknown, preventing the establishment of guidelines that will minimize animal disturbance. In this study, we evaluated the behavioral responses of coastal bottlenose dolphins (Tursiops truncatus) and Antillean manatees (Trichechus manatus manatus) to small multi-rotor UAS flight. From 2015 to 2017, we piloted 211 flights using DJI quadcopters (Phantom II Vision +, 3 Professional and 4) to approach and follow animals over shallow-water habitats in Belize. The quadcopters were equipped with high-resolution cameras to observe dolphins during 138 of these flights, and manatees during 73 flights. Aerial video observations of animal behavior were coded and paired with flight data to determine whether animal activity and/or the UAS's flight patterns caused behavioral changes in exposed animals. Dolphins responded to UAS flight at altitudes of 11\textendash 30 m, and responded primarily when they were alone or in small groups. Single dolphins and one pair responded to the UAS by orienting upward and turning towards the aircraft to observe it, before quickly returning to their pre-response activity. A higher number of manatees responded to the UAS, exhibiting strong disturbance in response to the aircraft from 6\textendash 104 m. Manatees changed their behavior by fleeing the area and sometimes this elicited the same response in nearby animals. If pursued post-response, manatees repeatedly responded to overhead flight by evading the aircraft's path. These findings suggest that the invasiveness of UAS varies across individuals, species, and taxa. We conclude that careful exploratory research is needed to determine the impact of multi-rotor UAS flight on diverse species, and to develop best practices aimed at reducing the disturbance to wildlife that may result from their use.},
  langid = {english},
  keywords = {Behavior,Dolphins,Drone,Manatees,marine mammals,UAS,UAV,Unmanned aerial systems,Unmanned Aerial Vehicle,wildlife monitoring},
  file = {/Users/b3020111/Zotero/storage/GWSACJL4/Ramos et al. - 2018 - Bottlenose Dolphins and Antillean Manatees Respond.pdf}
}

@article{ravoor_deep_2020,
  title = {Deep {{Learning Methods}} for {{Multi-Species Animal Re-identification}} and {{Tracking}} \textendash{} a {{Survey}}},
  author = {Ravoor, Prashanth C},
  year = {2020},
  journal = {Computer Science Review},
  pages = {10},
  abstract = {Technology has an important part to play in wildlife and ecosystem conservation, and can vastly reduce time and effort spent in the associated tasks. Deep learning methods for computer vision in particular show good performance on a variety of tasks; animal detection and classification using deep learning networks are widely used to assist ecological studies. A related challenge is tracking animal movement over multiple cameras. For effective animal movement tracking, it is necessary to distinguish between individuals of the same species to correctly identify an individual moving between two cameras. Such problems could potentially be solved through animal re-identification methods. In this paper, the applicability of existing animal re-identification techniques for fully automated individual animal tracking in a cross-camera setup is explored. Recent developments in animal re-identification in the context of open-set recognition of individuals, and the extension of these systems to multiple species is examined. Some of the best performing human re-identification and object tracking systems are also reviewed in view of extending ideas within them to individual animal tracking. The survey concludes by presenting common trends in re-identification methods, lists a few challenges in the domain and recommends possible solutions.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/46HM5H3A/Ravoor - 2020 - Deep Learning Methods for Multi-Species Animal Re-.pdf}
}

@article{reddi_convergence_2019,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.09237 [cs, math, stat]},
  eprint = {1904.09237},
  primaryclass = {cs, math, stat},
  urldate = {2019-08-08},
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/AMQ8GBCR/Reddi et al. - 2019 - On the Convergence of Adam and Beyond.pdf;/Users/b3020111/Zotero/storage/U9SZ6EY7/1904.html}
}

@article{redmon_yolov3_2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  shorttitle = {{{YOLOv3}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.02767 [cs]},
  eprint = {1804.02767},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/DA3GYMQL/Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf;/Users/b3020111/Zotero/storage/KL6PM7WN/1804.html}
}

@inproceedings{redmon_you_2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  pages = {779--788},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.91},
  urldate = {2019-06-05},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/UBCXIYVS/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@article{reisser_photographic_2008,
  title = {Photographic Identification of Sea Turtles: Method Description and Validation, with an Estimation of Tag Loss},
  shorttitle = {Photographic Identification of Sea Turtles},
  author = {Reisser, J{\'u}lia and Proietti, Ma{\'i}ra and Kinas, Paul and Sazima, Ivan},
  year = {2008},
  month = sep,
  journal = {Endangered Species Research},
  volume = {5},
  number = {1},
  pages = {73--82},
  issn = {1863-5407, 1613-4796},
  doi = {10.3354/esr00113},
  urldate = {2021-01-07},
  abstract = {Recognition of individual sea turtles is mostly achieved by checking artificial tags previously attached to them, a method which is made difficult by the considerable tag loss rate and which requires repeated manipulation of the marked individuals. We describe an individual recognition method for sea turtles of the family Cheloniidae based on a mark-recapture study that relied on both artificial tagging (Inconel tags, style 681) and natural marks (facial profile photographs). Juvenile green Chelonia mydas and hawksbill Eretmochelys imbricata turtles were manually caught at Arvoredo Island, southern Brazil, and through visual comparison of facial profile photographs we were able to identify recaptured individuals with 2, 1, or no artificial tags. Additionally, Bayesian inference based on tag loss information indicated that the way a tag is attached (position and distance from the flipper edge) affects significantly the probability of its loss. We encourage the use of photographic identification (facial profile) as a reliable method for individual recognition in studies of cheloniid turtles.},
  langid = {english},
  keywords = {Bayesian inference,Cheloniidae,Photo-identification,Sea turtles,Tag loss},
  file = {/Users/b3020111/Zotero/storage/NNI36ANW/Reisser et al. - 2008 - Photographic identification of sea turtles method.pdf;/Users/b3020111/Zotero/storage/4323J7WD/p73-82.html}
}

@article{ren_faster_2015,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.01497 [cs]},
  eprint = {1506.01497},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/8UESWG2A/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf;/Users/b3020111/Zotero/storage/MXAH4FRB/1506.html}
}

@article{reno_sift-based_2019,
  title = {A {{SIFT-based}} Software System for the Photo-Identification of the {{Risso}}'s Dolphin},
  author = {Ren{\`o}, V. and Dimauro, G. and Labate, G. and Stella, E. and Fanizza, C. and Cipriano, G. and Carlucci, R. and Maglietta, R.},
  year = {2019},
  month = mar,
  journal = {Ecological Informatics},
  volume = {50},
  pages = {95--101},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2019.01.006},
  urldate = {2021-01-07},
  abstract = {Photo-identification is a commonly used non-invasive technique that has been profitably employed in biological studies throughout the years. It starts from the assumption that a single individual can be recognized in multiple photos captured at different times by exploiting its unique representative and visible physical qualities such as marks, notches or any other definite feature. Hence, photo-identification is performed to infer knowledge about wild species' spatial and temporal distributions as well as population dynamics, thus providing valuable information especially when the species being investigated is ranked as data deficient. Furthermore, the technological improvements of the last decades and the large availability of devices with powerful computing capabilities are driving the research towards a common goal of enriching bio-ecological studies with innovative computer science approaches. In this scenario, computer vision plays a fundamental role, as it can successfully assist researchers in the analysis of large amounts of data. The aim of this paper is, in fact, to effectively provide a computer vision approach for the photo-identification of the Risso's dolphin, exploiting specific visual cues with a feature-based approach relying on SIFT and SURF feature detectors. The experiments have been conducted on image data acquired in the Gulf of Taranto from 2013 to 2017, conducting a comparative analysis of the performance of both SIFT and SURF, as well as a comparison with the state-of-the-art software DARWIN, and they proved the effectiveness of the proposed approach and suggested its application would be suitable to large scale studies. In conclusion, this paper shows an innovative computer vision application for the identification of unknown Risso's dolphin individuals that relies on a feature-based automated approach. The results suggest that the proposed approach can efficiently assist researchers during the photo-identification task of large amounts of data collected in such a challenging domain.},
  langid = {english},
  keywords = {Cetaceans,Computer vision,Photo-identification,Risso,SIFT features},
  file = {/Users/b3020111/Zotero/storage/FMKYWWQL/RenÃ² et al. - 2019 - A SIFT-based software system for the photo-identif.pdf;/Users/b3020111/Zotero/storage/UZ44KDHR/S1574954118301377.html}
}

@article{riaz_fouriernet_2020,
  title = {{{FourierNet}}: {{Compact}} Mask Representation for Instance Segmentation Using Differentiable Shape Decoders},
  shorttitle = {{{FourierNet}}},
  author = {Riaz, Hamd ul Moqeet and Benbarka, Nuri and Zell, Andreas},
  year = {2020},
  month = oct,
  journal = {arXiv:2002.02709 [cs, eess]},
  eprint = {2002.02709},
  primaryclass = {cs, eess},
  urldate = {2020-12-03},
  abstract = {We present FourierNet, a single shot, anchor-free, fully convolutional instance segmentation method that predicts a shape vector. Consequently, this shape vector is converted into the masks' contour points using a fast numerical transform. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which manages the automatic weight balancing of the shape vector's coefficients. We used the Fourier series as a shape encoder because of its coefficient interpretability and fast implementation. FourierNet shows promising results compared to polygon representation methods, achieving 30.6 mAP on the MS COCO 2017 benchmark. At lower image resolutions, it runs at 26.6 FPS with 24.3 mAP. It reaches 23.3 mAP using just eight parameters to represent the mask (note that at least four parameters are needed for bounding box prediction only). Qualitative analysis shows that suppressing a reasonable proportion of higher frequencies of Fourier series, still generates meaningful masks. These results validate our understanding that lower frequency components hold higher information for the segmentation task, and therefore, we can achieve a compressed representation. Code is available at: github.com/cogsys-tuebingen/FourierNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/b3020111/Zotero/storage/MYWEG2S8/Riaz et al. - 2020 - FourierNet Compact mask representation for instan.pdf;/Users/b3020111/Zotero/storage/DDBVEJ76/2002.html}
}

@article{robin_learning_2022,
  title = {Learning to Forecast Vegetation Greenness at Fine Resolution over {{Africa}} with {{ConvLSTMs}}},
  author = {Robin, Claire and {Requena-Mesa}, Christian and Benson, Vitus and Alonso, Lazaro and Poehls, Jeran and Carvalhais, Nuno and Reichstein, Markus},
  year = {2022},
  month = nov,
  journal = {arXiv:2210.13648 [cs.LG]},
  eprint = {2210.13648},
  primaryclass = {cs.LG},
  urldate = {2022-12-09},
  abstract = {Forecasting the state of vegetation in response to climate and weather events is a major challenge. Its implementation will prove crucial in predicting crop yield, forest damage, or more generally the impact on ecosystems services relevant for socio-economic functioning, which if absent can lead to humanitarian disasters. Vegetation status depends on weather and environmental conditions that modulate complex ecological processes taking place at several timescales. Interactions between vegetation and different environmental drivers express responses at instantaneous but also time-lagged effects, often showing an emerging spatial context at landscape and regional scales. We formulate the land surface forecasting task as a strongly guided video prediction task where the objective is to forecast the vegetation developing at very fine resolution using topography and weather variables to guide the prediction. We use a Convolutional LSTM (ConvLSTM) architecture to address this task and predict changes in the vegetation state in Africa using Sentinel-2 satellite NDVI, having ERA5 weather reanalysis, SMAP satellite measurements, and topography (DEM of SRTMv4.1) as variables to guide the prediction. Ours results highlight how ConvLSTM models can not only forecast the seasonal evolution of NDVI at high resolution, but also the differential impacts of weather anomalies over the baselines. The model is able to predict different vegetation types, even those with very high NDVI variability during target length, which is promising to support anticipatory actions in the context of drought-related disasters 1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/TTKXTX2G/Robin et al. - 2022 - Learning to forecast vegetation greenness at fine .pdf}
}

@inproceedings{roe_catching_2018,
  title = {Catching {{Toad Calls}} in the {{Cloud}}: {{Commodity Edge Computing}} for {{Flexible Analysis}} of {{Big Sound Data}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Roe, Paul and Ferroudj, Meriem and Towsey, Michael and Schwarzkopf, Lin},
  year = {2018},
  pages = {67--74},
  publisher = {{IEEE}},
  isbn = {1-5386-9156-6}
}

@article{rogan_bottlenose_nodate,
  title = {Bottlenose Dolphin Survey in the {{Lower River Shannon SAC}}, 2018},
  author = {Rogan, E and Garagouni, M and Nyk{\"a}nen, M and Whitaker, A and Ingram, S N},
  pages = {20},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/VP3YGVL4/Rogan et al. - Bottlenose dolphin survey in the Lower River Shann.pdf}
}

@inproceedings{rohit_malhotra_autonomous_2018,
  title = {Autonomous {{Detection}} of {{Disruptions}} in the {{Intensive Care Unit Using Deep Mask R-CNN}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Rohit Malhotra, Kumar and Davoudi, Anis and Siegel, Scott and Bihorac, Azra and Rashidi, Parisa},
  year = {2018},
  pages = {1863--1865},
  urldate = {2020-12-03},
  file = {/Users/b3020111/Zotero/storage/ZWQQ3V3W/Rohit Malhotra et al. - 2018 - Autonomous Detection of Disruptions in the Intensi.pdf;/Users/b3020111/Zotero/storage/G2IS7HTR/Malhotra_Autonomous_Detection_of_CVPR_2018_paper.html}
}

@article{ronneberger_u-net_2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  journal = {arXiv:1505.04597 [cs]},
  eprint = {1505.04597},
  primaryclass = {cs},
  urldate = {2021-01-05},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/Z7H8LXTL/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;/Users/b3020111/Zotero/storage/KVUC9DJ3/1505.html}
}

@article{rosso_colour_2008,
  title = {Colour Patterns and Pigmentation Variability on Striped Dolphin {{Stenella}} Coeruleoalba in North-Western {{Mediterranean Sea}}},
  author = {Rosso, Massimiliano and Moulins, Aur{\'e}lie and W{\"u}rtz, Maurizio},
  year = {2008},
  month = sep,
  journal = {Marine Biological Association of the United Kingdom. Journal of the Marine Biological Association of the United Kingdom},
  volume = {88},
  number = {6},
  pages = {1211--1219},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, United Kingdom}},
  issn = {00253154},
  doi = {http://dx.doi.org/10.1017/S0025315408001641},
  urldate = {2021-01-08},
  abstract = {Studies on differences in external morphology and pigmentation patterns were historically carried out using stranded individuals or opportunistic sightings; few studies have involved sampling systematically free-ranging individuals. In order to investigate and describe main pigmentation characteristics, outlining 'typical' regional pigmentations, this work analysed systematic photographic information taken on free-ranging striped dolphins, Stenella coeruleoalba. Photographs of dolphins in the Ligurian Sea were collected between May 2004 and December 2006. All individuals were described by the presence/absence of pigmentation variables and by differences in colour shades. The frequency of all the pigmentation variables analysed is stable in the population (10 'gene' variables, 19 'allele' variables), and remains similar between each different group of dolphins. But population presents widespread pigmentation variability between specimens, allowing identification even at single individual level. Cluster analysis also found that the majority of the pigmentations derive from two main colour patterns, called 'mat' and 'pale' patterns (fmat = 0.68; fpale = 0.12). The Bray-Curtis index showed a high variability of the intra-group pigmentation distance between groups. This resulted in a positive correlation between group size and 'intra-group' pigmentation distance: the distance increases rapidly up to a group size of 40 individuals. According to the results obtained, the striped dolphins seem to be concentrated in small groups in which there is a large phenotypic similarity among individuals. These small units could be associated between them to form temporary large groups observed only in pelagic waters. [PUBLICATION ABSTRACT]},
  copyright = {Copyright \textcopyright{} Marine Biological Association of the United Kingdom 2008},
  langid = {english},
  keywords = {Biology},
  file = {/Users/b3020111/Zotero/storage/8WUYPLVZ/Rosso et al. - 2008 - Colour patterns and pigmentation variability on st.pdf}
}

@article{rother_grabcut_2004,
  title = {``{{GrabCut}}'' \textemdash{} {{Interactive Foreground Extraction}} Using {{Iterated Graph Cuts}}},
  author = {Rother, Carsten and Kolmogorov, Vladimir and Blake, Andrew},
  year = {2004},
  journal = {ACM transactions on graphics},
  volume = {23},
  number = {3},
  pages = {309--314},
  abstract = {The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for ``border matting'' has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/FGEPHD3F/Rother et al. - âGrabCutâ â Interactive Foreground Extraction usin.pdf}
}

@article{rowcliffe_estimating_2008,
  title = {Estimating {{Animal Density Using Camera Traps}} without the {{Need}} for {{Individual Recognition}}},
  author = {Rowcliffe, J. Marcus and Field, Juliet and Turvey, Samuel T. and Carbone, Chris},
  year = {2008},
  journal = {Journal of Applied Ecology},
  volume = {45},
  number = {4},
  eprint = {20144086},
  eprinttype = {jstor},
  pages = {1228--1236},
  publisher = {{[British Ecological Society, Wiley]}},
  issn = {0021-8901},
  urldate = {2021-01-05},
  abstract = {1. Density estimation is of fundamental importance in wildlife management. The use of camera traps to estimate animal density has so far been restricted to capture-recapture analysis of species with individually identifiable markings. This study developed a method that eliminates the requirement for individual recognition of animals by modelling the underlying process of contact between animals and cameras. 2. The model provides a factor that linearly scales trapping rate with density, depending on two key biological variables (average animal group size and day range) and two characteristics of the camera sensor (distance and angle within which it detects animals). 3. We tested the approach in an enclosed animal park with known abundances of four species, obtaining accurate estimates in three out of four cases. Inaccuracy in the fourth species was because of biased placement of cameras with respect to the distribution of this species. 4. Synthesis and applications. Subject to unbiased camera placement and accurate measurement of model parameters, this method opens the possibility of reduced labour costs for estimating wildlife density and may make estimation possible where it has not been previously. We provide guidelines on the trapping effort required to obtain reasonably precise estimates.}
}

@article{royo-miquel_retinanet_2021,
  title = {{{RetinaNet Object Detector}} Based on {{Analog-to-Spiking Neural Network Conversion}}},
  author = {{Royo-Miquel}, Joaquin and Tolu, Silvia and Sch{\"o}ller, Frederik E. T. and Galeazzi, Roberto},
  year = {2021},
  month = sep,
  journal = {arXiv:2106.05624 [eess.IV]},
  eprint = {2106.05624},
  primaryclass = {eess.IV},
  urldate = {2023-04-27},
  abstract = {The paper proposes a method to convert a deep learning object detector into an equivalent spiking neural network. The aim is to provide a conversion framework that is not constrained to shallow network structures and classification problems as in state-of-the-art conversion libraries. The results show that models of higher complexity, such as the RetinaNet object detector, can be converted with limited loss in performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/b3020111/Zotero/storage/VLJSYU78/Royo-Miquel et al. - 2021 - RetinaNet Object Detector based on Analog-to-Spiki.pdf}
}

@article{ruder_overview_2016,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.04747 [cs]},
  eprint = {1609.04747},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/9MQR2SCP/Ruder - 2016 - An overview of gradient descent optimization algor.pdf;/Users/b3020111/Zotero/storage/62QFN8PH/1609.html}
}

@techreport{rumelhart_learning_1985,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year = {1985},
  pages = {49},
  institution = {{California Univ San Diego La Jolla Inst for Cognitive Science}},
  urldate = {2022-09-27},
  file = {/Users/b3020111/Zotero/storage/MX5CG4JZ/a164453.pdf}
}

@article{schevill_daily_1960,
  title = {Daily {{Patrol}} of a {{Megaptera}}},
  author = {Schevill, William E. and Backus, Richard H.},
  year = {1960},
  month = may,
  journal = {Journal of Mammalogy},
  volume = {41},
  number = {2},
  pages = {279},
  issn = {00222372},
  doi = {10.2307/1376380},
  urldate = {2021-01-07},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/79AGDK56/Schevill and Backus - 1960 - Daily Patrol of a Megaptera.pdf}
}

@article{schneider_past_2019,
  title = {Past, Present and Future Approaches Using Computer Vision for Animal Re-Identification from Camera Trap Data},
  author = {Schneider, Stefan and Taylor, Graham W. and Linquist, Stefan and Kremer, Stefan C.},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {461--470},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13133},
  urldate = {2020-10-01},
  abstract = {The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.},
  langid = {english},
  keywords = {animal reidentification,camera traps,computer vision,convolutional networks,deep learning,density estimation,monitoring,object detection},
  file = {/Users/b3020111/Zotero/storage/H2D82NL5/Schneider et al. - 2019 - Past, present and future approaches using computer.pdf;/Users/b3020111/Zotero/storage/QQJH5XV8/2041-210X.html}
}

@inproceedings{schroff_facenet_2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  pages = {815--823},
  urldate = {2020-02-12},
  file = {/Users/b3020111/Zotero/storage/RWHINAPE/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf;/Users/b3020111/Zotero/storage/X3ESPCJN/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html}
}

@article{selvaraju_grad-cam_2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2020-04-09},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/GMTDMNX8/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf}
}

@article{shabanov_stir_2023,
  title = {{{STIR}}: {{Siamese Transformer}} for {{Image Retrieval Postprocessing}}},
  shorttitle = {{{STIR}}},
  author = {Shabanov, Aleksei and Tarasov, Aleksei and Nikolenko, Sergey},
  year = {2023},
  month = apr,
  journal = {arXiv:2304.13393 [cs.IR]},
  eprint = {2304.13393},
  primaryclass = {cs.IR},
  urldate = {2023-05-09},
  abstract = {Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the art on standard image retrieval datasets: Stanford Online Products and DeepFashion In-shop. We also release the source code1 and an interactive demo2 of our approach.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,H.3.3},
  file = {/Users/b3020111/Zotero/storage/5RSN6MTF/Shabanov et al. - 2023 - STIR Siamese Transformer for Image Retrieval Post.pdf}
}

@article{shane_ecology_1986,
  title = {Ecology, {{Behavior}} and {{Social Organization}} of the {{Bottlenose Dolphin}}: {{A Review}}},
  author = {Shane, Susan H and Wells, Randall S and Wursig, Bernd},
  year = {1986},
  journal = {Marine Mammal Science},
  volume = {2},
  number = {1},
  pages = {30},
  abstract = {The authors review the literature on bottlenose dolphin ecology, behavior and social organization, focusing on data collected on free-ranging animals. Most bottlenose dolphins studied to date have had definable home ranges, and behavioral, morphological and biochemical information indicates discrete stocks in some areas. Bottlenose dolphins appear to form relatively permanent social groups based on sex and age. Mother-calf bonds are long-lasting. Movement patterns are extremely variable from location to location but are relatively predictable at any given site. Food resources are one of the most important factors affecting movements. Bottlenose dolphin behavior is very flexible, and these dolphins are generally active day and night. Feeding peaks in the morning and afternoon have been observed at several sites. Social behavior is an important component of daily activities. Sharks are the most significant predator on bottlenose dolphins in most areas, but captive and wild studies show that dolphins and sharks frequently live in harmony as well. Human activities may be helpful, harmful or neutral to bottlenose dolphins, but interactions with humans are frequent for these coastal cetaceans.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/8Q57GCN8/Shane et al. - 1986 - ECOLOGY, BEHAVIOR AND SOCIAL ORGANIZATION OF THE B.pdf}
}

@misc{sharma_image_2019,
  title = {Image {{Segmentation Python}} | {{Implementation}} of {{Mask R-CNN}} | {{https://analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/}}},
  author = {Sharma, Pulkit},
  year = {2019},
  month = jul,
  journal = {Analytics Vidhya},
  urldate = {2020-12-03},
  abstract = {An introduction to image segmentation. In this article learn about Mask R-CNN framework for image segmentation and implementation of mask r-cnn in python.},
  keywords = {instance segmentation,semantic segmentation},
  file = {/Users/b3020111/Zotero/storage/TAILWK7V/computer-vision-implementing-mask-r-cnn-image-segmentation.html}
}

@article{sharpe_indian_2019,
  title = {Indian {{Ocean}} Humpback Dolphin in the {{Menai Bay}} off the South Coast of {{Zanzibar}}, {{East Africa}} Is {{Critically Endangered}}},
  author = {Sharpe, Matt and Berggren, Per},
  year = {2019},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {29},
  number = {12},
  pages = {2133--2146},
  issn = {1099-0755},
  doi = {10.1002/aqc.3221},
  urldate = {2021-01-07},
  abstract = {Cetaceans occupying coastal habitats are at high risk of impact from anthropogenic sources which can cause direct mortality or affect long-term health. Monitoring and detecting change require long-term studies and reliable funding, not always available especially in developing countries. Management and conservation of cetaceans must therefore use precautionary methods that allow assessment from limited data sources to identify risk of, and prevent, species extirpation or extinction. IUCN Red List criteria for regional populations was applied to the population of Indian Ocean humpback dolphins (Sousa plumbea) resident in Menai Bay Conservation Area off the south coast of Zanzibar, East Africa which is subjected to unsustainable entanglement rates in gillnet fisheries and unregulated tourism activities. Photographic identification surveys were conducted in 2015 to generate a new abundance estimate from capture\textendash recapture analysis. Mortality estimates were calculated using available data from 1999 to 2002 and a population viability analysis was conducted based on population, species and genus specific parameters. The 2015 abundance estimate for humpback dolphins was 19 (95\% CI 14\textendash 25) non-calf individuals, representing a 63\% reduction in abundance since 2002. The population viability analysis baseline scenario predicted chance of extinction at 0.996 (SE 0.002) with the median time to extinction at 36 years. Sensitivity analysis suggested that population recovery would only be possible with a complete prevention of bycatch mortality. The population met the threshold for Critically Endangered for all criteria which could be directly assessed. This conservation assessment highlights the requirement for immediate management action to eliminate bycatch of humpback dolphins to prevent the local extinction of the species.},
  copyright = {\textcopyright{} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {coastal,endangered species,island,mammals,marine protected area},
  file = {/Users/b3020111/Zotero/storage/SI32ZTES/Sharpe and Berggren - 2019 - Indian Ocean humpback dolphin in the Menai Bay off.pdf;/Users/b3020111/Zotero/storage/WA9DN78K/aqc.html}
}

@article{sharpe_indian_2019-1,
  title = {Indian {{Ocean}} Humpback Dolphin in the {{Menai Bay}} off the South Coast of {{Zanzibar}}, {{East Africa}} Is {{Critically Endangered}}},
  author = {Sharpe, Matt and Berggren, Per},
  year = {2019},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {29},
  number = {12},
  pages = {2133--2146},
  issn = {1099-0755},
  doi = {10.1002/aqc.3221},
  urldate = {2021-10-10},
  abstract = {Cetaceans occupying coastal habitats are at high risk of impact from anthropogenic sources which can cause direct mortality or affect long-term health. Monitoring and detecting change require long-term studies and reliable funding, not always available especially in developing countries. Management and conservation of cetaceans must therefore use precautionary methods that allow assessment from limited data sources to identify risk of, and prevent, species extirpation or extinction. IUCN Red List criteria for regional populations was applied to the population of Indian Ocean humpback dolphins (Sousa plumbea) resident in Menai Bay Conservation Area off the south coast of Zanzibar, East Africa which is subjected to unsustainable entanglement rates in gillnet fisheries and unregulated tourism activities. Photographic identification surveys were conducted in 2015 to generate a new abundance estimate from capture\textendash recapture analysis. Mortality estimates were calculated using available data from 1999 to 2002 and a population viability analysis was conducted based on population, species and genus specific parameters. The 2015 abundance estimate for humpback dolphins was 19 (95\% CI 14\textendash 25) non-calf individuals, representing a 63\% reduction in abundance since 2002. The population viability analysis baseline scenario predicted chance of extinction at 0.996 (SE 0.002) with the median time to extinction at 36 years. Sensitivity analysis suggested that population recovery would only be possible with a complete prevention of bycatch mortality. The population met the threshold for Critically Endangered for all criteria which could be directly assessed. This conservation assessment highlights the requirement for immediate management action to eliminate bycatch of humpback dolphins to prevent the local extinction of the species.},
  langid = {english},
  keywords = {coastal,endangered species,island,mammals,marine protected area},
  file = {/Users/b3020111/Zotero/storage/UV2T96UC/Sharpe and Berggren - 2019 - Indian Ocean humpback dolphin in the Menai Bay off.pdf;/Users/b3020111/Zotero/storage/FH26PS7M/aqc.html}
}

@article{sharpe_indian_2019-2,
  title = {Indian {{Ocean}} Humpback Dolphin in the {{Menai Bay}} off the South Coast of {{Zanzibar}}, {{East Africa}} Is {{Critically Endangered}}},
  author = {Sharpe, Matt and Berggren, Per},
  year = {2019},
  journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
  volume = {29},
  number = {12},
  pages = {2133--2146},
  issn = {1099-0755},
  doi = {10.1002/aqc.3221},
  urldate = {2022-04-12},
  abstract = {Cetaceans occupying coastal habitats are at high risk of impact from anthropogenic sources which can cause direct mortality or affect long-term health. Monitoring and detecting change require long-term studies and reliable funding, not always available especially in developing countries. Management and conservation of cetaceans must therefore use precautionary methods that allow assessment from limited data sources to identify risk of, and prevent, species extirpation or extinction. IUCN Red List criteria for regional populations was applied to the population of Indian Ocean humpback dolphins (Sousa plumbea) resident in Menai Bay Conservation Area off the south coast of Zanzibar, East Africa which is subjected to unsustainable entanglement rates in gillnet fisheries and unregulated tourism activities. Photographic identification surveys were conducted in 2015 to generate a new abundance estimate from capture\textendash recapture analysis. Mortality estimates were calculated using available data from 1999 to 2002 and a population viability analysis was conducted based on population, species and genus specific parameters. The 2015 abundance estimate for humpback dolphins was 19 (95\% CI 14\textendash 25) non-calf individuals, representing a 63\% reduction in abundance since 2002. The population viability analysis baseline scenario predicted chance of extinction at 0.996 (SE 0.002) with the median time to extinction at 36 years. Sensitivity analysis suggested that population recovery would only be possible with a complete prevention of bycatch mortality. The population met the threshold for Critically Endangered for all criteria which could be directly assessed. This conservation assessment highlights the requirement for immediate management action to eliminate bycatch of humpback dolphins to prevent the local extinction of the species.},
  langid = {english},
  keywords = {coastal,endangered species,island,mammals,marine protected area},
  file = {/Users/b3020111/Zotero/storage/VK8KSFNC/Sharpe and Berggren - 2019 - Indian Ocean humpback dolphin in the Menai Bay off.pdf;/Users/b3020111/Zotero/storage/C6W6XWNN/aqc.html}
}

@article{shirke_tracking_2021,
  title = {Tracking {{Grow-Finish Pigs Across Large Pens Using Multiple Cameras}}},
  author = {Shirke, Aniket and Saifuddin, Aziz and Luthra, Achleshwar and Li, Jiangong and Williams, Tawni and Hu, Xiaodan and Kotnana, Aneesh and Kocabalkanli, Okan and Ahuja, Narendra and {Green-Miller}, Angela and Condotta, Isabella and Dilger, Ryan N. and Caesar, Matthew},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.10971 [cs]},
  eprint = {2111.10971},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Increasing demand for meat products combined with farm labor shortages has resulted in a need to develop new real-time solutions to monitor animals effectively. Significant progress has been made in continuously locating individual pigs using tracking-by-detection methods. However, these methods fail for oblong pens because a single fixed camera does not cover the entire floor at adequate resolution. We address this problem by using multiple cameras, placed such that the visual fields of adjacent cameras overlap, and together they span the entire floor. Avoiding breaks in tracking requires inter-camera handover when a pig crosses from one camera's view into that of an adjacent camera. We identify the adjacent camera and the shared pig location on the floor at the handover time using inter-view homography. Our experiments involve two grow-finish pens, housing 16-17 pigs each, and three RGB cameras. Our algorithm first detects pigs using a deep learning-based object detection model (YOLO) and creates their local tracking IDs using a multi-object tracking algorithm (DeepSORT). We then use inter-camera shared locations to match multiple views and generate a global ID for each pig that holds throughout tracking. To evaluate our approach, we provide five two-minutes long video sequences with fully annotated global identities. We track pigs in a single camera view with a Multi-Object Tracking Accuracy and Precision of 65.0\% and 54.3\% respectively and achieve a Camera Handover Accuracy of 74.0\%. We open-source our code and annotated dataset at https://github.com/AIFARMS/multi-camera-pig-tracking},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/97IUQYJU/Shirke et al. - 2021 - Tracking Grow-Finish Pigs Across Large Pens Using .pdf}
}

@article{shirke_tracking_2021-1,
  title = {Tracking {{Grow-Finish Pigs Across Large Pens Using Multiple Cameras}}},
  author = {Shirke, Aniket and Saifuddin, Aziz and Luthra, Achleshwar and Li, Jiangong and Williams, Tawni and Hu, Xiaodan and Kotnana, Aneesh and Kocabalkanli, Okan and Ahuja, Narendra and {Green-Miller}, Angela and Condotta, Isabella and Dilger, Ryan N. and Caesar, Matthew},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.10971 [cs]},
  eprint = {2111.10971},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Increasing demand for meat products combined with farm labor shortages has resulted in a need to develop new real-time solutions to monitor animals effectively. Significant progress has been made in continuously locating individual pigs using tracking-by-detection methods. However, these methods fail for oblong pens because a single fixed camera does not cover the entire floor at adequate resolution. We address this problem by using multiple cameras, placed such that the visual fields of adjacent cameras overlap, and together they span the entire floor. Avoiding breaks in tracking requires inter-camera handover when a pig crosses from one camera's view into that of an adjacent camera. We identify the adjacent camera and the shared pig location on the floor at the handover time using inter-view homography. Our experiments involve two grow-finish pens, housing 16-17 pigs each, and three RGB cameras. Our algorithm first detects pigs using a deep learning-based object detection model (YOLO) and creates their local tracking IDs using a multi-object tracking algorithm (DeepSORT). We then use inter-camera shared locations to match multiple views and generate a global ID for each pig that holds throughout tracking. To evaluate our approach, we provide five two-minutes long video sequences with fully annotated global identities. We track pigs in a single camera view with a Multi-Object Tracking Accuracy and Precision of 65.0\% and 54.3\% respectively and achieve a Camera Handover Accuracy of 74.0\%. We open-source our code and annotated dataset at https://github.com/AIFARMS/multi-camera-pig-tracking},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/M9ZRJEI9/Shirke et al. - 2021 - Tracking Grow-Finish Pigs Across Large Pens Using .pdf}
}

@article{silva_winter_2012,
  title = {Winter Sighting of a Known Western {{North Atlantic}} Right Whale in the {{Azores}}},
  author = {Silva, M{\'o}nica A and Steiner, Lisa and Casc{\~a}o, Irma and Cruz, Maria Jo{\~a}o and Prieto, Rui and Cole, Tim and Hamilton, Philip K and Baumgartner, Mark},
  year = {2012},
  journal = {Journal of Cetacean Research and Management},
  volume = {12},
  number = {1},
  pages = {6},
  abstract = {A right whale (Eubalaena glacialis) from the western North Atlantic population, sighted in the Azores, was subsequently found to have moved back to the northwest Atlantic. The whale was sighted in the Azores on 5 January 2009 travelling in a west-south westerly direction at a constant speed. A photographic match was found to an adult female in the North Atlantic Right Whale Catalogue. The whale's previous last sighting, on 24 September 2008 in the Bay of Fundy, Canada, implies movement to the Azores of at least 3,320km in 101 days. It was subsequently resighted in the Bay of Fundy on 2 September 2009, 237 days after being seen in the Azores. This appears to be the only documented evidence of a western North Atlantic right whale outside its normal range in winter, and provides additional evidence of the potential for interbreeding between western North Atlantic right whales and the remnant eastern population.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/KE36XX4E/Silva et al. - 2012 - Winter sighting of a known western North Atlantic .pdf}
}

@article{simonyan_very_2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/AJ495QIF/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/Users/b3020111/Zotero/storage/5KI73VRB/1409.html}
}

@article{sinha_face_2006,
  title = {Face {{Recognition}} by {{Humans}}: {{Nineteen Results All Computer Vision Researchers Should Know About}}},
  shorttitle = {Face {{Recognition}} by {{Humans}}},
  author = {Sinha, P. and Balas, B. and Ostrovsky, Y. and Russell, R.},
  year = {2006},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {94},
  number = {11},
  pages = {1948--1962},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2006.884093},
  urldate = {2019-03-14},
  abstract = {A key goal of computer vision researchers is to create automated face recognition systems that can equal, and eventually surpass, human performance. To this end, it is imperative that computational researchers know of the key findings from experimental studies of face recognition by humans. These findings provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities. In this paper, we present what we believe are 19 basic results, with implications for the design of computational systems. Each result is described briefly and appropriate pointers are provided to permit an in-depth study of any particular result.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/L2NCZB59/Sinha et al. - 2006 - Face Recognition by Humans Nineteen Results All C.pdf}
}

@inproceedings{siva_weakly_2011,
  title = {Weakly Supervised Object Detector Learning with Model Drift Detection},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Siva, Parthipan and Xiang, Tao},
  year = {2011},
  month = nov,
  pages = {343--350},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2011.6126261},
  abstract = {A conventional approach to learning object detectors uses fully supervised learning techniques which assumes that a training image set with manual annotation of object bounding boxes are provided. The manual annotation of objects in large image sets is tedious and unreliable. Therefore, a weakly supervised learning approach is desirable, where the training set needs only binary labels regarding whether an image contains the target object class. In the weakly supervised approach a detector is used to iteratively annotate the training set and learn the object model. We present a novel weakly supervised learning framework for learning an object detector. Our framework incorporates a new initial annotation model to start the iterative learning of a detector and a model drift detection method that is able to detect and stop the iterative learning when the detector starts to drift away from the objects of interest. We demonstrate the effectiveness of our approach on the challenging PASCAL 2007 dataset.},
  keywords = {Adaptation models,Detectors,Histograms,Measurement,Object detection,Support vector machines,Training},
  file = {/Users/b3020111/Zotero/storage/VSHGGXQQ/Siva and Xiang - 2011 - Weakly supervised object detector learning with mo.pdf;/Users/b3020111/Zotero/storage/ERPP6XG8/6126261.html}
}

@article{smith_cyclical_2015,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.01186 [cs]},
  eprint = {1506.01186},
  primaryclass = {cs},
  urldate = {2019-01-08},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/b3020111/Zotero/storage/BM9FYJNA/Smith - 2015 - Cyclical Learning Rates for Training Neural Networ.pdf;/Users/b3020111/Zotero/storage/UGUJ42DH/1506.html}
}

@article{smolker_sex_1992,
  title = {Sex {{Differences}} in {{Patterns}} of {{Association Among Indian Ocean Bottlenose Dolphins}}},
  author = {Smolker, Rachel A. and Richards, Andrew F. and Connor, Richard C. and Pepper, John W.},
  year = {1992},
  month = jan,
  journal = {Behaviour},
  volume = {123},
  number = {1-2},
  pages = {38--69},
  publisher = {{Brill}},
  issn = {0005-7959, 1568-539X},
  doi = {10.1163/156853992X00101},
  urldate = {2022-04-19},
  abstract = {Abstract Patterns of association among bottlenose dolphins resident in Shark Bay, Western Australia were analyzed using party membership data. Parties contained an average of 4.8 individuals, but party size and composition were unstable. While these temporary parties often contained both males and females, long term consistent associations generally were between members of the same sex. The highest association coefficients, resulting from very frequent co-occurrence within parties were between males and between mothers and offspring. Males formed subgroups of two or three individuals who consistently associated with each other, and these were stable over periods of at least seven years in some cases. Male subgroups preferentially associated with particular other male subgroups. Females associated most consistently with other females, although not to the same extent as some males. Female associations were better described as a network rather than discrete subgroups. Male-female associations were generally inconsistent and depended in part on female reproductive state. Mothers and their offspring associated very consistently for at least 4 years.},
  langid = {english},
  keywords = {Biology,Biology \& Environmental Sciences,Journal},
  file = {/Users/b3020111/Zotero/storage/TYNLCCSL/Smolker et al. - 1992 - Sex Differences in Patterns of Association Among I.pdf}
}

@article{soviany_optimizing_2018,
  title = {Optimizing the {{Trade-off}} between {{Single-Stage}} and {{Two-Stage Object Detectors}} Using {{Image Difficulty Prediction}}},
  author = {Soviany, Petru and Ionescu, Radu Tudor},
  year = {2018},
  month = aug,
  journal = {arXiv:1803.08707 [cs]},
  eprint = {1803.08707},
  primaryclass = {cs},
  urldate = {2021-06-16},
  abstract = {There are mainly two types of state-of-the-art object detectors. On one hand, we have two-stage detectors, such as Faster R-CNN (Region-based Convolutional Neural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to generate regions of interests in the first stage and (ii) send the region proposals down the pipeline for object classification and bounding-box regression. Such models reach the highest accuracy rates, but are typically slower. On the other hand, we have single-stage detectors, such as YOLO (You Only Look Once) and SSD (Singe Shot MultiBox Detector), that treat object detection as a simple regression problem by taking an input image and learning the class probabilities and bounding box coordinates. Such models reach lower accuracy rates, but are much faster than two-stage object detectors. In this paper, we propose to use an image difficulty predictor to achieve an optimal tradeoff between accuracy and speed in object detection. The image difficulty predictor is applied on the test images to split them into easy versus hard images. Once separated, the easy images are sent to the faster single-stage detector, while the hard images are sent to the more accurate two-stage detector. Our experiments on PASCAL VOC 2007 show that using image difficulty compares favorably to a random split of the images. Our method is flexible, in that it allows to choose a desired threshold for splitting the images into easy versus hard.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/IRXPY7K8/Soviany and Ionescu - 2018 - Optimizing the Trade-off between Single-Stage and .pdf}
}

@article{srivastava_dropout:_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  pages = {30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ZX3TYXVX/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@article{stankiewicz_segmentation_2021,
  title = {Segmentation of {{Preretinal Space}} in {{Optical Coherence Tomography Images Using Deep Neural Networks}}},
  author = {Stankiewicz, Agnieszka and Marciniak, Tomasz and Dabrowski, Adam and Stopa, Marcin and Marciniak, Elzbieta and Obara, Boguslaw},
  year = {2021},
  month = nov,
  journal = {Sensors},
  volume = {21},
  number = {22},
  pages = {7521},
  issn = {1424-8220},
  doi = {10.3390/s21227521},
  urldate = {2022-12-09},
  abstract = {This paper proposes an efficient segmentation of the preretinal area between the inner limiting membrane (ILM) and posterior cortical vitreous (PCV) of the human eye in an image obtained with the use of optical coherence tomography (OCT). The research was carried out using a database of three-dimensional OCT imaging scans obtained with the Optovue RTVue XR Avanti device. Various types of neural networks (UNet, Attention UNet, ReLayNet, LFUNet) were tested for semantic segmentation, their effectiveness was assessed using the Dice coefficient and compared to the graph theory techniques. Improvement in segmentation efficiency was achieved through the use of relative distance maps. We also show that selecting a larger kernel size for convolutional layers can improve segmentation quality depending on the neural network model. In the case of PVC, we obtain the effectiveness reaching up to 96.35\%. The proposed solution can be widely used to diagnose vitreomacular traction changes, which is not yet available in scientific or commercial OCT imaging solutions.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/KJ9ANA7B/Stankiewicz et al. - 2021 - Segmentation of Preretinal Space in Optical Cohere.pdf}
}

@article{stephenson_spatial_2017,
  title = {Spatial and Temporal Changes in Pot-Fishing Effort and Habitat Use},
  author = {Stephenson, Fabrice and Polunin, Nicholas V. C. and Mill, Aileen C. and Scott, Catherine and Lightfoot, Paula and Fitzsimmons, Clare},
  editor = {Kaiser, Michel},
  year = {2017},
  month = oct,
  journal = {ICES Journal of Marine Science},
  volume = {74},
  number = {8},
  pages = {2201--2212},
  issn = {1054-3139, 1095-9289},
  doi = {10.1093/icesjms/fsx051},
  urldate = {2022-04-14},
  abstract = {Habitat and fisheries usage data are key components for ecosystem-based approach to fisheries management (EBFM). Significant gaps in knowledge remain for fisheries\textendash habitat interactions, particularly in inshore fisheries where vessels are {$<$}12 m in length. Here, we show changes in inshore fishing effort distribution ({$<$}12 m) and habitat use over the decade 2004\textendash 2013. Sightings data of fishing vessel activity recorded by the Northumberland Inshore Fishery and Conservation Authority (NIFCA) were combined with landings data to estimate and map potfishing activity between 2004 and 2013. Spatial temporal changes were investigated using Monte Carlo simulation of randomly sampled fishing effort maps. High resolution (1 m) broadscale (EUNIS level 3) predictive habitat maps of the Coquet to St Marys' Marine Conservation Zone (CQSM MCZ) were used to investigate spatial temporal changes in fishers' habitat selection using compositional analysis. Fishing effort in Northumberland increased between 2004 and 2013 (233 642\textendash 354 193 pots year\`A1). Fishing effort distribution differed between individual years, decreasing over large areas between 2004 and 2007, followed by increases, especially inshore, between 2008 and 2013. Fishers in the CQSM MCZ showed a preference for rocky habitats over sediment habitats. Habitat preference did not vary between years although all habitats experienced increasing fishing pressure. Spatial temporal changes in fishing effort and habitat use were discussed in relation to EBFM.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/AL3GC837/Stephenson et al. - 2017 - Spatial and temporal changes in pot-fishing effort.pdf}
}

@inproceedings{stewman_iterative_2006,
  title = {Iterative 3-{{D Pose Correction}} and {{Content-Based Image Retrieval}} for {{Dorsal Fin Recognition}}},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  author = {Stewman, John and Debure, Kelly and Hale, Scott and Russell, Adam},
  editor = {Campilho, Aur{\'e}lio and Kamel, Mohamed S.},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {648--660},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Contour or boundary descriptors may be used in content-based image retrieval to effectively identify appropriate images when image content consists primarily of a single object of interest. The registration of object contours for the purposes of comparison is complicated when the objects of interest are characterized by open contours and when reliable feature points for contour alignment are absent. We present an application that employs an iterative approach to the alignment of open contours for the purposes of image retrieval and demonstrate its success in identifying individual bottlenose dolphins from the profiles of their dorsal fins.},
  isbn = {978-3-540-44893-8},
  langid = {english},
  keywords = {Active Contour,Bottlenose Dolphin,Feature Point,Humpback Whale,Sperm Whale}
}

@article{sug_effect_2010,
  title = {The {{Effect}} of {{Training Set Size}} for the {{Performance}} of {{Neural Networks}} of {{Classification}}},
  author = {Sug, Hyontai},
  year = {2010},
  journal = {WSEAS Transactions on Computers},
  volume = {9},
  number = {11},
  pages = {10},
  abstract = {Even though multilayer perceptrons and radial basis function networks belong to the class of artificial neural networks and they are used for similar tasks, they have very different structures and training mechanisms. So, some researchers showed better performance with radial basis function networks, while others showed some different results with multilayer perceptrons. This paper compares the classification accuracy of the two neural networks with respect to training data set size, and shows the performance of the two neural networks can be differently dependent on training data set size. Experiments show the tendency that multilayer perceptrons have better performance in relatively larger training data sets for some data sets, even though radial basis function networks have better performance in relatively smaller training set size for the same data sets. The experiment was done with four real world data sets.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/YVEL6RYY/Sug - 2010 - The Effect of Training Set Size for the Performanc.pdf}
}

@article{sun_circle_2020,
  title = {Circle {{Loss}}: {{A Unified Perspective}} of {{Pair Similarity Optimization}}},
  shorttitle = {Circle {{Loss}}},
  author = {Sun, Yifan and Cheng, Changmao and Zhang, Yuhan and Zhang, Chi and Zheng, Liang and Wang, Zhongdao and Wei, Yichen},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.10857 [cs]},
  eprint = {2002.10857},
  primaryclass = {cs},
  urldate = {2021-08-11},
  abstract = {This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity sp and minimize the between-class similarity sn. We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed sn and sp into similarity pairs and seek to reduce (sn - sp). Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, i.e., learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing (sn - sp). Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several finegrained image retrieval datasets, the achieved performance is on par with the state of the art.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/AKGZJ6LG/Sun et al. - 2020 - Circle Loss A Unified Perspective of Pair Similar.pdf}
}

@article{sun_revisiting_2017,
  title = {Revisiting {{Unreasonable Effectiveness}} of {{Data}} in {{Deep Learning Era}}},
  author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  year = {2017},
  month = aug,
  journal = {arXiv:1707.02968 [cs.CV]},
  eprint = {1707.02968},
  primaryclass = {cs.CV},
  urldate = {2022-09-27},
  abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10\texttimes{} or 100\texttimes? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pretraining) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-theart results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/8A4WYWJX/Sun et al. - 2017 - Revisiting Unreasonable Effectiveness of Data in D.pdf}
}

@article{suprem_odin_2020,
  title = {{{ODIN}}: {{Automated Drift Detection}} and {{Recovery}} in {{Video Analytics}}},
  shorttitle = {{{ODIN}}},
  author = {Suprem, Abhijit and Arulraj, Joy and Pu, Calton and Ferreira, Joao},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.05440 [cs.CV]},
  eprint = {2009.05440},
  primaryclass = {cs.CV},
  urldate = {2022-07-15},
  abstract = {Recent advances in computer vision have led to a resurgence of interest in visual data analytics. Researchers are developing systems for effectively and efficiently analyzing visual data at scale. A significant challenge that these systems encounter lies in the drift in real-world visual data. For instance, a model for self-driving vehicles that is not trained on images containing snow does not work well when it encounters them in practice. This drift phenomenon limits the accuracy of models employed for visual data analytics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/b3020111/Zotero/storage/AALYJYJM/Suprem et al. - 2020 - ODIN Automated Drift Detection and Recovery in Vi.pdf}
}

@article{swanson_snapshot_2015,
  title = {Snapshot {{Serengeti}}, High-Frequency Annotated Camera Trap Images of 40 Mammalian Species in an {{African}} Savanna},
  author = {Swanson, Alexandra and Kosmala, Margaret and Lintott, Chris and Simpson, Robert and Smith, Arfon and Packer, Craig},
  year = {2015},
  month = jun,
  journal = {Scientific Data},
  volume = {2},
  number = {1},
  pages = {150026},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2015.26},
  urldate = {2021-01-05},
  abstract = {Camera traps can be used to address large-scale questions in community ecology by providing systematic data on an array of wide-ranging species. We deployed 225 camera traps across 1,125\,km2\,in Serengeti National Park, Tanzania, to evaluate spatial and temporal inter-species dynamics. The cameras have operated continuously since 2010 and had accumulated 99,241 camera-trap days and produced 1.2 million sets of pictures by 2013. Members of the general public classified the images via the citizen-science website www.snapshotserengeti.org.Multiple users viewed each image and recorded the species, number of individuals, associated behaviours, and presence of young. Over 28,000 registered users contributed 10.8 million classifications. We applied a simple algorithm to aggregate these individual classifications into a final `consensus' dataset, yielding a final classification for each image and a measure of agreement among individual answers. The consensus classifications and raw imagery provide an unparalleled opportunity to investigate multi-species dynamics in an intact ecosystem and a valuable resource for machine-learning and computer-vision research.},
  copyright = {2015 The Author(s)},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/5NZQ2N83/Swanson et al. - 2015 - Snapshot Serengeti, high-frequency annotated camer.pdf;/Users/b3020111/Zotero/storage/JESA7QAS/sdata201526.html}
}

@inproceedings{szegedy_going_2015,
  title = {Going {{Deeper With Convolutions}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9},
  urldate = {2019-06-05},
  file = {/Users/b3020111/Zotero/storage/UCTZ9YS6/Szegedy et al. - 2015 - Going Deeper With Convolutions.pdf;/Users/b3020111/Zotero/storage/7MY2ALKU/Szegedy_Going_Deeper_With_2015_CVPR_paper.html}
}

@article{tabak_machine_2019,
  title = {Machine Learning to Classify Animal Species in Camera Trap Images: {{Applications}} in Ecology},
  shorttitle = {Machine Learning to Classify Animal Species in Camera Trap Images},
  author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Salvo, Paul A. Di and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {585--590},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13120},
  urldate = {2021-01-05},
  abstract = {Motion-activated cameras (``camera traps'') are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or ``out-of-distribution'' in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
  copyright = {\textcopyright{} 2018 The Authors. Methods in Ecology and Evolution \textcopyright{} 2018 British Ecological Society},
  langid = {english},
  keywords = {artificial intelligence,camera trap,convolutional neural network,deep neural networks,image classification,machine learning,r package,remote sensing},
  file = {/Users/b3020111/Zotero/storage/V6Y6TR3S/Tabak et al. - 2019 - Machine learning to classify animal species in cam.pdf;/Users/b3020111/Zotero/storage/976VRBAX/2041-210X.html}
}

@article{taigman_deepface_2014,
  title = {{{DeepFace}}: {{Closing}} the {{Gap}} to {{Human-Level Performance}} in {{Face Verification}}},
  shorttitle = {{{DeepFace}}},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year = {2014},
  journal = {Facebook Research},
  volume = {Proceedings of the IEEE conference on computer vision and pattern recognition.},
  urldate = {2020-02-12},
  abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={$>$} align ={$>$} represent ={$>$} classify. We revisit both the alignment step and the representation step by employing exp...},
  langid = {american},
  file = {/Users/b3020111/Zotero/storage/2QH2BWPA/deepface-closing-the-gap-to-human-level-performance-in-face-verification.html}
}

@inproceedings{tan_efficientdet_2020,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = {2020},
  month = jun,
  pages = {10778--10787},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01079},
  urldate = {2021-01-13},
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs1, being 4x \textendash{} 9x smaller and using 13x \textendash{} 42x fewer FLOPs than previous detector. Code is available at https://github.com/google/ automl/tree/master/efficientdet.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/YTARGKWL/Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf}
}

@misc{tan_efficientdet_2020-1,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = {2020},
  month = jul,
  number = {arXiv:1911.09070},
  eprint = {1911.09070},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with singlemodel and single-scale, our EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs1, being 4x \textendash{} 9x smaller and using 13x \textendash{} 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/ master/efficientdet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/b3020111/Zotero/storage/BXZ8J2HL/Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf}
}

@article{tassinari_computer_2021,
  title = {A Computer Vision Approach Based on Deep Learning for the Detection of Dairy Cows in Free Stall Barn},
  author = {Tassinari, Patrizia and Bovo, Marco and Benni, Stefano and Franzoni, Simone and Poggi, Matteo and Mammi, Ludovica Maria Eugenia and Mattoccia, Stefano and Di Stefano, Luigi and Bonora, Filippo and Barbaresi, Alberto and Santolini, Enrica and Torreggiani, Daniele},
  year = {2021},
  month = mar,
  journal = {Computers and Electronics in Agriculture},
  volume = {182},
  pages = {106030},
  issn = {01681699},
  doi = {10.1016/j.compag.2021.106030},
  urldate = {2022-05-03},
  abstract = {Precision Livestock Farming relies on several technological approaches to acquire in the most efficient way precise and up-to-date data concerning individual animals. In dairy farming, particular attention is paid to the automatic cow detection and tracking, as such information is closely related to animal welfare and thus to possible health issues. Computer vision represents a suitable and promising method for this purpose.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/F6LXRFQV/Tassinari et al. - 2021 - A computer vision approach based on deep learning .pdf}
}

@article{temitope_yekeen_novel_2020,
  title = {A Novel Deep Learning Instance Segmentation Model for Automated Marine Oil Spill Detection},
  author = {Temitope Yekeen, Shamsudeen and Balogun, Abdul-Lateef and Wan Yusof, Khamaruzaman B.},
  year = {2020},
  month = sep,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {167},
  pages = {190--200},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2020.07.011},
  urldate = {2020-12-03},
  abstract = {The visual similarity of oil slick and other elements, known as look-alike, affects the reliability of synthetic aperture radar (SAR) images for marine oil spill detection. So far, detection and discrimination of oil spill and look-alike are still limited to the use of traditional machine learning algorithms and semantic segmentation deep learning models with limited accuracy. Thus, this study developed a novel deep learning oil spill detection model using computer vision instance segmentation Mask-Region-based Convolutional Neural Network (Mask R-CNN) model. The model training was conducted using transfer learning on the ResNet 101 on COCO as backbone in combination with Feature Pyramid Network (FPN) architecture for feature extraction at 30 epochs with 0.001 learning rate. Testing of the model was conducted using the least training and validation loss value on the withheld testing images. The model's performance was evaluated using precision, recall, specificity, IoU, F1-measure and overall accuracy values. Ship detection and segmentation had the highest performance with overall accuracy of 98.3\%. The model equally showed a higher accuracy for oil spill and look-alike detection and segmentation although oil spill detection outperformed look-alike with overall accuracy values of 96.6\% and 91.0\% respectively. The study concluded that the deep learning instance segmentation model performs better than conventional machine learning models and deep learning semantic segmentation models in detection and segmentation.},
  langid = {english},
  keywords = {Deep learning,Detection,Instance segmentation,Mask R-CNN,Oil spill,SAR},
  file = {/Users/b3020111/Zotero/storage/D4LDBUY7/Temitope Yekeen et al. - 2020 - A novel deep learning instance segmentation model .pdf}
}

@article{temple_by-catch_2021,
  title = {By-Catch Risk for Toothed Whales in Global Small-Scale Fisheries},
  author = {Temple, Andrew J. and Westmerland, Ethan and Berggren, Per},
  year = {2021},
  journal = {Fish and Fisheries},
  volume = {22},
  number = {6},
  pages = {1155--1159},
  issn = {1467-2979},
  doi = {10.1111/faf.12581},
  urldate = {2022-04-13},
  abstract = {Fisheries by-catch poses the single greatest threat to cetacean (whales, dolphins and porpoises) populations. Despite this, by-catch of cetaceans does not receive proportionate levels of research or management effort. The contribution of small-scale fisheries to cetacean by-catch is generally overlooked because of the extreme data paucity in these fisheries. Here, we assess the likely geographic distribution of by-catch risk posed to the odontocetes (toothed whales) at the global scale. We combine species' occurrence and estimates of fisheries susceptibility for all 72 marine toothed whale species with estimates of small-scale fisheries' gillnet fishing pressure across 163 marine fishing nations. We show that the by-catch risk from small-scale fisheries is likely greatest in low- and middle-income regions, generally in the tropics and sub-tropics. Our findings highlight a ``wicked problem'', that the highest by-catch risks primarily occur in regions with lowest fisheries management efficacy. Addressing by-catch in these priority regions is fraught with potentially damaging consequences for the survival of vulnerable human coastal communities. Yet, immediate management and conservation actions are required to prevent species extirpation and extinction through the reduction of small-scale fisheries by-catch. To be successful, these actions will likely require multilateral cooperation and must carefully balance both species and human needs.},
  langid = {english},
  keywords = {artisanal,cetacean,extinction,gillnet,marine mammal,subsistence},
  file = {/Users/b3020111/Zotero/storage/SPHDKBUZ/Temple et al. - 2021 - By-catch risk for toothed whales in global small-s.pdf;/Users/b3020111/Zotero/storage/UT59UFSU/faf.html}
}

@article{temple_by-catch_2021-1,
  title = {By-Catch Risk for Toothed Whales in Global Small-Scale Fisheries},
  author = {Temple, Andrew J. and Westmerland, Ethan and Berggren, Per},
  year = {2021},
  journal = {Fish and Fisheries},
  volume = {22},
  number = {6},
  pages = {1155--1159},
  issn = {1467-2979},
  doi = {10.1111/faf.12581},
  urldate = {2022-04-13},
  abstract = {Fisheries by-catch poses the single greatest threat to cetacean (whales, dolphins and porpoises) populations. Despite this, by-catch of cetaceans does not receive proportionate levels of research or management effort. The contribution of small-scale fisheries to cetacean by-catch is generally overlooked because of the extreme data paucity in these fisheries. Here, we assess the likely geographic distribution of by-catch risk posed to the odontocetes (toothed whales) at the global scale. We combine species' occurrence and estimates of fisheries susceptibility for all 72 marine toothed whale species with estimates of small-scale fisheries' gillnet fishing pressure across 163 marine fishing nations. We show that the by-catch risk from small-scale fisheries is likely greatest in low- and middle-income regions, generally in the tropics and sub-tropics. Our findings highlight a ``wicked problem'', that the highest by-catch risks primarily occur in regions with lowest fisheries management efficacy. Addressing by-catch in these priority regions is fraught with potentially damaging consequences for the survival of vulnerable human coastal communities. Yet, immediate management and conservation actions are required to prevent species extirpation and extinction through the reduction of small-scale fisheries by-catch. To be successful, these actions will likely require multilateral cooperation and must carefully balance both species and human needs.},
  langid = {english},
  keywords = {artisanal,cetacean,extinction,gillnet,marine mammal,subsistence},
  file = {/Users/b3020111/Zotero/storage/PWP2L43U/Temple et al. - 2021 - By-catch risk for toothed whales in global small-s.pdf;/Users/b3020111/Zotero/storage/J2M5Z9N9/faf.html}
}

@article{temple_life-history_2020,
  title = {Life-History, Exploitation and Extinction Risk of the Data-Poor {{Baraka}}'s Whipray ({{Maculabatis}} Ambigua) in Small-Scale Tropical Fisheries},
  author = {Temple, Andrew J. and Stead, Selina M. and Jiddawi, Narriman and Wambiji, Nina and Dulvy, Nicholas K. and Barrowclift, Ellen and Berggren, Per},
  year = {2020},
  journal = {Journal of Fish Biology},
  volume = {97},
  number = {3},
  pages = {708--719},
  issn = {1095-8649},
  doi = {10.1111/jfb.14425},
  urldate = {2022-04-13},
  abstract = {The Baraka's whipray (Maculabatis ambigua) is a major constituent of small-scale fisheries catch in the south-western Indian Ocean. Despite this, little is known of its life-history or exploitation status. We provide the first estimates of crucial life-history parameters and the maximum intrinsic population growth rate rmax, using specimens collected from small-scale fisheries landings in Kenya, Zanzibar and Madagascar (with northern Madagascar representing a range extension for this species). We assess the relative risk of overexploitation by combining rmax with estimates of total Z, fishing F, and natural M mortality, and an estimate of the exploitation ratio E. The data indicate that Baraka's whipray is a medium-sized, fast-growing, early maturing species, with a relatively long lifespan. This results in a high rmax relative to many other elasmobranchs, which when combined with estimates of F suggests that the species is not at imminent risk of extinction. Yet, estimates of exploitation ratio E indicate likely overfishing for the species, with full recruitment to the fishery being post-maturation and exploitation occurring across a broad range of age and size classes. Thus, Baraka's whipray is unlikely to be biologically sustainable in the face of current fisheries pressures. This paper makes an important contribution to filling the gap in available data and is a step towards developing evidence-based fisheries management for this species. Further, it demonstrates a simple and widely applicable framework for assessment of data-poor elasmobranch exploitation status and extinction risk.},
  langid = {english},
  keywords = {Bland\textendash Altman,elasmobranch,Indian Ocean,life-history,rmax},
  file = {/Users/b3020111/Zotero/storage/GQJ5BK8Z/Temple et al. - 2020 - Life-history, exploitation and extinction risk of .pdf;/Users/b3020111/Zotero/storage/QXSLCKIQ/jfb.html}
}

@article{temple_marine_2018,
  title = {Marine Megafauna Interactions with Small-Scale Fisheries in the Southwestern {{Indian Ocean}}: A Review of Status and Challenges for Research and Management},
  shorttitle = {Marine Megafauna Interactions with Small-Scale Fisheries in the Southwestern {{Indian Ocean}}},
  author = {Temple, Andrew J. and Kiszka, Jeremy J. and Stead, Selina M. and Wambiji, Nina and Brito, Atan{\'a}sio and Poonian, Christopher N. S. and Amir, Omar A. and Jiddawi, Narriman and Fennessy, Sean T. and {P{\'e}rez-Jorge}, Sergi and Berggren, Per},
  year = {2018},
  month = mar,
  journal = {Reviews in Fish Biology and Fisheries},
  volume = {28},
  number = {1},
  pages = {89--115},
  issn = {0960-3166, 1573-5184},
  doi = {10.1007/s11160-017-9494-x},
  urldate = {2022-04-13},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/PYM56DMI/Temple et al. - 2018 - Marine megafauna interactions with small-scale fis.pdf}
}

@article{temple_marine_2019,
  title = {Marine Megafauna Catch in Southwestern {{Indian Ocean}} Small-Scale Fisheries from Landings Data},
  author = {Temple, Andrew J. and Wambiji, Nina and Poonian, Chris N.S. and Jiddawi, Narriman and Stead, Selina M. and Kiszka, Jeremy J. and Berggren, Per},
  year = {2019},
  month = feb,
  journal = {Biological Conservation},
  volume = {230},
  pages = {113--121},
  issn = {00063207},
  doi = {10.1016/j.biocon.2018.12.024},
  urldate = {2022-04-13},
  abstract = {The measurable impacts of small-scale fisheries on coastal marine ecosystems and vulnerable megafauna species (elasmobranchs, marine mammals and sea turtles) within them are largely unknown, particularly in developing countries. This study assesses megafauna catch and composition in handline, longline, bottom-set and drift gillnet fisheries of the southwestern Indian Ocean. Observers monitored 21 landing sites across Kenya, Zanzibar and northern Madagascar for 12 months in 2016\textendash 17. Landings (n = 4666) identified 59 species, including three sea turtles, two small cetaceans and one sirenian (Dugong dugon). Primary gear threats to investigated taxa were identified as bottom-set gillnets (marine mammals, sea turtles and batoids), drift gillnets (marine mammals, batoids and sharks) and longlines (sharks). Overall, catch was dominated by small and moderately sized coastal requiem sharks (Carcharhiniformes) and whiprays (Dasyatidae). Larger coastal and oceanic elasmobranchs were also recorded in substantial numbers as were a number of deeper-water species. The diversity of catch demonstrates the potential for small-scale fisheries to have impacts across a number of ecosystems. From the observed catch rates we calculated annual regional elasmobranch landings to be 35,445 (95\%CI 30,478\textendash 40,412) tonnes, 72.6\% more than officially reported in 2016 and 129.2\% more than the 10-year average (2006\textendash 16), constituting 2.48 (95\%CI 2.20\textendash 2.66) million individuals. Productivity-Susceptibility Analyses indicate that small and moderately sized elasmobranchs are most vulnerable in the small-scale fisheries. The study demonstrates substantial underreporting of catches in small-scale fisheries and highlights the need to expand efforts globally to assess the extent and impact of small-scale fisheries on vulnerable marine species and their respective ecosystems.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ITIEARNX/Temple et al. - 2019 - Marine megafauna catch in southwestern Indian Ocea.pdf}
}

@article{thompson_finfindr_2022,
  title = {{{finFindR}}: {{Automated}} Recognition and Identification of Marine Mammal Dorsal Fins Using Residual Convolutional Neural Networks},
  shorttitle = {{{finFindR}}},
  author = {Thompson, Jaime W. and Zero, Victoria H. and Schwacke, Lori H. and Speakman, Todd R. and Quigley, Brian M. and Morey, Jeanine S. and McDonald, Trent L.},
  year = {2022},
  journal = {Marine Mammal Science},
  volume = {38},
  number = {1},
  pages = {139--150},
  issn = {1748-7692},
  doi = {10.1111/mms.12849},
  urldate = {2022-04-21},
  abstract = {Photographic identification is an essential research and management tool for marine mammal scientists. However, manual identification of individuals is time-consuming. To shorten processing times, we developed finFindR, an open-source application that uses a series of neural networks to autonomously locate dorsal fins in unedited field images, quantify an individual's unique fin characteristics, and match them to an existing photograph catalog. During a blind test comparing manual searching to finFindR for common bottlenose dolphin (Tursiops Tursiops truncatus) photographs, experienced photo-identification technicians achieved similar match rates but examined an order of magnitude fewer photographs using finFindR (an average of 10 required with finFindR versus 124 with manual search). In those tests, the correct identity was ranked in the first position in 88\% of cases and was within the top 50 ranked positions in 97\% of cases. Our observations suggest that finFindR's matching capabilities are robust to moderate variation in image quality and fin distinctiveness. Importantly, finFindR allows users to build a catalog of known individuals through time and match an unlimited number of individuals instead of being restricted to a predefined set. finFindR's convolutional neural networks could be re-trained to identify members of many marine mammal species without altering finFindR's inherent structure.},
  langid = {english},
  keywords = {automated detection,cetacean,dolphin,machine learning,neural network,noninvasive sampling,photo-identification,Tursiops truncatus},
  file = {/Users/b3020111/Zotero/storage/Z5HCP8MJ/Thompson et al. - 2022 - finFindR Automated recognition and identification.pdf;/Users/b3020111/Zotero/storage/H2GEDPJ5/mms.html}
}

@misc{thompson_finfindrpdf_2019,
  title = {{{finFindR}}.Pdf},
  author = {Thompson, Jaime},
  year = {2019},
  journal = {Google Docs},
  urldate = {2020-02-12},
  howpublished = {https://drive.google.com/file/d/1jgbnAH2\_C0DoUe8HidJfP13yfhtqz2gB/view?usp=sharing\&usp=embed\_facebook},
  file = {/Users/b3020111/Zotero/storage/87LSWBKE/view.html}
}

@inproceedings{tian_eliminating_2018,
  title = {Eliminating {{Background-bias}} for {{Robust Person Re-identification}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tian, Maoqing and Yi, Shuai and Li, Hongsheng and Li, Shihua and Zhang, Xuesen and Shi, Jianping and Yan, Junjie and Wang, Xiaogang},
  year = {2018},
  month = jun,
  pages = {5794--5803},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00607},
  urldate = {2022-09-27},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/GWYIYAVS/Tian et al. - 2018 - Eliminating Background-bias for Robust Person Re-i.pdf}
}

@article{tian_fcos_2019,
  title = {{{FCOS}}: {{Fully Convolutional One-Stage Object Detection}}},
  shorttitle = {{{FCOS}}},
  author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  year = {2019},
  month = aug,
  journal = {arXiv:1904.01355 [cs]},
  eprint = {1904.01355},
  primaryclass = {cs},
  urldate = {2020-06-24},
  abstract = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7\% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/FXJ66A66/Tian et al. - 2019 - FCOS Fully Convolutional One-Stage Object Detecti.pdf;/Users/b3020111/Zotero/storage/SVAB2LB8/1904.html}
}

@article{tieleman_lecture_2012,
  title = {Lecture 6.5-Rmsprop: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  year = {2012},
  journal = {COURSERA: Neural networks for machine learning},
  volume = {4},
  number = {2},
  pages = {26--31}
}

@inproceedings{timm_large-scale_2018,
  title = {Large-{{Scale Ecological Analyses}} of {{Animals}} in the {{Wild Using Computer Vision}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Timm, Mikayla and Maji, Subhransu and Fuller, Todd},
  year = {2018},
  month = jun,
  pages = {1977--19772},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPRW.2018.00252},
  urldate = {2022-05-03},
  abstract = {Camera traps are increasingly being deployed by ecologists and citizen-scientists as a cost-effective way of obtaining large amounts of animal images in the wild. In order to analyze this data, the images are labeled manually by ecologists, where they identify species of animals and more fine-grained details, such as animal sex or age, or even individual animal identities. However, with the number of camera trap images quickly outgrowing the capacity of the labelers, ecologists are unable to keep up with the wealth of data they are obtaining. Using computer vision, we can automatically generate labels for new camera trap images at the rate that they are being obtained, allowing ecologists to uncover ecological and biological information at a scale previously not possible. In this paper, we explore computer vision approaches for species identification in camera trap images and for individual jaguar identification, both of which show promising results. We make this novel dataset publicly available for future research directions and further exploration.},
  isbn = {978-1-5386-6100-0},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/SUKURLHF/Timm et al. - 2018 - Large-Scale Ecological Analyses of Animals in the .pdf}
}

@inproceedings{tommasi_domain_2015,
  title = {Domain {{Generlization}}: {{A Survey}}},
  shorttitle = {Pattern {{Recognition}}},
  booktitle = {Pattern {{Recognition}}: 37th {{German Conference}}, {{GCPR}} 2015},
  author = {Tommasi, Tatiana and Patricia, Novi and Caputa, Barbara and Tuytelaars, Tinne},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9358},
  publisher = {{Springer International Publishing}},
  address = {{Aachen, Germany}},
  doi = {10.1007/978-3-319-24947-6},
  urldate = {2022-09-26},
  abstract = {The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this paper we propose to verify the potential of the DeCAF features when facing the dataset bias problem. We conduct a series of analyses looking at how existing datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled.},
  isbn = {978-3-319-24946-9 978-3-319-24947-6},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/YUVQ8M6V/Gall et al. - 2015 - Pattern Recognition 37th German Conference, GCPR .pdf}
}

@article{trotter_ndd20_2020,
  title = {{{NDD20}}: {{A}} Large-Scale Few-Shot Dolphin Dataset for Coarse and Fine-Grained Categorisation},
  shorttitle = {{{NDD20}}},
  author = {Trotter, Cameron and Atkinson, Georgia and Sharpe, Matt and Richardson, Kirsten and McGough, A. Stephen and Wright, Nick and Burville, Ben and Berggren, Per},
  year = {2020},
  month = may,
  journal = {arXiv:2005.13359 [cs]},
  eprint = {2005.13359},
  primaryclass = {cs},
  urldate = {2021-01-07},
  abstract = {We introduce the Northumberland Dolphin Dataset 2020 (NDD20), a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation. All data contained in NDD20 was obtained via manual collection in the North Sea around the Northumberland coastline, UK. We present experimentation using standard deep learning network architecture trained using NDD20 and report baselines results.},
  archiveprefix = {arxiv},
  copyright = {Creative Commons Attribution-NonCommercial 4.0 International Licence (CC-BY-NC)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/UZM5449L/Trotter et al. - 2020 - NDD20 A large-scale few-shot dolphin dataset for .pdf;/Users/b3020111/Zotero/storage/HV87KXC6/2005.html}
}

@article{trotter_northumberland_2019,
  title = {The {{Northumberland Dolphin Dataset}}: {{A Multimedia Individual Cetacean Dataset}} for {{Fine-Grained Categorisation}}},
  shorttitle = {The {{Northumberland Dolphin Dataset}}},
  author = {Trotter, Cameron and Atkinson, Georgia and Sharpe, Matthew and McGough, A. Stephen and Wright, Nick and Berggren, Per},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.02669 [cs]},
  eprint = {1908.02669},
  primaryclass = {cs},
  urldate = {2019-08-08},
  abstract = {Methods for cetacean research include photo-identification (photo-id) and passive acoustic monitoring (PAM) which generate thousands of images per expedition that are currently hand categorised by researchers into the individual dolphins sighted. With the vast amount of data obtained it is crucially important to develop a system that is able to categorise this quickly. The Northumberland Dolphin Dataset (NDD) is an on-going novel dataset project made up of above and below water images of, and spectrograms of whistles from, white-beaked dolphins. These are produced by photo-id and PAM data collection methods applied off the coast of Northumberland, UK. This dataset will aid in building cetacean identification models, reducing the number of human-hours required to categorise images. Example use cases and areas identified for speed up are examined.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/THL6KSXC/Trotter et al. - 2019 - The Northumberland Dolphin Dataset A Multimedia I.pdf;/Users/b3020111/Zotero/storage/TWQ2L5L7/1908.html}
}

@inproceedings{trotter_towards_2022,
  title = {Towards {{Automatic Cetacean Photo-Identification}}: {{A Framework}} for {{Fine-Grain}}, {{Few-Shot Learning}} in {{Marine Ecology}}},
  shorttitle = {Towards {{Automatic Cetacean Photo-Identification}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Trotter, Cameron and Wright, Nick and Stephen McGough, A. and Sharpe, Matt and Cheney, Barbara and Civil, Monica Arso and Tyson Moore, Reny and Allen, Jason and Berggren, Per},
  year = {2022},
  month = dec,
  pages = {1942--1949},
  publisher = {{IEEE}},
  address = {{Osaka, Japan}},
  doi = {10.1109/BigData55660.2022.10020942},
  urldate = {2023-02-16},
  copyright = {All rights reserved},
  isbn = {978-1-66548-045-1},
  file = {/Users/b3020111/Zotero/storage/SXE5AM39/Trotter et al. - 2022 - Towards Automatic Cetacean Photo-Identification A.pdf}
}

@inproceedings{truskinger_visualizing_2018,
  title = {Visualizing Five Decades of Environmental Acoustic Data},
  booktitle = {14th {{eScience IEEE International Conference}}},
  author = {Truskinger, Anthony and Brereton, Margot and Roe, Paul},
  year = {2018},
  publisher = {{IEEE  Computer Society}},
  address = {{Amsterdam, Netherlands}},
  abstract = {Monitoring the environment with acoustic sensors is now practical; sensors are sold as commercial devices, storage is cheap, and the field of ecoacoustics is recognized as an effective way to scale monitoring of the environment. However, a pressing challenge faced in many eScience projects is how to manage, analyze, and visualize very large data so that scientists can benefit, with ecoacoustic data presenting its own particular challenges.     This paper presents a new zoomable interactive visualization interface for the exploration of environmental audio data. The interface is a new tool in the Acoustic Workbench, an ecoacoustics software platform built for managing environmental audio data. This Google Maps like interface for audio data, enables zooming in and out of audio data by incorporating specialized, multiresolution, visual representations of audio data into the workbench website. The `zooming' visualization allows scientists to surface the structure, detail, and patterns in content that would otherwise be opaque to them, from scales of seconds through to weeks of data. The Ecosounds instance of the Acoustic Workbench contains 52 years (108 TB) of audio data, from 1016 locations, which results in a 180 million-tile, 8.3 terapixel visualization.   The design and implementation of this novel big audio data visualization is presented along with some design considerations for storing visualization tiles.},
  keywords = {Application file formats}
}

@techreport{tyson_moore_final_2020,
  title = {Final {{Report}}: {{Abundance}} and Distribution of Common Bottlenose Dolphins ({{Tursiops}} Truncatus) near {{Naples}} and {{Marco Island}}, {{Florida}}, {{USA}}, 2018-2019},
  author = {Tyson Moore, Reny B. and Barleycorn, A and Cush, C and Honaker, A and McBride, S.M and Toms, C and Wells, R},
  year = {2020},
  pages = {21},
  institution = {{The Batchelor Foundation}}
}

@article{tyson_moore_rise_2022,
  title = {Rise of the {{Machines}}: {{Best Practices}} and {{Experimental Evaluation}} of {{Computer-Assisted Dorsal Fin Image Matching Systems}} for {{Bottlenose Dolphins}}},
  shorttitle = {Rise of the {{Machines}}},
  author = {Tyson Moore, Reny B. and Urian, Kim W. and Allen, Jason B. and Cush, Carolyn and Parham, Jason R. and Blount, Drew and Holmberg, Jason and Thompson, Jamie W. and Wells, Randall S.},
  year = {2022},
  month = apr,
  journal = {Frontiers in Marine Science},
  volume = {9},
  pages = {849813},
  issn = {2296-7745},
  doi = {10.3389/fmars.2022.849813},
  urldate = {2022-04-21},
  abstract = {Photographic-identification (photo-ID) of bottlenose dolphins using individually distinctive features on the dorsal fin is a well-established and useful tool for tracking individuals; however, this method can be labor-intensive, especially when dealing with large catalogs and/or infrequently surveyed populations. Computer vision algorithms have been developed that can find a fin in an image, characterize the features of the fin, and compare the fin to a catalog of known individuals to generate a ranking of potential matches based on dorsal fin similarity. We examined if and how researchers use computer vision systems in their photo-ID process and developed an experiment to evaluate the performance of the most commonly used, recently developed, systems to date using a long-term photo-ID database of known individuals curated by the Chicago Zoological Society's Sarasota Dolphin Research Program. Survey results obtained for the ``Rise of the machines \textendash{} Application of automated systems for matching dolphin dorsal fins: current status and future directions'' workshop held at the 2019 World Marine Mammal Conference indicated that most researchers still rely on manual methods for comparing unknown dorsal fin images to reference catalogs of known individuals. Experimental evaluation of the finFindR R application, as well as the CurvRank, CurvRank v2, and finFindR implementations in Flukebook suggest that high match rates can be achieved with these systems, with the highest match rates found when only good to excellent quality images of fins with average to high distinctiveness are included in the matching process: for the finFindR R application and the CurvRank and CurvRank v2 algorithms within Flukebook more than 98.92\% of correct matches were in the top 50-ranked positions, and more than 91.94\% of correct matches were returned in the first ranked~position. Our results offer the first comprehensive examination into the performance and accuracy of computer vision algorithms designed to assist with the photo-ID process of bottlenose dolphins and can be used to build trust by researchers hesitant to use these systems. Based on our findings and discussions from the ``Rise of the Machines'' workshop we provide recommendations for best practices for using computer vision systems for dorsal fin photo-ID.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/5U9HPZ44/Tyson Moore et al. - 2022 - Rise of the Machines Best Practices and Experimen.pdf}
}

@misc{tzutalin_labelimg_2021,
  title = {{{LabelImg}}},
  author = {Tzutalin},
  year = {2021},
  month = jan,
  urldate = {2021-01-18},
  abstract = {ðï¸ LabelImg is a graphical image annotation tool and label object bounding boxes in images},
  copyright = {MIT License         ,                 MIT License},
  howpublished = {https://github.com/tzutalin/labelImg},
  keywords = {annotations,deep-learning,detection,image-classification,imagenet,python2,python3,recognition,tools}
}

@article{uijlings_selective_2013,
  title = {Selective Search for Object Recognition},
  author = {Uijlings, Jasper RR and Van De Sande, Koen EA and Gevers, Theo and Smeulders, Arnold WM},
  year = {2013},
  journal = {International journal of computer vision},
  volume = {104},
  number = {2},
  pages = {154--171},
  keywords = {Appearance Model,Colour Space,Exhaustive Search,Object Location,Object Recognition},
  file = {/Users/b3020111/Zotero/storage/JAPFEW4A/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf}
}

@article{urian_recommendations_2015,
  title = {Recommendations for Photo-Identification Methods Used in Capture-Recapture Models with Cetaceans},
  author = {Urian, Kim and Gorgone, Antoinette and Read, Andrew and Balmer, Brian and Wells, Randall S. and Berggren, Per and Durban, John and Eguchi, Tomoharu and Rayment, William and Hammond, Philip S.},
  year = {2015},
  month = jan,
  journal = {Marine Mammal Science},
  volume = {31},
  number = {1},
  pages = {298--321},
  issn = {08240469},
  doi = {10.1111/mms.12141},
  urldate = {2019-01-08},
  langid = {english},
  keywords = {cetaceans,dolphins,photo-id},
  file = {/Users/b3020111/Zotero/storage/WWTMTPAX/Urian et al. - 2015 - Recommendations for photo-identification methods u.pdf}
}

@article{van_aswegen_morphological_2019,
  title = {Morphological Differences between Coastal Bottlenose Dolphin ({{Tursiops}} Aduncus) Populations Identified Using Non-Invasive Stereo-Laser Photogrammetry},
  author = {{van Aswegen}, Martin and Christiansen, Fredrik and Symons, John and Mann, Janet and Nicholson, Krista and Sprogis, Kate and Bejder, Lars},
  year = {2019},
  month = dec,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {12235},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-48419-3},
  urldate = {2019-08-27},
  langid = {english}
}

@article{van_bressem_visual_2018,
  title = {Visual Health Assessment of White-Beaked Dolphins off the Coast of {{Northumberland}}, {{North Sea}}, Using Underwater Photography},
  author = {Van Bressem, Marie-Fran{\c c}oise and Burville, Ben and Sharpe, Matt and Berggren, Per and Van Waerebeek, Koen},
  year = {2018},
  journal = {Marine Mammal Science},
  file = {/Users/b3020111/Zotero/storage/VJNT7LBB/05D562E6-4213-4113-B3BD-9A2A0E599BFD.pdf}
}

@article{van_bressem_visual_2018-1,
  title = {Visual Health Assessment of White-Beaked Dolphins off the Coast of {{Northumberland}}, {{North Sea}}, Using Underwater Photography},
  author = {Van Bressem, Marie-Fran{\c c}oise and Burville, Ben and Sharpe, Matt and Berggren, Per and Van Waerebeek, Koen},
  year = {2018},
  month = oct,
  journal = {Marine Mammal Science},
  volume = {34},
  number = {4},
  pages = {1119--1133},
  issn = {08240469},
  doi = {10.1111/mms.12501},
  urldate = {2019-01-08},
  langid = {english},
  keywords = {health,underwater},
  file = {/Users/b3020111/Zotero/storage/XAEYFGHB/Van Bressem et al. - 2018 - Visual health assessment of white-beaked dolphins .pdf}
}

@misc{van_horn_benchmarking_2021,
  title = {Benchmarking {{Representation Learning}} for {{Natural World Image Collections}}},
  author = {Van Horn, Grant and Cole, Elijah and Beery, Sara and Wilber, Kimberly and Belongie, Serge and Mac Aodha, Oisin},
  year = {2021},
  month = jun,
  number = {arXiv:2103.16483},
  eprint = {2103.16483},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-16},
  abstract = {Recent progress in self-supervised learning has resulted in models that are capable of extracting rich representations from image collections without requiring any explicit label supervision. However, to date the vast majority of these approaches have restricted themselves to training on standard benchmark datasets such as ImageNet. We argue that fine-grained visual categorization problems, such as plant and animal species classification, provide an informative testbed for self-supervised learning. In order to facilitate progress in this area we present two new natural world visual classification datasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k different species uploaded by users of the citizen science application iNaturalist. We designed the latter, NeWT, in collaboration with domain experts with the aim of benchmarking the performance of representation learning algorithms on a suite of challenging natural world binary classification tasks that go beyond standard species classification. These two new datasets allow us to explore questions related to large-scale representation and transfer learning in the context of finegrained categories. We provide a comprehensive analysis of feature extractors trained with and without supervision on ImageNet and iNat2021, shedding light on the strengths and weaknesses of different learned features across a diverse set of tasks. We find that features produced by standard supervised methods still outperform those produced by self-supervised approaches such as SimCLR. However, improved self-supervised learning methods are constantly being released and the iNat2021 and NeWT datasets are a valuable resource for tracking their progress.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/ZE9Z5HUY/Van Horn et al. - 2021 - Benchmarking Representation Learning for Natural W.pdf}
}

@inproceedings{van_horn_building_2015,
  title = {Building a Bird Recognition App and Large Scale Dataset with Citizen Scientists: {{The}} Fine Print in Fine-Grained Dataset Collection},
  shorttitle = {Building a Bird Recognition App and Large Scale Dataset with Citizen Scientists},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Van Horn, Grant and Branson, Steve and Farrell, Ryan and Haber, Scott and Barry, Jessie and Ipeirotis, Panos and Perona, Pietro and Belongie, Serge},
  year = {2015},
  month = jun,
  pages = {595--604},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298658},
  urldate = {2019-03-14},
  abstract = {From these results, we can see that there are clear distinctions between the two different worker pools. Citizen scientists are clearly more capable at labeling fine-grained categories than MTurkers. However, the raw throughput of MTurk means that you can finish annotating your dataset sooner than when using citizen scientists. If the annotation task does not require much domain knowledge (such as part annotation), then MTurkers can perform on par with citizen scientists. Gathering fine-grained category labels with MTurk should be done with care, as we have shown that naive averaging of labels does not converge to the correct label. Finally, the cost savings of using citizen scientists can be significant when the number of annotation tasks grows.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/6QZ4QHGX/Van Horn et al. - 2015 - Building a bird recognition app and large scale da.pdf}
}

@inproceedings{van_horn_inaturalist_2018,
  title = {The Inaturalist Species Classification and Detection Dataset},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  year = {2018},
  pages = {8769--8778}
}

@article{vanbressem_visual_2018,
  title = {Visual Health Assessment of White-Beaked Dolphins off the Coast of {{Northumberland}}, {{North Sea}}, Using Underwater Photography},
  author = {VanBressem, Marie-Fran\{{\textbackslash}c\{c\}\}oise and Burville, Ben and Sharpe, Matt and Berggren, Per and VanWaerebeek, Koen},
  year = {2018},
  journal = {Marine Mammal Science},
  file = {/Users/b3020111/Zotero/storage/HR5D2K2A/VanBressem et al. - 2018 - Visual health assessment of white-beaked dolphins .pdf}
}

@article{vaswani_attention_2017,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {arXiv:1706.03762 [cs.CL]},
  eprint = {1706.03762},
  primaryclass = {cs.CL},
  pages = {11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/IKHF9VL4/Vaswani et al. - Attention is All you Need.pdf}
}

@article{vernazzani_eastern_2013,
  title = {Eastern {{South Pacific}} Southern Right Whale Photoidentification Catalog Reveals Behavior and Habitat Use Patterns},
  author = {Vernazzani, Rbara Galletti and Cabrera, Elsa},
  year = {2013},
  journal = {Marine Mammal Science},
  pages = {10},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/NS64DLCS/Vernazzani and Cabrera - 2013 - Eastern South Pacific southern right whale photoid.pdf}
}

@article{veronique_underwater_2022,
  title = {Underwater Photo-Identification of Sperm Whales ({{Physeter}} Macrocephalus) off {{Mauritius}}},
  author = {V{\'e}ronique, Sarano},
  year = {2022},
  journal = {Marine Biology Research},
  volume = {18},
  number = {1-2},
  pages = {17},
  abstract = {The long-term monitoring of long-lived animals often requires individual identification. For cetaceans, this identification may be based on morphological characters observable from a boat such as shape, spots and cuts of the back, fluke and dorsal fins. However, for some species such as the sperm whales (Physeter macrocephalus), this approach may be challenging as individuals display a rather uniform skin pigmentation. They also do not very often show their fluke, complicating individual identification from a boat. Immature sperm whales that usually have an unharmed fluke may be excluded from photo-identification catalogues. Within the framework of the Maubydick project, focusing on the long-term monitoring of sperm-whales in Mauritius, passive underwater observation and video recording were used to identify long-lasting body markers (e.g. sex, ventral white markings, cut-outs of fins). A catalogue of 38 individuals (six adult males, 18 adult females and 14 immatures) enabled observers to record some nearly-daily, and yearly resightings.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/PRF94PK6/VÃ©ronique - Underwater photo-identification of sperm whales (P.pdf}
}

@inproceedings{vetrova_hidden_2018,
  title = {Hidden {{Features}}: {{Experiments}} with {{Feature Transfer}} for {{Fine-Grained Multi-Class}} and {{One-Class Image Categorization}}},
  shorttitle = {Hidden {{Features}}},
  booktitle = {2018 {{International Conference}} on {{Image}} and {{Vision Computing New Zealand}} ({{IVCNZ}})},
  author = {Vetrova, Varvara and Coup, Sheldon and Frank, Eibe and Cree, Michael J.},
  year = {2018},
  month = nov,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Auckland, New Zealand}},
  doi = {10.1109/IVCNZ.2018.8634790},
  urldate = {2020-06-12},
  abstract = {Can we apply out-of-the box feature transfer using pre-trained convolutional neural networks in fine-grained multiclass image categorization tasks? What is the effect of (a) domainspecific fine-tuning and (b) a special-purpose network architecture designed and trained specifically for the target domain? How do these approaches perform in one-class classification? We investigate these questions by tackling two biological object recognition tasks: classification of ``cryptic'' plants of genus Coprosma and identification of New Zealand moth species. We compare results based on out-of-the-box features extracted using a pre-trained state-of-the-art network to those obtained by finetuning to the target domain, and also evaluate features learned using a simple Siamese network trained only on data from the target domain. For each extracted feature set, we test a number of classifiers, e.g., support vector machines. In addition to multiclass classification, we also consider one-class classification, a scenario that is particularly relevant to biosecurity applications. In the multi-class setting, we find that out-of-the-box lowlevel features extracted from the generic pre-trained network yield high accuracy (90.76\%) when coupled with a simple LDA classifier. Fine-tuning improves accuracy only slightly (to 91.6\%). Interestingly, features extracted from the much simpler Siamese network trained on data from the target domain lead to comparable results (90.8\%). In the one-class classification setting, we note high variability in the area under the ROC curve across feature sets, opening up the possibility of considering an ensemble approach.},
  isbn = {978-1-72810-125-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/Y2692WBU/Vetrova et al. - 2018 - Hidden Features Experiments with Feature Transfer.pdf}
}

@article{voulodimos_deep_2018,
  title = {Deep {{Learning}} for {{Computer Vision}}: {{A Brief Review}}},
  shorttitle = {Deep {{Learning}} for {{Computer Vision}}},
  author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  year = {2018},
  month = feb,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2018},
  pages = {e7068349},
  publisher = {{Hindawi}},
  issn = {1687-5265},
  doi = {10.1155/2018/7068349},
  urldate = {2021-06-14},
  abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/TNBUSWP3/Voulodimos et al. - 2018 - Deep Learning for Computer Vision A Brief Review.pdf;/Users/b3020111/Zotero/storage/AK4BSWQJ/7068349.html}
}

@article{vuola_mask-rcnn_2019,
  title = {Mask-{{RCNN}} and {{U-net Ensembled}} for {{Nuclei Segmentation}}},
  author = {Vuola, Aarno Oskar and Akram, Saad Ullah and Kannala, Juho},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.10170 [cs]},
  eprint = {1901.10170},
  primaryclass = {cs},
  urldate = {2021-01-14},
  abstract = {Nuclei segmentation is both an important and in some ways ideal task for modern computer vision methods, e.g. convolutional neural networks. While recent developments in theory and open-source software have made these tools easier to implement, expert knowledge is still required to choose the right model architecture and training setup. We compare two popular segmentation frameworks, U-Net and Mask-RCNN in the nuclei segmentation task and find that they have different strengths and failures. To get the best of both worlds, we develop an ensemble model to combine their predictions that can outperform both models by a significant margin and should be considered when aiming for best nuclei segmentation performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/RVFC7DYA/Vuola et al. - 2019 - Mask-RCNN and U-net Ensembled for Nuclei Segmentat.pdf}
}

@article{wah_caltech-ucsd_2011,
  title = {The {{Caltech-UCSD Birds-200-2011 Dataset}}},
  author = {Wah, C and Branson, S and Welinder, P and Perona, P and Belongie, S},
  year = {2011},
  journal = {Computation \& Neural Systems Technical Report},
  urldate = {2019-08-07},
  file = {/Users/b3020111/Zotero/storage/7DLXYRVN/27452.html}
}

@misc{waleed_mask_2017,
  title = {Mask {{R-CNN}} for Object Detection and Instance Segmentation on {{Keras}} and {{TensorFlow}}},
  shorttitle = {Mask {{R-CNN}} for Object Detection and Instance Segmentation on {{Keras}} and {{TensorFlow}}},
  author = {Waleed, Abdulla},
  year = {2017},
  urldate = {2019-08-08},
  copyright = {View license},
  howpublished = {https://github.com/matterport/Mask\_RCNN}
}

@article{wang_discriminative_2020,
  title = {Discriminative Fine-Grained Network for Vehicle Re-Identification Using Two-Stage Re-Ranking},
  author = {Wang, Qi and Min, Weidong and He, Daojing and Zou, Song and Huang, Tiemei and Zhang, Yu and Liu, Ruikang},
  year = {2020},
  month = nov,
  journal = {Science China Information Sciences},
  volume = {63},
  number = {11},
  pages = {212102},
  issn = {1674-733X, 1869-1919},
  doi = {10.1007/s11432-019-2811-8},
  urldate = {2022-06-12},
  abstract = {Research on the application of vehicle re-identification to video surveillance has attracted increasingly growing attention. Existing methods are associated with the difficulties of distinguishing different instances of the same car model owing to the incapability of recognizing subtle differences among these instances and the possibility that a subtle difference may lead to incorrect results of ranking. In this paper, a discriminative fine-grained network for vehicle re-identification based on a two-stage re-ranking framework is proposed to address these issues. This discriminative fine-grained network (DFN) is composed of fine-grained and Siamese networks. The proposed hybrid network can extract discriminative features of the vehicle instances with subtle differences. The Siamese network is rather suitable for general object re-identification using two streams of the network, while the fine-grained network is capable of detecting subtle differences. The proposed two-stage re-ranking method allows obtaining a more reliable ranking list by using the Jaccard metric and merging the first and second re-ranking lists, where the latter list is formed using the sample mean feature. Experimental results on the VeRi-776 and VehicleID datasets show that the proposed method achieves the superior performance compared to the state-of-the-art methods used in vehicle re-identification.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/NEIAYYEJ/Wang et al. - 2020 - Discriminative fine-grained network for vehicle re.pdf}
}

@inproceedings{wang_end--end_2021,
  title = {End-to-{{End Video Instance Segmentation}} with {{Transformers}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Yuqing and Xu, Zhaoliang and Wang, Xinlong and Shen, Chunhua and Cheng, Baoshan and Shen, Hao and Xia, Huaxia},
  year = {2021},
  month = jun,
  pages = {8737--8746},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00863},
  urldate = {2022-09-27},
  abstract = {Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/6PI6MKQI/Wang et al. - 2021 - End-to-End Video Instance Segmentation with Transf.pdf}
}

@article{wang_solo_2020,
  title = {{{SOLO}}: {{Segmenting Objects}} by {{Locations}}},
  shorttitle = {{{SOLO}}},
  author = {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},
  year = {2020},
  month = jul,
  journal = {arXiv:1912.04488 [cs]},
  eprint = {1912.04488},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/6N3RNKSH/Wang et al. - 2020 - SOLO Segmenting Objects by Locations.pdf;/Users/b3020111/Zotero/storage/VCL7S5P8/1912.html}
}

@article{wang_solov2_2020,
  title = {{{SOLOv2}}: {{Dynamic}} and {{Fast Instance Segmentation}}},
  shorttitle = {{{SOLOv2}}},
  author = {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},
  year = {2020},
  month = oct,
  journal = {arXiv:2003.10152 [cs]},
  eprint = {2003.10152},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1\% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: https://git.io/AdelaiDet},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/B3WFVJFR/Wang et al. - 2020 - SOLOv2 Dynamic and Fast Instance Segmentation.pdf;/Users/b3020111/Zotero/storage/K7K9WBEM/2003.html}
}

@article{ward_hierarchical_1963,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}},
  author = {Ward, Joe H.},
  year = {1963},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  issn = {0162-1459},
  doi = {10.1080/01621459.1963.10500845},
  urldate = {2020-02-12},
  abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {$>$} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n - 1 mutually exclusive sets by considering the union of all possible n(n - 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
  file = {/Users/b3020111/Zotero/storage/FC6PSAID/01621459.1963.html}
}

@misc{wei_contrastive_2022,
  title = {Contrastive {{Learning Rivals Masked Image Modeling}} in {{Fine-tuning}} via {{Feature Distillation}}},
  author = {Wei, Yixuan and Hu, Han and Xie, Zhenda and Zhang, Zheng and Cao, Yue and Bao, Jianmin and Chen, Dong and Guo, Baining},
  year = {2022},
  month = aug,
  number = {arXiv:2205.14141},
  eprint = {2205.14141},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {Masked image modeling (MIM) learns representations with remarkably good finetuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0\% top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter SwinV2G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1 mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/SwinTransformer/Feature-Distillation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/GB7U5335/Wei et al. - 2022 - Contrastive Learning Rivals Masked Image Modeling .pdf}
}

@misc{wei_fine-grained_2021,
  title = {Fine-{{Grained Image Analysis}} with {{Deep Learning}}: {{A Survey}}},
  shorttitle = {Fine-{{Grained Image Analysis}} with {{Deep Learning}}},
  author = {Wei, Xiu-Shen and Song, Yi-Zhe and Mac Aodha, Oisin and Wu, Jianxin and Peng, Yuxin and Tang, Jinhui and Yang, Jian and Belongie, Serge},
  year = {2021},
  month = nov,
  number = {arXiv:2111.06119},
  eprint = {2111.06119},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-16},
  abstract = {Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to fine-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained research areas \textendash{} fine-grained image recognition and fine-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-specific applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/523T6CIR/Wei et al. - 2021 - Fine-Grained Image Analysis with Deep Learning A .pdf}
}

@article{weideman_integral_2017,
  title = {Integral {{Curvature Representation}} and {{Matching Algorithms}} for {{Identification}} of {{Dolphins}} and {{Whales}}},
  author = {Weideman, Hendrik J. and Jablons, Zachary M. and Holmberg, Jason and Flynn, Kiirsten and Calambokidis, John and Tyson, Reny B. and Allen, Jason B. and Wells, Randall S. and Hupman, Krista and Urian, Kim and Stewart, Charles V.},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.07785 [cs]},
  eprint = {1708.07785},
  primaryclass = {cs},
  urldate = {2020-02-12},
  abstract = {We address the problem of identifying individual cetaceans from images showing the trailing edge of their fins. Given the trailing edge from an unknown individual, we produce a ranking of known individuals from a database. The nicks and notches along the trailing edge define an individual's unique signature. We define a representation based on integral curvature that is robust to changes in viewpoint and pose, and captures the pattern of nicks and notches in a local neighborhood at multiple scales. We explore two ranking methods that use this representation. The first uses a dynamic programming time-warping algorithm to align two representations, and interprets the alignment cost as a measure of similarity. This algorithm also exploits learned spatial weights to downweight matches from regions of unstable curvature. The second interprets the representation as a feature descriptor. Feature keypoints are defined at the local extrema of the representation. Descriptors for the set of known individuals are stored in a tree structure, which allows us to perform queries given the descriptors from an unknown trailing edge. We evaluate the top-k accuracy on two real-world datasets to demonstrate the effectiveness of the curvature representation, achieving top-1 accuracy scores of approximately 95\% and 80\% for bottlenose dolphins and humpback whales, respectively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/C6JAW9XJ/Weideman et al. - 2017 - Integral Curvature Representation and Matching Alg.pdf;/Users/b3020111/Zotero/storage/8E3NGLCW/1708.html}
}

@article{weigmann_revision_2020,
  title = {Revision of the Sixgill Sawsharks, Genus {{Pliotrema}} ({{Chondrichthyes}}, {{Pristiophoriformes}}), with Descriptions of Two New Species and a Redescription of {{P}}. Warreni {{Regan}}},
  author = {Weigmann, Simon and Gon, Ofer and Leeney, Ruth H. and Barrowclift, Ellen and Berggren, Per and Jiddawi, Narriman and Temple, Andrew J.},
  editor = {Schubert, Michael},
  year = {2020},
  month = mar,
  journal = {PLOS ONE},
  volume = {15},
  number = {3},
  pages = {e0228791},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0228791},
  urldate = {2022-04-13},
  abstract = {Recent sampling efforts in Madagascar and Zanzibar, as well as examinations of six-gilled sawsharks in several museum collections provided evidence for a complex of species within Pliotrema warreni Regan. The present manuscript contains a redescription of P. warreni involving the syntypes and additional material, as well as formal descriptions of two new species of Pliotrema Regan. All specimens of both new species were found in the western Indian Ocean. Individuals of the first new species, hereafter referred to as P. kajae sp. nov., were identified originating from Madagascar and the Mascarene Ridge. Specimens of the second new species, hereafter referred to as P. annae sp. nov., were only found off Zanzibar. Pliotrema kajae sp. nov. appears to inhabit upper insular slopes and submarine ridges at depths of 214\textendash 320 m, P. annae sp. nov. so far is only known from shallow waters (20\textendash 35 m). Both new species differ from P. warreni in a number of characteristics including the known distribution range and fresh coloration. Taxonomical differences include barbels that are situated approximately half way from rostral tip to mouth, with prebarbel length equidistant from barbel origin to symphysis of the upper jaw in P. kajae sp. nov. and P. annae sp. nov. (vs. about two thirds way from rostral tip to mouth, with prebarbel length about twice the distance from barbel origin to symphysis of upper jaw in P. warreni) and rostra that are clearly and slightly constricted between barbel origin and nostrils, respectively (vs. rostrum not constricted). Pliotrema kajae sp. nov. differs from P. annae sp. nov. in a longer snout, more numerous large lateral rostral teeth and upper jaw tooth rows, jaw teeth with (vs. without) sharp basal folds, and coloration, particularly pale to light brown (vs. medium to dark brown) dorsal coloration with (vs. without) two indistinct yellowish stripes. A revised diagnosis of Pliotrema and a key to the species are provided.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/LU54BFPE/Weigmann et al. - 2020 - Revision of the sixgill sawsharks, genus Pliotrema.pdf}
}

@article{weinstein_computer_2018,
  title = {A Computer Vision for Animal Ecology},
  author = {Weinstein, Ben G.},
  year = {2018},
  journal = {Journal of Animal Ecology},
  volume = {87},
  number = {3},
  pages = {533--545},
  issn = {1365-2656},
  doi = {10.1111/1365-2656.12780},
  urldate = {2020-02-12},
  abstract = {A central goal of animal ecology is to observe species in the natural world. The cost and challenge of data collection often limit the breadth and scope of ecological study. Ecologists often use image capture to bolster data collection in time and space. However, the ability to process these images remains a bottleneck. Computer vision can greatly increase the efficiency, repeatability and accuracy of image review. Computer vision uses image features, such as colour, shape and texture to infer image content. I provide a brief primer on ecological computer vision to outline its goals, tools and applications to animal ecology. I reviewed 187 existing applications of computer vision and divided articles into ecological description, counting and identity tasks. I discuss recommendations for enhancing the collaboration between ecologists and computer scientists and highlight areas for future growth of automated image analysis.},
  copyright = {\textcopyright{} 2017 The Author. Journal of Animal Ecology \textcopyright{} 2017 British Ecological Society},
  langid = {english},
  keywords = {automation,camera traps,ecological monitoring,images,unmanned aerial vehicles},
  file = {/Users/b3020111/Zotero/storage/GZY4PVE6/Weinstein - 2018 - A computer vision for animal ecology.pdf;/Users/b3020111/Zotero/storage/WNXSDRJR/1365-2656.html}
}

@article{welinder_caltech-ucsd_2010,
  title = {Caltech-{{UCSD Birds}} 200},
  author = {Welinder, Peter and Branson, Steve and Mita, Takeshi and Wah, Catherine and Schroff, Florian and Belongie, Serge and Perona, Pietro},
  year = {2010},
  month = sep,
  journal = {Computation \& Neural Systems Technical Report},
  urldate = {2019-08-07},
  abstract = {Caltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated with 200 bird species. It was created to enable the study of subordinate categorization, which is not possible with other popular datasets that focus on basic level categories (such as PASCAL VOC, Caltech-101, etc). The images were downloaded from the website Flickr and filtered by workers on Amazon Mechanical Turk. Each image is annotated with a bounding box, a rough bird segmentation, and a set of attribute labels.},
  file = {/Users/b3020111/Zotero/storage/G9FNXHGZ/Welinder et al. - 2010 - Caltech-UCSD Birds 200.pdf;/Users/b3020111/Zotero/storage/PGPKCN6N/27468.html}
}

@phdthesis{weller_global_1998,
  title = {Global and Regional Variation in the Biology and Behavior of Bottlenose Dolphins.},
  author = {Weller, D.W.},
  year = {1998},
  address = {{College Station}},
  langid = {english},
  school = {Texas A\&M University}
}

@inproceedings{wilber_animal_2013,
  title = {Animal Recognition in the {{Mojave Desert}}: {{Vision}} Tools for Field Biologists},
  shorttitle = {Animal Recognition in the {{Mojave Desert}}},
  booktitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Wilber, Michael J. and Scheirer, Walter J. and Leitner, Phil and Heflin, Brian and Zott, James and Reinke, Daniel and Delaney, David K. and Boult, Terrance E.},
  year = {2013},
  month = jan,
  pages = {206--213},
  issn = {1550-5790},
  doi = {10.1109/WACV.2013.6475020},
  abstract = {The outreach of computer vision to non-traditional areas has enormous potential to enable new ways of solving real world problems. One such problem is how to incorporate technology in the effort to protect endangered and threatened species in the wild. This paper presents a snapshot of our interdisciplinary team's ongoing work in the Mojave Desert to build vision tools for field biologists to study the currently threatened Desert Tortoise and Mohave Ground Squirrel. Animal population studies in natural habitats present new recognition challenges for computer vision, where open set testing and access to just limited computing resources lead us to algorithms that diverge from common practices. We introduce a novel algorithm for animal classification that addresses the open set nature of this problem and is suitable for implementation on a smartphone. Further, we look at a simple model for object recognition applied to the problem of individual species identification. A thorough experimental analysis is provided for real field data collected in the Mojave desert.},
  keywords = {animal classification,animal population studies,animal recognition,Animals,biology computing,computer vision,Computer vision,desert tortoise,field biologists,image classification,individual species identification,interdisciplinary team,Mohave ground squirrel,Mojave desert,natural habitats,nontraditional areas,object recognition,smart phones,smartphone,Sociology,Statistics,Support vector machines,Training data,vision tools,zoology},
  file = {/Users/b3020111/Zotero/storage/Y9HLV52F/Wilber et al. - 2013 - Animal recognition in the Mojave Desert Vision to.pdf;/Users/b3020111/Zotero/storage/RNDZ2RWM/6475020.html}
}

@article{willi_identifying_2019,
  title = {Identifying Animal Species in Camera Trap Images Using Deep Learning and Citizen Science},
  author = {Willi, Marco and Pitman, Ross T. and Cardoso, Anabelle W. and Locke, Christina and Swanson, Alexandra and Boyer, Amy and Veldthuis, Marten and Fortson, Lucy},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {1},
  pages = {80--91},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13099},
  urldate = {2021-01-05},
  abstract = {Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2\% and 98.0\%, whereas accuracies for identifying specific species were between 88.7\% and 92.7\%. Transferring information from CNNs trained on large datasets (``transfer-learning'') was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3\%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43\% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.},
  copyright = {\textcopyright{} 2018 The Authors. Methods in Ecology and Evolution \textcopyright{} 2018 British Ecological Society},
  langid = {english},
  keywords = {animal identification,camera trap,citizen science,convolutional neural networks,deep learning,machine learning},
  file = {/Users/b3020111/Zotero/storage/G8NR8BHC/Willi et al. - 2019 - Identifying animal species in camera trap images u.pdf;/Users/b3020111/Zotero/storage/XHU2HS3R/2041-210X.html}
}

@article{wolf_transformers_2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  author = {Wolf, Thomas and Chaumond, Julien and Debut, Lysandre and Sanh, Victor and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Louf, Remi},
  year = {2020},
  journal = {Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages = {8},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/WJZ26PTR/Wolf et al. - Transformers State-of-the-Art Natural Language Pr.pdf}
}

@techreport{world_meteorologicial_society_beaufort_1970,
  title = {The {{Beaufort Scale}} of {{Wind Force}}: ({{Technical}} and {{Operational Aspects}})},
  author = {World Meteorologicial Society},
  year = {1970},
  institution = {{World Meteorological Organization}}
}

@misc{wu_detectron2_2020,
  title = {Detectron2},
  author = {Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
  year = {2020},
  month = dec,
  journal = {Detectron2},
  urldate = {2020-12-03},
  abstract = {Detectron2 is FAIR's next-generation platform for object detection and segmentation.},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  howpublished = {https://github.com/facebookresearch/detectron2}
}

@inproceedings{wu_mobile_nodate,
  title = {A {{Mobile Application}} for {{Dog Breed Detection}} and {{Recognition}} Based on {{Deep Learning}}},
  author = {Wu, Fang and Chen, Wenbin and Sinnott, Richard O},
  pages = {10},
  abstract = {Deep learning provides the ability to train algorithms (models) that can tackle the problems of data classification and prediction based on deriving (learning) knowledge from raw data. Convolutional Neural Networks (CNNs) provides one commonly used approach for image classification and detection. In this work we describe a CNN-based method for detecting dogs in potentially complex images and subsequently consider the identification of the type/breed of dogs. The results achieve nearly 85\% accuracy for breed classification for a set of 50 classes of dogs and 64\% accuracy for 120 other less common dog types. An iOS application and associated big data processing infrastructure utilizing a variety of GPUs was used to support the image classification algorithms.},
  langid = {english},
  file = {/Users/b3020111/Dropbox/University/PhD/Data and Cetacians/Wu et al. - A Mobile Application for Dog Breed Detection and R.pdf}
}

@inproceedings{wu_sampling_2017,
  title = {Sampling {{Matters}} in {{Deep Embedding Learning}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wu, Chao-Yuan and Manmatha, R. and Smola, Alexander J. and Krahenbuhl, Philipp},
  year = {2017},
  month = oct,
  pages = {2859--2867},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.309},
  urldate = {2022-09-26},
  abstract = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/U3HT8SXM/Wu et al. - 2017 - Sampling Matters in Deep Embedding Learning.pdf}
}

@article{wu_sampling_2018,
  title = {Sampling {{Matters}} in {{Deep Embedding Learning}}},
  author = {Wu, Chao-Yuan and Manmatha, R. and Smola, Alexander J. and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2018},
  month = jan,
  journal = {arXiv:1706.07567 [cs]},
  eprint = {1706.07567},
  primaryclass = {cs},
  urldate = {2021-02-08},
  abstract = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/7VESJ9E5/Wu et al. - 2018 - Sampling Matters in Deep Embedding Learning.pdf}
}

@article{wursig_methods_1990,
  title = {Methods of Photo-Identification for Small Cetaceans},
  author = {W{\"u}rsig, Bernd and Jefferson, Thomas A},
  year = {1990},
  journal = {Reports of the International Whaling Commission},
  volume = {12},
  pages = {43--52},
  file = {/Users/b3020111/Zotero/storage/FJH76VPM/WÃ¼rsig and Jefferson - 1990 - Methods of photo-identification for small cetacean.pdf}
}

@article{wursig_photographic_1977,
  title = {The {{Photographic Determination}} of {{Group Size}}, {{Composition}}, and {{Stability}} of {{Coastal Porpoises}} ({{Tursiops}} Truncatus)},
  author = {W{\"u}rsig, Bernd and W{\"u}rsig, Melany},
  year = {1977},
  month = nov,
  journal = {Science},
  volume = {198},
  number = {4318},
  pages = {755--756},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.198.4318.755},
  urldate = {2019-08-07},
  abstract = {During a 21-month study, 53 individual bottle-nosed porpoises were recognized by photographs of their dorsal fins. They traveled in small subgroups (mean size = 15) composed of a stable core of five animals plus other individuals that varied greatly from sighting to sighting.},
  copyright = {\textcopyright{} 1977},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/DZ3CT76C/tab-pdf.html}
}

@article{xiao_fashion-mnist:_2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  month = sep,
  journal = {arXiv:1708.07747 [cs, stat]},
  eprint = {1708.07747},
  primaryclass = {cs, stat},
  urldate = {2019-11-30},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/VBYDQTWM/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf;/Users/b3020111/Zotero/storage/KEE89UXE/1708.html}
}

@inproceedings{xie_hierarchical_2013,
  title = {Hierarchical {{Part Matching}} for {{Fine-Grained Visual Categorization}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Xie, Lingxi and Tian, Qi and Hong, Richang and Yan, Shuicheng and Zhang, Bo},
  year = {2013},
  month = dec,
  pages = {1641--1648},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.206},
  urldate = {2019-04-12},
  abstract = {As a special topic in computer vision, fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model.},
  isbn = {978-1-4799-2840-8},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/EIXGXSV4/Xie et al. - 2013 - Hierarchical Part Matching for Fine-Grained Visual.pdf}
}

@article{xie_polarmask_2020,
  title = {{{PolarMask}}: {{Single Shot Instance Segmentation}} with {{Polar Representation}}},
  shorttitle = {{{PolarMask}}},
  author = {Xie, Enze and Sun, Peize and Song, Xiaoge and Wang, Wenhai and Liang, Ding and Shen, Chunhua and Luo, Ping},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.13226 [cs]},
  eprint = {1909.13226},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used as a mask prediction module for instance segmentation, by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9\% in mask mAP with single-model and single-scale training/testing on challenging COCO dataset. For the first time, we demonstrate a much simpler and flexible instance segmentation framework achieving competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation tasks. Code is available at: github.com/xieenze/PolarMask.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/YSBR8T8P/Xie et al. - 2020 - PolarMask Single Shot Instance Segmentation with .pdf;/Users/b3020111/Zotero/storage/LTXTJWBD/1909.html}
}

@article{xu_explicit_2019,
  title = {Explicit {{Shape Encoding}} for {{Real-Time Instance Segmentation}}},
  author = {Xu, Wenqiang and Wang, Haiyang and Qi, Fubo and Lu, Cewu},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.04067 [cs]},
  eprint = {1908.04067},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {In this paper, we propose a novel top-down instance segmentation framework based on explicit shape encoding, named \textbackslash textbf\{ESE-Seg\}. It largely reduces the computational consumption of the instance segmentation by explicitly decoding the multiple object shapes with tensor operations, thus performs the instance segmentation at almost the same speed as the object detection. ESE-Seg is based on a novel shape signature Inner-center Radius (IR), Chebyshev polynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3 outperforms the Mask R-CNN on Pascal VOC 2012 at mAP\$\^r\$@0.5 while 7 times faster.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/EUYDL8GS/Xu et al. - 2019 - Explicit Shape Encoding for Real-Time Instance Seg.pdf;/Users/b3020111/Zotero/storage/CRLEY4M7/1908.html}
}

@article{xu_pp-yoloe_2022,
  title = {{{PP-YOLOE}}: {{An}} Evolved Version of {{YOLO}}},
  shorttitle = {{{PP-YOLOE}}},
  author = {Xu, Shangliang and Wang, Xinxin and Lv, Wenyu and Chang, Qinyao and Cui, Cheng and Deng, Kaipeng and Wang, Guanzhong and Dang, Qingqing and Wei, Shengyu and Du, Yuning and Lai, Baohua},
  year = {2022},
  month = dec,
  journal = {arXiv:2203.16250 [cs.CV]},
  eprint = {2203.16250},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {In this report, we present PP-YOLOE, an industrial state-of-the-art object detector with high performance and friendly deployment. We optimize on the basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful backbone and neck equipped with CSPRepResStage, ET-head and dynamic label assignment algorithm TAL. We provide s/m/l/x models for different practice scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1 FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35\% speed up) and (+1.3 AP, +24.96\% speed up), compared to the previous state-of-the-art industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct extensive experiments to verify the effectiveness of our designs. Source code and pre-trained models are available at https://github.com/PaddlePaddle/PaddleDetection.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/GKR8KCP2/Xu et al. - 2022 - PP-YOLOE An evolved version of YOLO.pdf;/Users/b3020111/Zotero/storage/KRJA37SP/2203.html}
}

@article{yahn_how_2019,
  title = {How to Tell Them Apart? {{Discriminating}} Tropical Blackfish Species Using Fin and Body Measurements from Photographs Taken at Sea},
  shorttitle = {How to Tell Them Apart?},
  author = {Yahn, Shelby N. and Baird, Robin W. and Mahaffy, Sabre D. and Webster, Daniel L.},
  year = {2019},
  month = jan,
  journal = {Marine Mammal Science},
  issn = {08240469},
  doi = {10.1111/mms.12584},
  urldate = {2019-02-04},
  langid = {english}
}

@inproceedings{yan_clusterfit_2020,
  title = {{{ClusterFit}}: {{Improving Generalization}} of {{Visual Representations}}},
  shorttitle = {{{ClusterFit}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yan, Xueting and Misra, Ishan and Gupta, Abhinav and Ghadiyaram, Deepti and Mahajan, Dhruv},
  year = {2020},
  month = jun,
  pages = {6508--6517},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00654},
  urldate = {2022-09-26},
  abstract = {Pre-training convolutional neural networks with weaklysupervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy - ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pretrained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pretraining frameworks \textendash{} weak- and self-supervised, modalities \textendash{} images and videos, and pre-training tasks \textendash{} object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that CF significantly improves the representation quality compared to the state-ofthe-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/FAHE5T8T/Yan et al. - 2020 - ClusterFit Improving Generalization of Visual Rep.pdf}
}

@article{yang_characterization_2021,
  title = {Characterization and Comparison of Echolocation Clicks of White-Beaked Dolphins ({{Lagenorhynchus}} Albirostris) off the {{Northumberland}} Coast, {{UK}}},
  author = {Yang, Liangliang and Sharpe, Matt and Temple, Andrew J and Berggren, Per},
  year = {2021},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {149},
  number = {3},
  pages = {1498--1506},
  issn = {0001-4966},
  doi = {10.1121/10.0003560},
  urldate = {2022-04-13},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/83I9LF6A/Yang et al. - 2021 - Characterization and comparison of echolocation cl.pdf}
}

@article{yang_classification_2017,
  title = {Classification of Underwater Vocalizations of Wild Spotted Seals ({{Phoca}} Largha) in {{Liaodong Bay}}, {{China}}},
  author = {Yang, Liangliang and Xu, Xiaomei and Zhang, Peijun and Han, Jiabo and Li, Bing and Berggren, Per},
  year = {2017},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {141},
  number = {3},
  pages = {2256--2262},
  issn = {0001-4966},
  doi = {10.1121/1.4979056},
  urldate = {2022-04-13},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/L72QVC8Z/Yang et al. - 2017 - Classification of underwater vocalizations of wild.pdf}
}

@article{yang_description_2020,
  title = {Description and Classification of Echolocation Clicks of {{Indian Ocean}} Humpback ({{Sousa}} Plumbea) and {{Indo-Pacific}} Bottlenose ({{Tursiops}} Aduncus) Dolphins from {{Menai Bay}}, {{Zanzibar}}, {{East Africa}}},
  author = {Yang, Liangliang and Sharpe, Matt and Temple, Andrew J. and Jiddawi, Narriman and Xu, Xiaomei and Berggren, Per},
  editor = {Halliday, William David},
  year = {2020},
  month = mar,
  journal = {PLOS ONE},
  volume = {15},
  number = {3},
  pages = {e0230319},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0230319},
  urldate = {2022-04-13},
  abstract = {Passive acoustic monitoring (PAM) is a powerful method to study the occurrence, movement and behavior of echolocating odontocetes (toothed whales) in the wild. However, in areas occupied by more than one species, echolocation clicks need to be classified into species. The present study investigated whether the echolocation clicks produced by small, at-risk, resident sympatric populations of Indian Ocean humpback dolphin (Sousa plumbea) and Indo-Pacific bottlenose dolphin (Tursiops aduncus) in Menai Bay, Zanzibar, East Africa, could be classified to allow species specific monitoring. Underwater sounds of S. plumbea and T. aduncus groups were recorded using a SoundTrap 202HF in January and June-August 2015. Eight acoustic parameters, i.e. -10 dB duration, peak, centroid, lower -3 and lower -10 dB frequencies, and -3 dB, -10 dB and root-mean-squared bandwidth, were used to describe and compare the two species' echolocation clicks. Statistical analyses showed that S. plumbea clicks had significantly higher peak, centroid, lower -3 and lower -10 dB frequencies compared to T. aduncus, whereas duration and bandwidth parameters were similar for the two species. Random Forest (RF) classifiers were applied to determine parameters that could be used to classify the two species from echolocation clicks and achieved 28.6\% and 90.2\% correct species classification rates for S. plumbea and T. aduncus, respectively. Both species were classified at a higher rate than expected at random, however the identified classifiers would only be useful for T. aduncus monitoring. The frequency and bandwidth parameters provided most power for species classification. Further study is necessary to identify useful classifiers for S. plumbea. This study represents a first step in acoustic description and classification of S. plumbea and T. aduncus in the western Indian Ocean region, with potential application for future acoustic monitoring of species-specific temporal and spatial occurrence in these sympatric species.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ZF9BWWLH/Yang et al. - 2020 - Description and classification of echolocation cli.pdf}
}

@article{yang_influence_2022,
  title = {Influence of Ice Concentration and Thickness on Under-Ice Ambient Noise Levels in Shallow Coastal Waters of {{Liaodong Bay}}, {{China}}},
  author = {Yang, Liangliang and Xu, Xiaomei and Berggren, Per},
  year = {2022},
  month = mar,
  journal = {Estuarine, Coastal and Shelf Science},
  volume = {266},
  pages = {107706},
  issn = {02727714},
  doi = {10.1016/j.ecss.2021.107706},
  urldate = {2022-04-13},
  abstract = {Underwater noise levels may be strongly influenced by sea ice. This study presents systematic measurements of under-ice ambient noise levels in shallow ({$<$}30 m) coastal waters using passive acoustic technology. Acoustic data were collected at 6 sites with different ice concentration (two levels: incomplete and complete ice cover) and thickness (three levels: {$<$}10 cm, 10\textendash 20 cm and {$>$}20 cm) in Liaodong Bay, China. Wind (max = 5.3 m/s) and current (max = 0.3 m/s) conditions were very weak, making minimal contributions to under-ice noise in this study. Root-mean-squared broadband (20\textendash 10,000 Hz) noise levels (BNLs) and one-third-octave-band noise levels (TOLs) at 63, 125 and 2000 Hz were measured. Median BNLs of the 6 recording sites ranged from 104.5 to 129.1 dB re 1 {$\mu$}Pa, and median TOLs at 63, 125 and 2000 Hz were between 76.5 and 118.0 dB re 1 {$\mu$}Pa at the 6 sites. Generalized Linear Models were used to analyze broadband and three one-third-octave-band noise levels in different ice concentration and thickness levels. The results showed that, regardless of the ice thickness, the sites with incomplete ice concentration had higher BNLs and 63, 125 and 2000 Hz TOLs than the sites with complete ice concentration, and regardless of the ice concentration, both BNLs and all three TOLs decreased with increased ice thickness. This study provides baseline characterization of under-ice noise in shallow coastal waters and indicating that both ice concentration and thickness have significant influence on the under-ice noise levels. These findings are important for future studies investigating acoustic communication and noise pollution in shallow coastal waters with sea ice.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/ADFYL92U/Yang et al. - 2022 - Influence of ice concentration and thickness on un.pdf}
}

@inproceedings{yang_large-scale_2015,
  title = {A Large-Scale Car Dataset for Fine-Grained Categorization and Verification},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  year = {2015},
  month = jun,
  pages = {3973--3981},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7299023},
  urldate = {2019-03-14},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/X57PHTKL/Yang et al. - 2015 - A large-scale car dataset for fine-grained categor.pdf}
}

@article{yu_fruit_2019,
  title = {Fruit Detection for Strawberry Harvesting Robot in Non-Structural Environment Based on {{Mask-RCNN}}},
  author = {Yu, Yang and Zhang, Kailiang and Yang, Li and Zhang, Dongxing},
  year = {2019},
  month = aug,
  journal = {Computers and Electronics in Agriculture},
  volume = {163},
  pages = {104846},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2019.06.001},
  urldate = {2021-01-20},
  abstract = {Deep learning has demonstrated excellent capabilities for learning image features and is widely used in image object detection. In order to improve the performance of machine vision in fruit detection for a strawberry harvesting robot, Mask Region Convolutional Neural Network (Mask-RCNN) was introduced. Resnet50 was adopted as backbone network, combined with the Feature Pyramid Network (FPN) architecture for feature extraction. The Region Proposal Network (RPN) was trained end-to-end to create region proposals for each feature map. After generating mask images of ripe fruits from Mask R-CNN, a visual localization method for strawberry picking points was performed. Fruit detection results of 100 test images showed that the average detection precision rate was 95.78\%, the recall rate was 95.41\% and the mean intersection over union (MIoU) rate for instance segmentation was 89.85\%. The prediction results of 573 ripe fruit picking points showed that the average error was {$\pm$}1.2\,mm. Compared with four traditional methods, the method proposed demonstrates improved universality and robustness in a non-structural environment, particularly for overlapping and hidden fruits, and those under varying illumination.},
  langid = {english},
  keywords = {Fruit detection,Instance segmentation,Mask-RCNN,Non-structural environment,Picking point},
  file = {/Users/b3020111/Zotero/storage/3PZBX7MQ/Yu et al. - 2019 - Fruit detection for strawberry harvesting robot in.pdf;/Users/b3020111/Zotero/storage/ALENYE22/S0168169919301103.html}
}

@inproceedings{yuan_one-shot_2017,
  title = {One-Shot Learning for Fine-Grained Relation Extraction via Convolutional Siamese Neural Network},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Yuan, Jianbo and Guo, Han and Jin, Zhiwei and Jin, Hongxia and Zhang, Xianchao and Luo, Jiebo},
  year = {2017},
  month = dec,
  pages = {2194--2199},
  doi = {10.1109/BigData.2017.8258168},
  abstract = {Extracting fine-grained relations between entities of interest is of great importance to information extraction and large-scale knowledge graph construction. Conventional approaches on relation extraction require an existing knowledge graph to start with or sufficient observed samples from each relation type in the training process. However, such resources are not always available, and fine-grained manual labeling is extremely time-consuming and requires extensive expertise for specific domains such as healthcare and bioinformatics. Additionally, the distribution of fine-grained relations is often highly imbalanced in practice. We tackle this label scarcity and distribution imbalance issue from a one-shot classification perspective via a convolutional siamese neural network which extracts discriminative semantic-aware features to verify the relations between a pair of input samples. The proposed siamese network effectively extracts uncommon relations with only limited observed samples on the tasks of 1-shot and few-shot classification, demonstrating significant benefits to domain-specific information extraction in practical applications.},
  keywords = {Data mining,Feature extraction,fine-grained,Neural networks,one-shot learning,relation extraction,Semantics,siamese neural network,Testing,Training},
  file = {/Users/b3020111/Zotero/storage/HMP2Y3GY/Yuan et al. - 2017 - One-shot learning for fine-grained relation extrac.pdf;/Users/b3020111/Zotero/storage/YSBHZX27/8258168.html}
}

@article{zeiler_stochastic_2013,
  title = {Stochastic {{Pooling}} for {{Regularization}} of {{Deep Convolutional Neural Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2013},
  month = jan,
  journal = {arXiv:1301.3557 [cs, stat]},
  eprint = {1301.3557},
  primaryclass = {cs, stat},
  urldate = {2019-06-04},
  abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/HXPYGB2H/Zeiler and Fergus - 2013 - Stochastic Pooling for Regularization of Deep Conv.pdf;/Users/b3020111/Zotero/storage/DTNWHC5P/1301.html}
}

@inproceedings{zhang_deformable_2013,
  title = {Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zhang, Ning and Farrell, Ryan and Iandola, Forrest and Darrell, Trevor},
  year = {2013},
  pages = {729--736}
}

@article{zhang_dino_2022,
  title = {{{DINO}}: {{DETR}} with {{Improved DeNoising Anchor Boxes}} for {{End-to-End Object Detection}}},
  shorttitle = {{{DINO}}},
  author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
  year = {2022},
  month = jul,
  journal = {arXiv:2203.03605 [cs.CV]},
  eprint = {2203.03605},
  primaryclass = {cs.CV},
  urldate = {2023-04-27},
  abstract = {We present DINO (\textbackslash textbf\{D\}ETR with \textbackslash textbf\{I\}mproved de\textbackslash textbf\{N\}oising anch\textbackslash textbf\{O\}r boxes), a state-of-the-art end-to-end object detector. \% in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves \$49.4\$AP in \$12\$ epochs and \$51.3\$AP in \$24\$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of \$\textbackslash textbf\{+6.0\}\$\textbackslash textbf\{AP\} and \$\textbackslash textbf\{+2.7\}\$\textbackslash textbf\{AP\}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \textbackslash texttt\{val2017\} (\$\textbackslash textbf\{63.2\}\$\textbackslash textbf\{AP\}) and \textbackslash texttt\{test-dev\} (\textbackslash textbf\{\$\textbackslash textbf\{63.3\}\$AP\}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \textbackslash url\{https://github.com/IDEACVR/DINO\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/Z8FJ5GC5/Zhang et al. - 2022 - DINO DETR with Improved DeNoising Anchor Boxes fo.pdf;/Users/b3020111/Zotero/storage/PZS47NJZ/2203.html}
}

@article{zhang_machine_2017,
  title = {From Machine Learning to Deep Learning: Progress in Machine Intelligence for Rational Drug Discovery},
  shorttitle = {From Machine Learning to Deep Learning},
  author = {Zhang, Lu and Tan, Jianjun and Han, Dan and Zhu, Hao},
  year = {2017},
  month = nov,
  journal = {Drug Discovery Today},
  volume = {22},
  number = {11},
  pages = {1680--1685},
  issn = {13596446},
  doi = {10.1016/j.drudis.2017.08.010},
  urldate = {2021-06-11},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/K4HVK2BN/Zhang et al. - 2017 - From machine learning to deep learning progress i.pdf}
}

@article{zhang_mixup:_2017,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.09412 [cs, stat]},
  eprint = {1710.09412},
  primaryclass = {cs, stat},
  urldate = {2019-01-08},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/GJMG7XN2/Zhang et al. - 2017 - mixup Beyond Empirical Risk Minimization.pdf;/Users/b3020111/Zotero/storage/GVN2VDY8/1710.html}
}

@inproceedings{zhang_part-based_2014,
  title = {Part-{{Based R-CNNs}} for {{Fine-Grained Category Detection}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zhang, Ning and Donahue, Jeff and Girshick, Ross and Darrell, Trevor},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {834--849},
  publisher = {{Springer International Publishing}},
  abstract = {Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.},
  isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {convolutional models,Fine-grained recognition,object detection},
  file = {/Users/b3020111/Zotero/storage/ZUDK3G2G/Zhang et al. - 2014 - Part-Based R-CNNs for Fine-Grained Category Detect.pdf}
}

@article{zhang_unsupervised_2022,
  title = {Unsupervised {{Wildfire Change Detection}} Based on {{Contrastive Learning}}},
  author = {Zhang, Beichen and Wang, Huiqi and Alabri, Amani and Bot, Karol and McCall, Cole and Hamilton, Dale and R{\r{u}}{\v z}i{\v c}ka, V{\'i}t},
  year = {2022},
  month = nov,
  journal = {arXiv:2211.14654 [cs.CV]},
  eprint = {2211.14654},
  primaryclass = {cs.CV},
  urldate = {2022-12-09},
  abstract = {The accurate characterization of the severity of the wildfire event strongly contributes to the characterization of the fuel conditions in fire-prone areas, and provides valuable information for disaster response. The aim of this study is to develop an autonomous system built on top of high-resolution multispectral satellite imagery, with an advanced deep learning method for detecting burned area change. This work proposes an initial exploration of using an unsupervised model for feature extraction in wildfire scenarios. It is based on the contrastive learning technique SimCLR, which is trained to minimize the cosine distance between augmentations of images. The distance between encoded images can also be used for change detection. We propose changes to this method that allows it to be used for unsupervised burned area detection and following downstream tasks. We show that our proposed method outperforms the tested baseline approaches.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/Y2HLNNJ2/Zhang et al. - 2022 - Unsupervised Wildfire Change Detection based on Co.pdf}
}

@inproceedings{zhao_comparing_2018,
  title = {Comparing {{U-Net}} Convolutional Network with Mask {{R-CNN}} in the Performances of Pomegranate Tree Canopy Segmentation},
  booktitle = {Multispectral, {{Hyperspectral}}, and {{Ultraspectral Remote Sensing Technology}}, {{Techniques}} and {{Applications VII}}},
  author = {Zhao, Tiebiao and Yang, Yonghuan and Niu, Haoyu and Wang, Dong and Chen, YangQuan},
  year = {2018},
  month = oct,
  volume = {10780},
  pages = {107801J},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2325570},
  urldate = {2020-12-03},
  abstract = {In the last decade, technologies of unmanned aerial vehicles (UAVs) and small imaging sensors have achieved a significant improvement in terms of equipment cost, operation cost and image quality. These low-cost platforms provide flexible access to high resolution visible and multispectral images. As a result, many studies have been conducted regarding the applications in precision agriculture, such as water stress detection, nutrient status detection, yield prediction, etc. Different from traditional satellite low-resolution images, high-resolution UAVbased images allow much more freedom in image post-processing. For example, the very first procedure in post-processing is pixel classification, or image segmentation for extracting region of interest(ROI). With the very high resolution, it becomes possible to classify pixels from a UAV-based image, yet it is still a challenge to conduct pixel classification using traditional remote sensing features such as vegetation indices (VIs), especially considering various changes during the growing season such as light intensity, crop size, crop color etc. Thanks to the development of deep learning technologies, it provides a general framework to solve this problem. In this study, we proposed to use deep learning methods to conduct image segmentation. We created our data set of pomegranate trees by flying an off-shelf commercial camera at 30 meters above the ground around noon, during the whole growing season from the beginning of April to the middle of October 2017. We then trained and tested two convolutional network based methods U-Net and Mask R-CNN using this data set. Finally, we compared their performances with our dataset aerial images of pomegranate trees. [Tiebiao- add a sentence to summarize the findings and their implications to precision agriculture]},
  file = {/Users/b3020111/Zotero/storage/C9RNN8AM/12.2325570.html}
}

@inproceedings{zhao_pyramid_2017,
  title = {Pyramid {{Scene Parsing Network}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = {2017},
  month = jul,
  pages = {6230--6239},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.660},
  urldate = {2023-04-27},
  abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-regionbased context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixellevel prediction. The proposed approach achieves state-ofthe-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/XIIWGK5I/Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf}
}

@article{zheng_exploiting_2019,
  title = {Exploiting {{Time-Series Image-to-Image Translation}} to {{Expand}} the {{Range}} of {{Wildlife Habitat Analysis}}},
  author = {Zheng, Ruobing and Luo, Ze and Yan, Baoping},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {825--832},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.3301825},
  urldate = {2022-09-26},
  abstract = {Characterizing wildlife habitat is one of the main topics in animal ecology. Locational data obtained from radio tracking and field observation are widely used in habitat analysis. However, such sampling methods are costly and laborious, and insufficient relocations often prevent scientists from conducting large-range and long-term research. In this paper, we innovatively exploit the image-to-image translation technology to expand the range of wildlife habitat analysis. We proposed a novel approach for implementing time-series imageto-image translation via metric embedding. A siamese neural network is used to learn the Euclidean temporal embedding from the image space. This embedding produces temporal vectors which bring time information into the adversarial network. The well-trained framework could effectively map the probabilistic habitat models from remote sensing imagery, helping scientists get rid of the persistent dependence on animal relocations. We illustrate our approach in a real-world application for mapping the habitats of Bar-headed Geese at Qinghai Lake breeding ground. We compare our model against several baselines and achieve promising results.},
  langid = {english},
  file = {/Users/b3020111/Zotero/storage/842EYESM/Zheng et al. - 2019 - Exploiting Time-Series Image-to-Image Translation .pdf}
}

@article{zhong_random_2017,
  title = {Random {{Erasing Data Augmentation}}},
  author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  year = {2017},
  month = nov,
  journal = {arXiv:1708.04896 [cs]},
  eprint = {1708.04896},
  primaryclass = {cs},
  urldate = {2020-10-06},
  abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/9GQEJ45U/Zhong et al. - 2017 - Random Erasing Data Augmentation.pdf;/Users/b3020111/Zotero/storage/P4ZDV8KM/1708.html}
}

@article{zhou_bottom-up_2019,
  title = {Bottom-up {{Object Detection}} by {{Grouping Extreme}} and {{Center Points}}},
  author = {Zhou, Xingyi and Zhuo, Jiacheng and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2019},
  month = apr,
  journal = {arXiv:1901.08043 [cs]},
  eprint = {1901.08043},
  primaryclass = {cs},
  urldate = {2020-12-03},
  abstract = {With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2\% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9\%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6\% Mask AP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/b3020111/Zotero/storage/PI4LK8RY/Zhou et al. - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf;/Users/b3020111/Zotero/storage/H8ZKZJPX/1901.html}
}

@article{zhou_domain_2022,
  title = {Domain {{Generalization}}: {{A Survey}}},
  shorttitle = {Domain {{Generalization}}},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2103.02503},
  primaryclass = {cs},
  pages = {1--20},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3195549},
  urldate = {2022-09-26},
  abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/7DHR9E7X/Zhou et al. - 2022 - Domain Generalization A Survey.pdf}
}

@article{zhou_domain_2022-1,
  title = {Domain {{Generalization}}: {{A Survey}}},
  shorttitle = {Domain {{Generalization}}},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2103.02503},
  primaryclass = {cs},
  pages = {1--20},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3195549},
  urldate = {2022-09-26},
  abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/b3020111/Zotero/storage/U86BMD5Q/Zhou et al. - 2022 - Domain Generalization A Survey.pdf}
}

@inproceedings{zhou_towards_2020,
  title = {Towards {{MLOps}}: {{A Case Study}} of {{ML Pipeline Platform}}},
  shorttitle = {Towards {{MLOps}}},
  booktitle = {2020 {{International Conference}} on {{Artificial Intelligence}} and {{Computer Engineering}} ({{ICAICE}})},
  author = {Zhou, Yue and Yu, Yue and Ding, Bo},
  year = {2020},
  month = oct,
  pages = {494--500},
  doi = {10.1109/ICAICE51518.2020.00102},
  abstract = {The development and deployment of machine learning (ML) applications differ significantly from traditional applications in many ways, which have led to an increasing need for efficient and reliable production of ML applications and supported infrastructures. Though platforms such as TensorFlow Extended (TFX), ModelOps, and Kubeflow have provided end-to-end lifecycle management for ML applications by orchestrating its phases into multistep ML pipelines, their performance is still uncertain. To address this, we built a functional ML platform with DevOps capability from existing continuous integration (CI) or continuous delivery (CD) tools and Kubeflow, constructed and ran ML pipelines to train models with different layers and hyperparameters while time and computing resources consumed were recorded. On this basis, we analyzed the time and resource consumption of each step in the ML pipeline, explored the consumption concerning the ML platform and computational models, and proposed potential performance bottlenecks such as GPU utilization. Our work provides a valuable reference for ML pipeline platform construction in practice.},
  keywords = {Computational modeling,continuous training,Data models,DevOps,end-to-end platform,Graphics processing units,machine learning,MLOps,Pipelines,Task analysis,Tools,Training},
  file = {/Users/b3020111/Zotero/storage/UQRN9H89/Zhou et al. - 2020 - Towards MLOps A Case Study of ML Pipeline Platfor.pdf;/Users/b3020111/Zotero/storage/VD987K9G/9361315.html}
}
