
@article{urian_recommendations_2015,
	title = {Recommendations for photo-identification methods used in capture-recapture models with cetaceans},
	volume = {31},
	issn = {08240469},
	url = {http://doi.wiley.com/10.1111/mms.12141},
	doi = {10.1111/mms.12141},
	language = {en},
	number = {1},
	urldate = {2019-01-08},
	journal = {Marine Mammal Science},
	author = {Urian, Kim and Gorgone, Antoinette and Read, Andrew and Balmer, Brian and Wells, Randall S. and Berggren, Per and Durban, John and Eguchi, Tomoharu and Rayment, William and Hammond, Philip S.},
	month = jan,
	year = {2015},
	keywords = {photo-id, cetaceans, dolphins},
	pages = {298--321},
	file = {Submitted Version:/Users/b3020111/Zotero/storage/WWTMTPAX/Urian et al. - 2015 - Recommendations for photo-identification methods u.pdf:application/pdf}
}

@article{van_bressem_visual_2018,
	title = {Visual health assessment of white-beaked dolphins off the coast of {Northumberland}, {North} {Sea}, using underwater photography},
	volume = {34},
	issn = {08240469},
	url = {http://doi.wiley.com/10.1111/mms.12501},
	doi = {10.1111/mms.12501},
	language = {en},
	number = {4},
	urldate = {2019-01-08},
	journal = {Marine Mammal Science},
	author = {Van Bressem, Marie-Françoise and Burville, Ben and Sharpe, Matt and Berggren, Per and Van Waerebeek, Koen},
	month = oct,
	year = {2018},
	keywords = {underwater, health},
	pages = {1119--1133}
}

@inproceedings{quinonez_using_2019,
	title = {Using {Convolutional} {Neural} {Networks} to {Recognition} of {Dolphin} {Images}},
	isbn = {978-3-030-01171-0},
	abstract = {Classification of specific objects through Convolutional Neural Networks (CNN) has become an interesting research line in the area from information processing and machine learning, main idea is training a image dataset to perform the classifying a given pattern. In this work, a new dataset with 2504 images was introduced, the method used to train the networks was transfer learning to recognition of dolphin images. For this purpose, two models were used: Inception V3 and Inception ResNet V2 to train on TensorFlow platform with different images, corresponding to the four main classes: dolphin, dolphin\_pod, open\_sea, and seabirds. The paper ends with a critical discussion of the experimental results.},
	booktitle = {Trends and {Applications} in {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Quiñonez, Yadira and Zatarain, Oscar and Lizarraga, Carmen and Peraza, Juan},
	editor = {Mejia, Jezreel and Muñoz, Mirna and Rocha, Alvaro and Peña, Adriana and Pérez-Cisneros, Marco},
	year = {2019},
	pages = {236--245},
	file = {Quiñonez et al. - 2019 - Using Convolutional Neural Networks to Recognition.pdf:/Users/b3020111/Zotero/storage/R54YVDT3/Quiñonez et al. - 2019 - Using Convolutional Neural Networks to Recognition.pdf:application/pdf}
}

@inproceedings{bouma_individual_2018,
	title = {Individual common dolphin identification via metric embedding learning},
	abstract = {Photo-identiﬁcation (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin ﬁn photograph captured in the ﬁeld to a catalogue of known individuals.},
	language = {en},
	author = {Bouma, Soren and Pawley, Matthew D M and Hupman, Krista and Gilman, Andrew},
	year = {2018},
	pages = {7},
	file = {Bouma et al. - Individual common dolphin identiﬁcation via metric.pdf:/Users/b3020111/Dropbox/University/PhD/Data and Cetacians/Bouma et al. - Individual common dolphin identiﬁcation via metric.pdf:application/pdf}
}

@article{berger-wolf_wildbook:_2017,
	title = {Wildbook: {Crowdsourcing}, computer vision, and data science for conservation},
	shorttitle = {Wildbook},
	url = {http://arxiv.org/abs/1710.08880},
	abstract = {Photographs, taken by field scientists, tourists, automated cameras, and incidental photographers, are the most abundant source of data on wildlife today. Wildbook is an autonomous computational system that starts from massive collections of images and, by detecting various species of animals and identifying individuals, combined with sophisticated data management, turns them into high resolution information database, enabling scientific inquiry, conservation, and citizen science. We have built Wildbooks for whales (flukebook.org), sharks (whaleshark.org), two species of zebras (Grevy's and plains), and several others. In January 2016, Wildbook enabled the first ever full species (the endangered Grevy's zebra) census using photographs taken by ordinary citizens in Kenya. The resulting numbers are now the official species census used by IUCN Red List: http://www.iucnredlist.org/details/7950/0. In 2016, Wildbook partnered up with WWF to build Wildbook for Sea Turtles, Internet of Turtles (IoT), as well as systems for seals and lynx. Most recently, we have demonstrated that we can now use publicly available social media images to count and track wild animals. In this paper we present and discuss both the impact and challenges that the use of crowdsourced images can have on wildlife conservation.},
	urldate = {2019-01-08},
	journal = {arXiv:1710.08880 [cs]},
	author = {Berger-Wolf, Tanya Y. and Rubenstein, Daniel I. and Stewart, Charles V. and Holmberg, Jason A. and Parham, Jason and Menon, Sreejith and Crall, Jonathan and Van Oast, Jon and Kiciman, Emre and Joppa, Lucas},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.08880},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv\:1710.08880 PDF:/Users/b3020111/Zotero/storage/SNQUXM4C/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf:application/pdf;arXiv\:1710.08880 PDF:/Users/b3020111/Zotero/storage/7XVBYQTL/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/GTMS4JR9/1710.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/UG5YRIIF/1710.html:text/html}
}

@inproceedings{truskinger_visualizing_2018,
	address = {Amsterdam, Netherlands},
	title = {Visualizing five decades of environmental acoustic data},
	url = {https://eprints.qut.edu.au/122730/},
	abstract = {Monitoring the environment with acoustic sensors is now practical; sensors are sold as commercial devices, storage is cheap, and the field of ecoacoustics is recognized as an effective way to scale monitoring of the environment. However, a pressing challenge faced in many eScience projects is how to manage, analyze, and visualize very large data so that scientists can benefit, with ecoacoustic data presenting its own particular challenges.     This paper presents a new zoomable interactive visualization interface for the exploration of environmental audio data. The interface is a new tool in the Acoustic Workbench, an ecoacoustics software platform built for managing environmental audio data. This Google Maps like interface for audio data, enables zooming in and out of audio data by incorporating specialized, multiresolution, visual representations of audio data into the workbench website. The ‘zooming’ visualization allows scientists to surface the structure, detail, and patterns in content that would otherwise be opaque to them, from scales of seconds through to weeks of data. The Ecosounds instance of the Acoustic Workbench contains 52 years (108 TB) of audio data, from 1016 locations, which results in a 180 million-tile, 8.3 terapixel visualization.   The design and implementation of this novel big audio data visualization is presented along with some design considerations for storing visualization tiles.},
	booktitle = {14th {eScience} {IEEE} {International} {Conference}},
	publisher = {IEEE  Computer Society},
	author = {Truskinger, Anthony and Brereton, Margot and Roe, Paul},
	year = {2018},
	keywords = {Application file formats}
}

@inproceedings{roe_catching_2018,
	title = {Catching {Toad} {Calls} in the {Cloud}: {Commodity} {Edge} {Computing} for {Flexible} {Analysis} of {Big} {Sound} {Data}},
	isbn = {1-5386-9156-6},
	booktitle = {2018 {IEEE} 14th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Roe, Paul and Ferroudj, Meriem and Towsey, Michael and Schwarzkopf, Lin},
	year = {2018},
	pages = {67--74}
}

@article{oswald_tool_2007,
	title = {A tool for real-time acoustic species identification of delphinid whistles},
	volume = {122},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2743157},
	doi = {10.1121/1.2743157},
	language = {en},
	number = {1},
	urldate = {2019-01-08},
	journal = {The Journal of the Acoustical Society of America},
	author = {Oswald, Julie N. and Rankin, Shannon and Barlow, Jay and Lammers, Marc O.},
	month = jul,
	year = {2007},
	pages = {587--595}
}

@article{pan_survey_2010,
	title = {A survey on transfer learning},
	volume = {22},
	number = {10},
	journal = {IEEE Transactions on knowledge and data engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	pages = {1345--1359}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2019-01-08},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, resnet50},
	file = {arXiv\:1512.03385 PDF:/Users/b3020111/Zotero/storage/U79QHJ2E/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv\:1512.03385 PDF:/Users/b3020111/Zotero/storage/7STEMDJW/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/RMK4EIHR/1512.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/QE5S4JJE/1512.html:text/html;He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/Users/b3020111/Zotero/storage/AMDEJUSK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2019-01-08},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.01497 PDF:/Users/b3020111/Zotero/storage/8UESWG2A/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/MXAH4FRB/1506.html:text/html}
}

@article{liu_ssd:_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2019-01-08},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
	file = {arXiv\:1512.02325 PDF:/Users/b3020111/Zotero/storage/W27GTLB9/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ISCZ9YLM/1512.html:text/html}
}

@article{huang_speed/accuracy_2016,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	url = {http://arxiv.org/abs/1611.10012},
	abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
	urldate = {2019-01-08},
	journal = {arXiv:1611.10012 [cs]},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.10012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.10012 PDF:/Users/b3020111/Zotero/storage/TPUHAW9Q/Huang et al. - 2016 - Speedaccuracy trade-offs for modern convolutional.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/6PSRKK4Z/1611.html:text/html}
}

@inproceedings{lim_automated_2018,
	title = {Automated {Interpretation} of {Seafloor} {Visual} {Maps} {Obtained} {Using} {Underwater} {Robots}},
	isbn = {1-5386-1654-8},
	booktitle = {2018 {OCEANS}-{MTS}/{IEEE} {Kobe} {Techno}-{Oceans} ({OTO})},
	publisher = {IEEE},
	author = {Lim, Jin Wei and Prügel-Bennett, Adam and Thornton, Blair},
	year = {2018},
	pages = {1--8}
}

@inproceedings{abadi_tensorflow:_2016,
	title = {Tensorflow: a system for large-scale machine learning.},
	volume = {16},
	booktitle = {{OSDI}},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael},
	year = {2016},
	pages = {265--283}
}

@inproceedings{wu_mobile_nodate,
	title = {A {Mobile} {Application} for {Dog} {Breed} {Detection} and {Recognition} based on {Deep} {Learning}},
	abstract = {Deep learning provides the ability to train algorithms (models) that can tackle the problems of data classification and prediction based on deriving (learning) knowledge from raw data. Convolutional Neural Networks (CNNs) provides one commonly used approach for image classification and detection. In this work we describe a CNN-based method for detecting dogs in potentially complex images and subsequently consider the identification of the type/breed of dogs. The results achieve nearly 85\% accuracy for breed classification for a set of 50 classes of dogs and 64\% accuracy for 120 other less common dog types. An iOS application and associated big data processing infrastructure utilizing a variety of GPUs was used to support the image classification algorithms.},
	language = {en},
	author = {Wu, Fang and Chen, Wenbin and Sinnott, Richard O},
	pages = {10},
	file = {Wu et al. - A Mobile Application for Dog Breed Detection and R.pdf:/Users/b3020111/Dropbox/University/PhD/Data and Cetacians/Wu et al. - A Mobile Application for Dog Breed Detection and R.pdf:application/pdf}
}

@article{elsken_neural_2018,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	shorttitle = {Neural {Architecture} {Search}},
	url = {http://arxiv.org/abs/1808.05377},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	urldate = {2019-01-08},
	journal = {arXiv:1808.5377 null},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.5377},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1808.5377 PDF:/Users/b3020111/Zotero/storage/U7VMLSNG/Elsken et al. - 2018 - Neural Architecture Search A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/753P6DD5/1808.html:text/html}
}

@article{breuel_effects_2015,
	title = {The {Effects} of {Hyperparameters} on {SGD} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1508.02788},
	abstract = {The performance of neural network classifiers is determined by a number of hyperparameters, including learning rate, batch size, and depth. A number of attempts have been made to explore these parameters in the literature, and at times, to develop methods for optimizing them. However, exploration of parameter spaces has often been limited. In this note, I report the results of large scale experiments exploring these different parameters and their interactions.},
	urldate = {2019-01-08},
	journal = {arXiv:1508.02788 [cs]},
	author = {Breuel, Thomas M.},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.02788},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, K.3.2},
	file = {arXiv\:1508.02788 PDF:/Users/b3020111/Zotero/storage/3R9PU3QP/Breuel - 2015 - The Effects of Hyperparameters on SGD Training of .pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/XIN5IH2B/1508.html:text/html}
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2019-01-08},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1609.04747 PDF:/Users/b3020111/Zotero/storage/9MQR2SCP/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/62QFN8PH/1609.html:text/html}
}

@article{loshchilov_sgdr:_2016,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	urldate = {2019-01-08},
	journal = {arXiv:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.03983},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv\:1608.03983 PDF:/Users/b3020111/Zotero/storage/43KPKN54/Loshchilov and Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ZGNZ6WS6/1608.html:text/html}
}

@article{jaderberg_population_2017,
	title = {Population {Based} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.09846},
	abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present {\textbackslash}emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
	urldate = {2019-01-08},
	journal = {arXiv:1711.09846 [cs]},
	author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09846},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1711.09846 PDF:/Users/b3020111/Zotero/storage/7MFTI52F/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/BIR9BCMK/1711.html:text/html}
}

@article{huang_snapshot_2017,
	title = {Snapshot {Ensembles}: {Train} 1, get {M} for free},
	shorttitle = {Snapshot {Ensembles}},
	url = {http://arxiv.org/abs/1704.00109},
	abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
	urldate = {2019-01-08},
	journal = {arXiv:1704.00109 [cs]},
	author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
	month = mar,
	year = {2017},
	note = {arXiv: 1704.00109},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1704.00109 PDF:/Users/b3020111/Zotero/storage/RJ7LY6XI/Huang et al. - 2017 - Snapshot Ensembles Train 1, get M for free.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/LQY9MDXI/1704.html:text/html}
}

@article{smith_cyclical_2015,
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.01186},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	urldate = {2019-01-08},
	journal = {arXiv:1506.01186 [cs]},
	author = {Smith, Leslie N.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01186},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1506.01186 PDF:/Users/b3020111/Zotero/storage/BM9FYJNA/Smith - 2015 - Cyclical Learning Rates for Training Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/UGUJ42DH/1506.html:text/html}
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2019-01-08},
	journal = {arXiv:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.06993},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1608.06993 PDF:/Users/b3020111/Zotero/storage/K3U93XP6/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/WIUXMLVY/1608.html:text/html}
}

@article{zhang_mixup:_2017,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2019-01-08},
	journal = {arXiv:1710.09412 [cs, stat]},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09412},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.09412 PDF:/Users/b3020111/Zotero/storage/GJMG7XN2/Zhang et al. - 2017 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/GVN2VDY8/1710.html:text/html}
}

@article{devries_improved_2017,
	title = {Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}},
	url = {http://arxiv.org/abs/1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
	urldate = {2019-01-08},
	journal = {arXiv:1708.04552 [cs]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.04552},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1708.04552 PDF:/Users/b3020111/Zotero/storage/URI6BI5T/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/K735KXT3/1708.html:text/html}
}

@article{elsken_simple_2017,
	title = {Simple {And} {Efficient} {Architecture} {Search} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.04528},
	abstract = {Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6\% in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5\%.},
	urldate = {2019-01-08},
	journal = {arXiv:1711.04528 [cs, stat]},
	author = {Elsken, Thomas and Metzen, Jan-Hendrik and Hutter, Frank},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.04528},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1711.04528 PDF:/Users/b3020111/Zotero/storage/843UYA4I/Elsken et al. - 2017 - Simple And Efficient Architecture Search for Convo.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/GBEBMT6I/1711.html:text/html}
}

@article{he_bag_2018,
	title = {Bag of {Tricks} for {Image} {Classification} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.01187},
	abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
	urldate = {2019-01-08},
	journal = {arXiv:1812.01187 [cs]},
	author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01187},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1812.01187 PDF:/Users/b3020111/Zotero/storage/8FH5QY53/He et al. - 2018 - Bag of Tricks for Image Classification with Convol.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/M9KGD67C/1812.html:text/html}
}

@inproceedings{karnowski_dolphin_2015,
	address = {Waikoloa, HI, USA},
	title = {Dolphin {Detection} and {Tracking}},
	isbn = {978-0-7695-5469-3},
	url = {http://ieeexplore.ieee.org/document/7046814/},
	doi = {10.1109/WACVW.2015.10},
	urldate = {2019-01-14},
	booktitle = {2015 {IEEE} {Winter} {Applications} and {Computer} {Vision} {Workshops}},
	publisher = {IEEE},
	author = {Karnowski, Jeremy and Hutchins, Edwin and Johnson, Christine},
	month = jan,
	year = {2015},
	pages = {51--56}
}

@article{yahn_how_2019,
	title = {How to tell them apart? {Discriminating} tropical blackfish species using fin and body measurements from photographs taken at sea: {DISCRIMINATING} {TROPICAL} {BLACKFISH} {SPECIES}},
	issn = {08240469},
	shorttitle = {How to tell them apart?},
	url = {http://doi.wiley.com/10.1111/mms.12584},
	doi = {10.1111/mms.12584},
	language = {en},
	urldate = {2019-02-04},
	journal = {Marine Mammal Science},
	author = {Yahn, Shelby N. and Baird, Robin W. and Mahaffy, Sabre D. and Webster, Daniel L.},
	month = jan,
	year = {2019}
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2019-02-11},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1405.0312 PDF:/Users/b3020111/Zotero/storage/V4BHGBCE/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/4S98T37Y/1405.html:text/html}
}

@article{liu_mobile_2017,
	title = {Mobile {Video} {Object} {Detection} with {Temporally}-{Aware} {Feature} {Maps}},
	url = {http://arxiv.org/abs/1711.06368},
	abstract = {This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.},
	urldate = {2019-03-06},
	journal = {arXiv:1711.06368 [cs]},
	author = {Liu, Mason and Zhu, Menglong},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06368},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1711.06368 PDF:/Users/b3020111/Zotero/storage/JYZEHKYN/Liu and Zhu - 2017 - Mobile Video Object Detection with Temporally-Awar.pdf:application/pdf;arXiv\:1711.06368 PDF:/Users/b3020111/Zotero/storage/GBUBLB8I/Liu and Zhu - 2017 - Mobile Video Object Detection with Temporally-Awar.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/T9XUGQYL/1711.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/9Y23LQU2/1711.html:text/html}
}

@inproceedings{patterson_fine-grained_2005,
	title = {Fine-grained activity recognition by aggregating abstract object usage},
	doi = {10.1109/ISWC.2005.22},
	abstract = {In this paper we present results related to achieving finegrained activity recognition for context-aware computing applications. We examine the advantages and challenges of reasoning with globally unique object instances detected by an RFID glove. We present a sequence of increasingly powerful probabilistic graphical models for activity recognition. We show the advantages of adding additional complexity and conclude with a model that can reason tractably about aggregated object instances and gracefully generalizes from object instances to their classes by using abstraction smoothing. We apply these models to data collected from a morning household routine.},
	booktitle = {Ninth {IEEE} {International} {Symposium} on {Wearable} {Computers} ({ISWC}'05)},
	author = {Patterson, D. J. and Fox, D. and Kautz, H. and Philipose, M.},
	month = oct,
	year = {2005},
	keywords = {abstract object usage, abstraction smoothing, Character recognition, computer vision, context-aware computing RFID glove, fine-grained activity recognition, Inference algorithms, inference mechanisms, Machine vision, mobile computing, Multimodal sensors, Object detection, planning (artificial intelligence), probabilistic graphical model, radiofrequency identification, Radiofrequency identification, Robustness, Sensor phenomena and characterization, Wearable computers, Wearable sensors},
	pages = {44--51},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/DSTJVV2V/1550785.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/IIMGYJ6S/Patterson et al. - 2005 - Fine-grained activity recognition by aggregating a.pdf:application/pdf}
}

@article{hughes_automated_2017,
	title = {Automated {Visual} {Fin} {Identification} of {Individual} {Great} {White} {Sharks}},
	volume = {122},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-016-0961-y},
	doi = {10.1007/s11263-016-0961-y},
	abstract = {This paper discusses the automated visual identiﬁcation of individual great white sharks from dorsal ﬁn imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained ﬁn images. To the best of our knowledge this line of work establishes the ﬁrst fully automated contour-based visual ID system in the ﬁeld of animal biometrics. The approach put forward appreciates shark ﬁns as textureless, ﬂexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we ﬁrst introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust ﬁn detection. Secondly, we show that combinatorial, scale-space selective ﬁngerprinting can successfully encode ﬁn individuality. We then measure the species-speciﬁc distribution of visual individuality along the ﬁn contour via an embedding into a global ‘ﬁn space’. Exploiting this domain, we ﬁnally propose a non-linear model for individual animal recognition and combine all approaches into a ﬁne-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail.},
	language = {en},
	number = {3},
	urldate = {2019-03-14},
	journal = {International Journal of Computer Vision},
	author = {Hughes, Benjamin and Burghardt, Tilo},
	month = may,
	year = {2017},
	pages = {542--557},
	file = {Hughes and Burghardt - 2017 - Automated Visual Fin Identification of Individual .pdf:/Users/b3020111/Zotero/storage/R5KW3Y26/Hughes and Burghardt - 2017 - Automated Visual Fin Identification of Individual .pdf:application/pdf}
}

@inproceedings{van_horn_building_2015,
	address = {Boston, MA, USA},
	title = {Building a bird recognition app and large scale dataset with citizen scientists: {The} fine print in fine-grained dataset collection},
	isbn = {978-1-4673-6964-0},
	shorttitle = {Building a bird recognition app and large scale dataset with citizen scientists},
	url = {http://ieeexplore.ieee.org/document/7298658/},
	doi = {10.1109/CVPR.2015.7298658},
	abstract = {From these results, we can see that there are clear distinctions between the two different worker pools. Citizen scientists are clearly more capable at labeling ﬁne-grained categories than MTurkers. However, the raw throughput of MTurk means that you can ﬁnish annotating your dataset sooner than when using citizen scientists. If the annotation task does not require much domain knowledge (such as part annotation), then MTurkers can perform on par with citizen scientists. Gathering ﬁne-grained category labels with MTurk should be done with care, as we have shown that naive averaging of labels does not converge to the correct label. Finally, the cost savings of using citizen scientists can be signiﬁcant when the number of annotation tasks grows.},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Van Horn, Grant and Branson, Steve and Farrell, Ryan and Haber, Scott and Barry, Jessie and Ipeirotis, Panos and Perona, Pietro and Belongie, Serge},
	month = jun,
	year = {2015},
	pages = {595--604},
	file = {Van Horn et al. - 2015 - Building a bird recognition app and large scale da.pdf:/Users/b3020111/Zotero/storage/6QZ4QHGX/Van Horn et al. - 2015 - Building a bird recognition app and large scale da.pdf:application/pdf}
}

@inproceedings{di_style_2013,
	address = {OR, USA},
	title = {Style {Finder}: {Fine}-{Grained} {Clothing} {Style} {Detection} and {Retrieval}},
	isbn = {978-0-7695-4990-3},
	shorttitle = {Style {Finder}},
	url = {http://ieeexplore.ieee.org/document/6595844/},
	doi = {10.1109/CVPRW.2013.6},
	abstract = {With the rapid proliferation of smartphones and tablet computers, search has moved beyond text to other modalities like images and voice. For many applications like Fashion, visual search offers a compelling interface that can capture stylistic visual elements beyond color and pattern that cannot be as easily described using text. However, extracting and matching such attributes remains an extremely challenging task due to high variability and deformability of clothing items. In this paper, we propose a ﬁne-grained learning model and multimedia retrieval framework to address this problem. First, an attribute vocabulary is constructed using human annotations obtained on a novel ﬁnegrained clothing dataset. This vocabulary is then used to train a ﬁne-grained visual recognition system for clothing styles. We report benchmark recognition and retrieval results on Women’s Fashion Coat Dataset and illustrate potential mobile applications for attribute-based multimedia retrieval of clothing items and image annotation.},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Di, Wei and Wah, Catherine and Bhardwaj, Anurag and Piramuthu, Robinson and Sundaresan, Neel},
	month = jun,
	year = {2013},
	pages = {8--13},
	file = {Di et al. - 2013 - Style Finder Fine-Grained Clothing Style Detectio.pdf:/Users/b3020111/Zotero/storage/BMFGKBV8/Di et al. - 2013 - Style Finder Fine-Grained Clothing Style Detectio.pdf:application/pdf}
}

@article{sinha_face_2006,
	title = {Face {Recognition} by {Humans}: {Nineteen} {Results} {All} {Computer} {Vision} {Researchers} {Should} {Know} {About}},
	volume = {94},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Face {Recognition} by {Humans}},
	url = {http://ieeexplore.ieee.org/document/4052483/},
	doi = {10.1109/JPROC.2006.884093},
	abstract = {A key goal of computer vision researchers is to create automated face recognition systems that can equal, and eventually surpass, human performance. To this end, it is imperative that computational researchers know of the key findings from experimental studies of face recognition by humans. These findings provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities. In this paper, we present what we believe are 19 basic results, with implications for the design of computational systems. Each result is described briefly and appropriate pointers are provided to permit an in-depth study of any particular result.},
	language = {en},
	number = {11},
	urldate = {2019-03-14},
	journal = {Proceedings of the IEEE},
	author = {Sinha, P. and Balas, B. and Ostrovsky, Y. and Russell, R.},
	month = nov,
	year = {2006},
	pages = {1948--1962},
	file = {Sinha et al. - 2006 - Face Recognition by Humans Nineteen Results All C.pdf:/Users/b3020111/Zotero/storage/L2NCZB59/Sinha et al. - 2006 - Face Recognition by Humans Nineteen Results All C.pdf:application/pdf}
}

@book{noauthor_computer_2014,
	address = {New York},
	edition = {1st edition},
	series = {Lecture notes in computer science},
	title = {Computer vision - {ECCV} 2014: 13th {European} {Conference}, {Zurich}, {Switzerland}, {September} 6-12, 2014, {Proceedings}, part i},
	isbn = {978-3-319-10589-5},
	shorttitle = {Computer vision - {ECCV} 2014},
	language = {en},
	number = {8689},
	publisher = {Springer},
	year = {2014},
	file = {2014 - Computer vision - ECCV 2014 13th European Confere.pdf:/Users/b3020111/Zotero/storage/VADUBVSN/2014 - Computer vision - ECCV 2014 13th European Confere.pdf:application/pdf}
}

@inproceedings{yang_large-scale_2015,
	address = {Boston, MA, USA},
	title = {A large-scale car dataset for fine-grained categorization and verification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299023/},
	doi = {10.1109/CVPR.2015.7299023},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
	month = jun,
	year = {2015},
	pages = {3973--3981},
	file = {Yang et al. - 2015 - A large-scale car dataset for fine-grained categor.pdf:/Users/b3020111/Zotero/storage/X57PHTKL/Yang et al. - 2015 - A large-scale car dataset for fine-grained categor.pdf:application/pdf}
}

@book{institute_of_electrical_and_electronics_engineers_2009_2009,
	address = {Piscataway, NJ},
	title = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}: {CVPR} 2009 ; {Miami} [{Beach}], {Florida}, {USA}, 20 - 25 {June} 2009},
	isbn = {978-1-4244-3992-8 978-1-4244-3991-1},
	shorttitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers and IEEE Computer Society},
	year = {2009}
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	language = {en},
	number = {2},
	urldate = {2019-03-20},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	pages = {303--338},
	file = {Submitted Version:/Users/b3020111/Zotero/storage/Z9XEBVKA/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf}
}

@article{fei-fei_learning_2007,
	title = {Learning generative visual models from few training examples: {An} incremental {Bayesian} approach tested on 101 object categories},
	volume = {106},
	issn = {10773142},
	shorttitle = {Learning generative visual models from few training examples},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206001688},
	doi = {10.1016/j.cviu.2005.09.012},
	language = {en},
	number = {1},
	urldate = {2019-03-20},
	journal = {Computer Vision and Image Understanding},
	author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
	month = apr,
	year = {2007},
	pages = {59--70}
}

@inproceedings{van_horn_inaturalist_2018,
	title = {The inaturalist species classification and detection dataset},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
	year = {2018},
	pages = {8769--8778}
}

@article{uijlings_selective_2013,
	title = {Selective search for object recognition},
	volume = {104},
	number = {2},
	journal = {International journal of computer vision},
	author = {Uijlings, Jasper RR and Van De Sande, Koen EA and Gevers, Theo and Smeulders, Arnold WM},
	year = {2013},
	keywords = {Appearance Model, Colour Space, Exhaustive Search, Object Location, Object Recognition},
	pages = {154--171},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/JAPFEW4A/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf}
}

@inproceedings{farrell_birdlets:_2011,
	title = {Birdlets: {Subordinate} categorization using volumetric primitives and pose-normalized appearance},
	isbn = {1-4577-1102-8},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Farrell, Ryan and Oza, Om and Zhang, Ning and Morariu, Vlad I. and Darrell, Trevor and Davis, Larry S.},
	year = {2011},
	pages = {161--168}
}

@inproceedings{bourdev_poselets:_2009,
	title = {Poselets: {Body} part detectors trained using 3d human pose annotations},
	isbn = {1-4244-4420-9},
	booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Bourdev, Lubomir and Malik, Jitendra},
	year = {2009},
	pages = {1365--1372}
}

@article{felzenszwalb_object_2010,
	title = {Object detection with discriminatively trained part-based models},
	volume = {32},
	number = {9},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
	year = {2010},
	pages = {1627--1645}
}

@inproceedings{parkhi_cats_2012,
	title = {Cats and dogs},
	isbn = {1-4673-1228-2},
	booktitle = {2012 {IEEE} conference on computer vision and pattern recognition},
	publisher = {IEEE},
	author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
	year = {2012},
	pages = {3498--3505}
}

@inproceedings{zhang_deformable_2013,
	title = {Deformable part descriptors for fine-grained recognition and attribute prediction},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Zhang, Ning and Farrell, Ryan and Iandola, Forrest and Darrell, Trevor},
	year = {2013},
	pages = {729--736}
}

@article{belhumeur_localizing_2013,
	title = {Localizing parts of faces using a consensus of exemplars},
	volume = {35},
	number = {12},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Belhumeur, Peter N. and Jacobs, David W. and Kriegman, David J. and Kumar, Neeraj},
	year = {2013},
	pages = {2930--2940}
}

@inproceedings{liu_dog_2012,
	title = {Dog breed classification using part localization},
	booktitle = {European conference on computer vision},
	publisher = {Springer},
	author = {Liu, Jiongxin and Kanazawa, Angjoo and Jacobs, David and Belhumeur, Peter},
	year = {2012},
	pages = {172--185}
}

@inproceedings{gavves_fine-grained_2013,
	title = {Fine-{Grained} {Categorization} by {Alignments}},
	url = {http://openaccess.thecvf.com/content_iccv_2013/html/Gavves_Fine-Grained_Categorization_by_2013_ICCV_paper.html},
	urldate = {2019-04-12},
	author = {Gavves, E. and Fernando, B. and Snoek, C. G. M. and Smeulders, A. W. M. and Tuytelaars, T.},
	year = {2013},
	pages = {1713--1720},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/R336N6YG/Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf:application/pdf;Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf:/Users/b3020111/Zotero/storage/3FNJNV8Y/Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/DLKGGEFZ/Gavves_Fine-Grained_Categorization_by_2013_ICCV_paper.html:text/html}
}

@inproceedings{xie_hierarchical_2013,
	address = {Sydney, Australia},
	title = {Hierarchical {Part} {Matching} for {Fine}-{Grained} {Visual} {Categorization}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751314/},
	doi = {10.1109/ICCV.2013.206},
	abstract = {As a special topic in computer vision, ﬁne-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classiﬁcation tasks in which objects have large inter-class variation, the visual concepts in the ﬁne-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difﬁcult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model.},
	language = {en},
	urldate = {2019-04-12},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Xie, Lingxi and Tian, Qi and Hong, Richang and Yan, Shuicheng and Zhang, Bo},
	month = dec,
	year = {2013},
	pages = {1641--1648},
	file = {Xie et al. - 2013 - Hierarchical Part Matching for Fine-Grained Visual.pdf:/Users/b3020111/Zotero/storage/EIXGXSV4/Xie et al. - 2013 - Hierarchical Part Matching for Fine-Grained Visual.pdf:application/pdf}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	pages = {4},
	file = {Paszke et al. - Automatic differentiation in PyTorch.pdf:/Users/b3020111/Zotero/storage/C7IV49XY/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf}
}

@inproceedings{deng_imagenet:_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	urldate = {2019-05-30},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
	file = {Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:/Users/b3020111/Zotero/storage/DRIS6Y3J/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/Users/b3020111/Zotero/storage/ZX3TYXVX/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2019-06-04},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation, beginnings},
	pages = {115--133},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/DN2VLF7L/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2019-06-04},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1502.03167 PDF:/Users/b3020111/Zotero/storage/PXJLUU3K/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv\:1502.03167 PDF:/Users/b3020111/Zotero/storage/JN2GN9LU/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/T6VEIVFW/1502.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/LMBHZT8H/1502.html:text/html}
}

@inproceedings{bottou_large-scale_2010,
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	isbn = {978-3-7908-2604-3},
	abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
	language = {en},
	booktitle = {Proceedings of {COMPSTAT}'2010},
	publisher = {Physica-Verlag HD},
	author = {Bottou, Léon},
	editor = {Lechevallier, Yves and Saporta, Gilbert},
	year = {2010},
	keywords = {efficiency, online learning, stochastic gradient descent},
	pages = {177--186},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/7ITG9H83/Bottou - 2010 - Large-Scale Machine Learning with Stochastic Gradi.pdf:application/pdf}
}

@inproceedings{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN’s), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.},
	booktitle = {Proceedings of the {IEEE}},
	author = {Lecun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	pages = {2278--2324},
	file = {Citeseer - Full Text PDF:/Users/b3020111/Zotero/storage/EJZX7HLS/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf;Citeseer - Snapshot:/Users/b3020111/Zotero/storage/MUKSRDLZ/summary.html:text/html}
}

@article{boureau_theoretical_2010,
	title = {A {Theoretical} {Analysis} of {Feature} {Pooling} in {Visual} {Recognition}},
	abstract = {Many modern visual recognition algorithms incorporate a step of spatial ‘pooling’, where the outputs of several nearby feature detectors are combined into a local or global ‘bag of features’, in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly inﬂuence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.},
	language = {en},
	author = {Boureau, Y-Lan and Ponce, Jean and LeCun, Yann},
	year = {2010},
	pages = {8},
	file = {Boureau et al. - A Theoretical Analysis of Feature Pooling in Visua.pdf:/Users/b3020111/Zotero/storage/BKAPV48R/Boureau et al. - A Theoretical Analysis of Feature Pooling in Visua.pdf:application/pdf}
}

@article{zeiler_stochastic_2013,
	title = {Stochastic {Pooling} for {Regularization} of {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1301.3557},
	abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
	urldate = {2019-06-04},
	journal = {arXiv:1301.3557 [cs, stat]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3557},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1301.3557 PDF:/Users/b3020111/Zotero/storage/HXPYGB2H/Zeiler and Fergus - 2013 - Stochastic Pooling for Regularization of Deep Conv.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/DTNWHC5P/1301.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2019-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Snapshot:/Users/b3020111/Zotero/storage/BHYY9P53/4824-imagenet-classification-with-deep-convolutional-neural-networ.html:text/html}
}

@inproceedings{girshick_rich_2014,
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
	urldate = {2019-06-05},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	pages = {580--587},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/JGNEX6FD/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/9VHKQ9ZC/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html:text/html}
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} {With} {Convolutions}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
	urldate = {2019-06-05},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	pages = {1--9},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/UCTZ9YS6/Szegedy et al. - 2015 - Going Deeper With Convolutions.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/7MY2ALKU/Szegedy_Going_Deeper_With_2015_CVPR_paper.html:text/html}
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2019-06-05},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1504.08083 PDF:/Users/b3020111/Zotero/storage/X6QD9372/Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ZL4KXHHF/1504.html:text/html}
}

@inproceedings{redmon_you_2016,
	address = {Las Vegas, NV, USA},
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	language = {en},
	urldate = {2019-06-05},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2016},
	pages = {779--788},
	file = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:/Users/b3020111/Zotero/storage/UBCXIYVS/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}

@misc{griffin_caltech-256_2007,
	type = {Report or {Paper}},
	title = {Caltech-256 {Object} {Category} {Dataset}},
	url = {http://resolver.caltech.edu/CaltechAUTHORS:CNS-TR-2007-001},
	abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
	urldate = {2019-06-05},
	author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
	month = mar,
	year = {2007},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/5GCVQRFP/Griffin et al. - 2007 - Caltech-256 Object Category Dataset.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/6MHTRL5S/7694.html:text/html}
}

@inproceedings{zhang_part-based_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Part-{Based} {R}-{CNNs} for {Fine}-{Grained} {Category} {Detection}},
	isbn = {978-3-319-10590-1},
	abstract = {Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zhang, Ning and Donahue, Jeff and Girshick, Ross and Darrell, Trevor},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {convolutional models, Fine-grained recognition, object detection},
	pages = {834--849},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/ZUDK3G2G/Zhang et al. - 2014 - Part-Based R-CNNs for Fine-Grained Category Detect.pdf:application/pdf}
}

@article{moore_marine_2008,
	title = {Marine mammals as ecosystem sentinels},
	volume = {89},
	issn = {0022-2372, 1545-1542},
	url = {https://academic.oup.com/jmammal/article-lookup/doi/10.1644/07-MAMM-S-312R1.1},
	doi = {10.1644/07-MAMM-S-312R1.1},
	abstract = {The earth’s climate is changing, possibly at an unprecedented rate. Overall, the planet is warming, sea ice and glaciers are in retreat, sea level is rising, and pollutants are accumulating in the environment and within organisms. These clear physical changes undoubtedly affect marine ecosystems. Species dependent on sea ice, such as the polar bear (Ursus maritimus) and the ringed seal (Phoca hispida), provide the clearest examples of sensitivity to climate change. Responses of cetaceans to climate change are more difﬁcult to discern, but in the eastern North Paciﬁc evidence is emerging that gray whales (Eschrichtius robustus) are delaying their southbound migration, expanding their feeding range along the migration route and northward to Arctic waters, and even remaining in polar waters over winter—all indications that North Paciﬁc and Arctic ecosystems are in transition. To use marine mammals as sentinels of ecosystem change, we must expand our existing research strategies to encompass the decadal and ocean-basin temporal and spatial scales consistent with their natural histories.},
	language = {en},
	number = {3},
	urldate = {2019-08-07},
	journal = {Journal of Mammalogy},
	author = {Moore, Sue E.},
	month = jun,
	year = {2008},
	pages = {534--540},
	file = {Moore - 2008 - Marine mammals as ecosystem sentinels.pdf:/Users/b3020111/Zotero/storage/K3JB8DTI/Moore - 2008 - Marine mammals as ecosystem sentinels.pdf:application/pdf}
}

@article{connor_male_2015,
	title = {Male dolphin alliances in {Shark} {Bay}: changing perspectives in a 30-year study},
	volume = {103},
	issn = {0003-3472},
	shorttitle = {Male dolphin alliances in {Shark} {Bay}},
	url = {http://www.sciencedirect.com/science/article/pii/S0003347215000810},
	doi = {10.1016/j.anbehav.2015.02.019},
	abstract = {Bottlenose dolphins, Tursiops cf. aduncus, in Shark Bay, Western Australia exhibit the most complex alliances known outside of humans. Advances in our understanding of these alliances have occurred with expansions of our study area each decade. In the 1980s, we discovered that males cooperated in stable trios and pairs (first-order alliances) to herd individual oestrous females, and that two such alliances of four to six, sometimes related, individuals (second-order alliances) cooperated against other males in contests over females. The 1990s saw the discovery of a large 14-member second-order alliance whose members exhibited labile first-order alliance formation among nonrelatives. Partner preferences as well as a relationship between first-order alliance stability and consortship rate in this ‘super-alliance’ indicated differentiated relationships. The contrast between the super-alliance and the 1980s alliances suggested two alliance tactics. An expansion of the study area in the 2000s revealed a continuum of second-order alliance sizes in an open social network and no simple relationship between second-order alliance size and alliance stability, but generalized the relationship between first-order alliance stability and consortship rate within second-order alliances. Association preferences and contests involving three second-order alliances indicated the presence of third-order alliances. Second-order alliances may persist for 20 years with stability thwarted by gradual attrition, but underlying flexibility is indicated by observations of individuals joining other alliances, including old males joining young or old second-order alliances. The dolphin research has informed us on the evolution of complex social relationships and large brain evolution in mammals and the ecology of alliance formation. Variation in odontocete brain size and the large radiation of delphinids into a range of habitats holds great promise that further effort to describe their societies will be rewarded with similar advances in our understanding of these important issues.},
	urldate = {2019-08-07},
	journal = {Animal Behaviour},
	author = {Connor, Richard C. and Krützen, Michael},
	month = may,
	year = {2015},
	keywords = {alliance, bottlenose dolphin, social organization, social structure},
	pages = {223--235},
	file = {ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/QRX3NQY5/S0003347215000810.html:text/html}
}

@article{wursig_photographic_1977,
	title = {The {Photographic} {Determination} of {Group} {Size}, {Composition}, and {Stability} of {Coastal} {Porpoises} ({Tursiops} truncatus)},
	volume = {198},
	copyright = {© 1977},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/198/4318/755},
	doi = {10.1126/science.198.4318.755},
	abstract = {During a 21-month study, 53 individual bottle-nosed porpoises were recognized by photographs of their dorsal fins. They traveled in small subgroups (mean size = 15) composed of a stable core of five animals plus other individuals that varied greatly from sighting to sighting.},
	language = {en},
	number = {4318},
	urldate = {2019-08-07},
	journal = {Science},
	author = {Würsig, Bernd and Würsig, Melany},
	month = nov,
	year = {1977},
	pages = {755--756},
	file = {Snapshot:/Users/b3020111/Zotero/storage/DZ3CT76C/tab-pdf.html:text/html}
}

@inproceedings{stewman_iterative_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Iterative 3-{D} {Pose} {Correction} and {Content}-{Based} {Image} {Retrieval} for {Dorsal} {Fin} {Recognition}},
	isbn = {978-3-540-44893-8},
	abstract = {Contour or boundary descriptors may be used in content-based image retrieval to effectively identify appropriate images when image content consists primarily of a single object of interest. The registration of object contours for the purposes of comparison is complicated when the objects of interest are characterized by open contours and when reliable feature points for contour alignment are absent. We present an application that employs an iterative approach to the alignment of open contours for the purposes of image retrieval and demonstrate its success in identifying individual bottlenose dolphins from the profiles of their dorsal fins.},
	language = {en},
	booktitle = {Image {Analysis} and {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Stewman, John and Debure, Kelly and Hale, Scott and Russell, Adam},
	editor = {Campilho, Aurélio and Kamel, Mohamed S.},
	year = {2006},
	keywords = {Active Contour, Bottlenose Dolphin, Feature Point, Humpback Whale, Sperm Whale},
	pages = {648--660}
}

@article{galatius_lagenorhynchus_2016,
	title = {Lagenorhynchus albirostris ({Cetacea}: {Delphinidae})},
	volume = {48},
	issn = {0076-3519},
	shorttitle = {Lagenorhynchus albirostris ({Cetacea}},
	url = {https://academic.oup.com/mspecies/article/48/933/35/2583995},
	doi = {10.1093/mspecies/sew003},
	abstract = {Abstract.  Lagenorhynchus albirostris ( Gray, 1846a ) is a delphinid commonly called the white-beaked dolphin. A robustly built dolphin with black, white, and g},
	language = {en},
	number = {933},
	urldate = {2019-08-07},
	journal = {Mammalian Species},
	author = {Galatius, Anders and Kinze, Carl Christian},
	month = aug,
	year = {2016},
	pages = {35--47},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/ZBZNMHAX/Galatius and Kinze - 2016 - Lagenorhynchus albirostris (Cetacea Delphinidae).pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/6AIVQWB2/2583995.html:text/html}
}

@article{hammond_cetacean_2013,
	title = {Cetacean abundance and distribution in {European} {Atlantic} shelf waters to inform conservation and management},
	volume = {164},
	issn = {0006-3207},
	url = {http://www.sciencedirect.com/science/article/pii/S0006320713001055},
	doi = {https://doi.org/10.1016/j.biocon.2013.04.010},
	abstract = {The European Union (EU) Habitats Directive requires Member States to monitor and maintain at favourable conservation status those species identified to be in need of protection, including all cetaceans. In July 2005 we surveyed the entire EU Atlantic continental shelf to generate robust estimates of abundance for harbour porpoise and other cetacean species. The survey used line transect sampling methods and purpose built data collection equipment designed to minimise bias in estimates of abundance. Shipboard transects covered 19,725km in sea conditions ⩽Beaufort 4 in an area of 1,005,743km2. Aerial transects covered 15,802km in good/moderate conditions (⩽Beaufort 3) in an area of 364,371km2. Thirteen cetacean species were recorded; abundance was estimated for harbour porpoise (375,358; CV=0.197), bottlenose dolphin (16,485; CV=0.422), white-beaked dolphin (16,536; CV=0.303), short-beaked common dolphin (56,221; CV=0.234) and minke whale (18,958; CV=0.347). Abundance in 2005 was similar to that estimated in July 1994 for harbour porpoise, white-beaked dolphin and minke whale in a comparable area. However, model-based density surfaces showed a marked difference in harbour porpoise distribution between 1994 and 2005. Our results allow EU Member States to discharge their responsibilities under the Habitats Directive and inform other international organisations concerning the assessment of conservation status of cetaceans and the impact of bycatch at a large spatial scale. The lack of evidence for a change in harbour porpoise abundance in EU waters as a whole does not exclude the possibility of an impact of bycatch in some areas. Monitoring bycatch and estimation of abundance continue to be essential.},
	journal = {Biological Conservation},
	author = {Hammond, Philip S. and Macleod, Kelly and Berggren, Per and Borchers, David L. and Burt, Louise and Cañadas, Ana and Desportes, Geneviève and Donovan, Greg P. and Gilles, Anita and Gillespie, Douglas and Gordon, Jonathan and Hiby, Lex and Kuklik, Iwona and Leaper, Russell and Lehnert, Kristina and Leopold, Mardik and Lovell, Phil and Øien, Nils and Paxton, Charles G. M. and Ridoux, Vincent and Rogan, Emer and Samarra, Filipa and Scheidat, Meike and Sequeira, Marina and Siebert, Ursula and Skov, Henrik and Swift, René and Tasker, Mark L. and Teilmann, Jonas and Canneyt, Olivier Van and Vázquez, José Antonio},
	month = aug,
	year = {2013},
	keywords = {Bottlenose dolphin, Bycatch, Common dolphin, Conservation status, Habitats Directive, Harbour porpoise, Line transect sampling, Minke whale, North Sea, SCANS, White-beaked dolphin},
	pages = {107 -- 122}
}

@article{khosla_novel_2011,
	title = {Novel {Dataset} for {Fine}-{Grained} {Image} {Categorization}: {Stanford} {Dogs}},
	language = {en},
	author = {Khosla, Aditya and Jayadevaprakash, Nityananda and Yao, Bangpeng and Li, Fei-Fei},
	year = {2011},
	pages = {2},
	file = {Khosla et al. - Novel Dataset for Fine-Grained Image Categorizatio.pdf:/Users/b3020111/Zotero/storage/Z6NWN3YS/Khosla et al. - Novel Dataset for Fine-Grained Image Categorizatio.pdf:application/pdf}
}

@misc{welinder_caltech-ucsd_2010,
	type = {Report or {Paper}},
	title = {Caltech-{UCSD} {Birds} 200},
	url = {http://resolver.caltech.edu/CaltechAUTHORS:20111026-155425465},
	abstract = {Caltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated with 200 bird species. It was created to enable the study of subordinate categorization, which is not possible with other popular datasets that focus on basic level categories (such as PASCAL VOC, Caltech-101, etc). The images were downloaded from the website Flickr and filtered by workers on Amazon Mechanical Turk. Each image is annotated with a bounding box, a rough bird segmentation, and a set of attribute labels.},
	urldate = {2019-08-07},
	author = {Welinder, Peter and Branson, Steve and Mita, Takeshi and Wah, Catherine and Schroff, Florian and Belongie, Serge and Perona, Pietro},
	month = sep,
	year = {2010},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/G9FNXHGZ/Welinder et al. - 2010 - Caltech-UCSD Birds 200.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/PGPKCN6N/27468.html:text/html}
}

@misc{wah_caltech-ucsd_2011,
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset} - {CaltechAUTHORS}},
	url = {https://authors.library.caltech.edu/27452/},
	urldate = {2019-08-07},
	author = {Wah, C and Branson, S and Welinder, P and Perona, P and Belongie, S},
	year = {2011},
	file = {The Caltech-UCSD Birds-200-2011 Dataset - CaltechAUTHORS:/Users/b3020111/Zotero/storage/7DLXYRVN/27452.html:text/html}
}

@article{trotter_northumberland_2019,
	title = {The {Northumberland} {Dolphin} {Dataset}: {A} {Multimedia} {Individual} {Cetacean} {Dataset} for {Fine}-{Grained} {Categorisation}},
	copyright = {All rights reserved},
	shorttitle = {The {Northumberland} {Dolphin} {Dataset}},
	url = {http://arxiv.org/abs/1908.02669},
	abstract = {Methods for cetacean research include photo-identification (photo-id) and passive acoustic monitoring (PAM) which generate thousands of images per expedition that are currently hand categorised by researchers into the individual dolphins sighted. With the vast amount of data obtained it is crucially important to develop a system that is able to categorise this quickly. The Northumberland Dolphin Dataset (NDD) is an on-going novel dataset project made up of above and below water images of, and spectrograms of whistles from, white-beaked dolphins. These are produced by photo-id and PAM data collection methods applied off the coast of Northumberland, UK. This dataset will aid in building cetacean identification models, reducing the number of human-hours required to categorise images. Example use cases and areas identified for speed up are examined.},
	urldate = {2019-08-08},
	journal = {arXiv:1908.02669 [cs]},
	author = {Trotter, Cameron and Atkinson, Georgia and Sharpe, Matthew and McGough, A. Stephen and Wright, Nick and Berggren, Per},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.02669},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1908.02669 PDF:/Users/b3020111/Zotero/storage/THL6KSXC/Trotter et al. - 2019 - The Northumberland Dolphin Dataset A Multimedia I.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/TWQ2L5L7/1908.html:text/html}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2019-08-08},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444}
}

@incollection{hecht-nielsen_iii.3_1992,
	title = {{III}.3 - {Theory} of the {Backpropagation} {Neural} {Network}**{Based} on “nonindent” by {Robert} {Hecht}-{Nielsen}, which appeared in {Proceedings} of the {International} {Joint} {Conference} on {Neural} {Networks} 1, 593–611, {June} 1989. © 1989 {IEEE}.},
	isbn = {978-0-12-741252-8},
	url = {http://www.sciencedirect.com/science/article/pii/B9780127412528500108},
	abstract = {This chapter presents a survey of the elementary theory of the basic backpropagation neural network architecture, covering the areas of architectural design, performance measurement, function approximation capability, and learning. The survey includes a formulation of the backpropagation neural network architecture to make it a valid neural network and a proof that the backpropagation mean squared error function exists and is differentiable. Also included in the survey is a theorem showing that any L2 function can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. An appendix presents a speculative neurophysiological model illustrating the way in which the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of cerebral cortex. One of the crucial decisions in the design of the backpropagation architecture is the selection of a sigmoidal activation function.},
	urldate = {2019-08-08},
	booktitle = {Neural {Networks} for {Perception}},
	publisher = {Academic Press},
	author = {Hecht-nielsen, ROBERT},
	editor = {Wechsler, Harry},
	month = jan,
	year = {1992},
	doi = {10.1016/B978-0-12-741252-8.50010-8},
	pages = {65--93},
	file = {ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/EU4UZNPJ/B9780127412528500108.html:text/html}
}

@article{hartigan_algorithm_1979,
	title = {Algorithm {AS} 136: {A} {K}-{Means} {Clustering} {Algorithm}},
	volume = {28},
	issn = {0035-9254},
	shorttitle = {Algorithm {AS} 136},
	url = {https://www.jstor.org/stable/2346830},
	doi = {10.2307/2346830},
	number = {1},
	urldate = {2019-08-08},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Hartigan, J. A. and Wong, M. A.},
	year = {1979},
	pages = {100--108}
}

@incollection{bottou_tradeoffs_2008,
	title = {The {Tradeoffs} of {Large} {Scale} {Learning}},
	url = {http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf},
	urldate = {2019-08-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Bottou, Léon and Bousquet, Olivier},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {161--168},
	file = {NIPS Full Text PDF:/Users/b3020111/Zotero/storage/TUEX7IP8/Bottou and Bousquet - 2008 - The Tradeoffs of Large Scale Learning.pdf:application/pdf;NIPS Snapshot:/Users/b3020111/Zotero/storage/XLK98IPG/3323-the-tradeoffs-of-large-scale-learning.html:text/html}
}

@article{tieleman_lecture_2012,
	title = {Lecture 6.5-rmsprop: {Divide} the gradient by a running average of its recent magnitude},
	volume = {4},
	number = {2},
	journal = {COURSERA: Neural networks for machine learning},
	author = {Tieleman, Tijmen and Hinton, Geoffrey},
	year = {2012},
	pages = {26--31}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2019-08-08},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/Users/b3020111/Zotero/storage/B6RJQ8WY/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/7UDECC6K/1412.html:text/html}
}

@article{reddi_convergence_2019,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	url = {http://arxiv.org/abs/1904.09237},
	abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
	urldate = {2019-08-08},
	journal = {arXiv:1904.09237 [cs, math, stat]},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09237},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv\:1904.09237 PDF:/Users/b3020111/Zotero/storage/AMQ8GBCR/Reddi et al. - 2019 - On the Convergence of Adam and Beyond.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/U9SZ6EY7/1904.html:text/html}
}

@article{qian_momentum_1999,
	title = {On the momentum term in gradient descent learning algorithms},
	volume = {12},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608098001166},
	doi = {10.1016/S0893-6080(98)00116-6},
	abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
	number = {1},
	urldate = {2019-08-08},
	journal = {Neural Networks},
	author = {Qian, Ning},
	month = jan,
	year = {1999},
	keywords = {Critical damping, Damped harmonic oscillator, Gradient descent learning algorithm, Learning rate, Momentum, Speed of convergence},
	pages = {145--151},
	file = {ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/C5KJ3QL6/S0893608098001166.html:text/html}
}

@incollection{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf},
	urldate = {2019-08-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2933--2941},
	file = {NIPS Full Text PDF:/Users/b3020111/Zotero/storage/FDV754IE/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf:application/pdf;NIPS Snapshot:/Users/b3020111/Zotero/storage/EEX376Q9/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimiza.html:text/html}
}

@article{choromanska_loss_2015,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	language = {en},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gerard Ben and LeCun, Yann},
	year = {2015},
	pages = {13},
	file = {Choromanska et al. - The Loss Surfaces of Multilayer Networks.pdf:/Users/b3020111/Zotero/storage/ZLYBNI7R/Choromanska et al. - The Loss Surfaces of Multilayer Networks.pdf:application/pdf}
}

@article{alber_backprop_2018,
	title = {Backprop {Evolution}},
	url = {http://arxiv.org/abs/1808.02822},
	abstract = {The back-propagation algorithm is the cornerstone of deep learning. Despite its importance, few variations of the algorithm have been attempted. This work presents an approach to discover new variations of the back-propagation equation. We use a domain speciﬁc language to describe update equations as a list of primitive functions. An evolution-based method is used to discover new propagation rules that maximize the generalization performance after a few epochs of training. We ﬁnd several update equations that can train faster with short training times than standard back-propagation, and perform similar as standard back-propagation at convergence.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1808.02822 [cs, stat]},
	author = {Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.02822},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Alber et al. - 2018 - Backprop Evolution.pdf:/Users/b3020111/Zotero/storage/R9F77IAP/Alber et al. - 2018 - Backprop Evolution.pdf:application/pdf}
}

@article{linnainmaa_representation_1970,
	title = {The representation of the cumulative rounding error of an algorithm as a {Taylor} expansion of the local rounding errors},
	journal = {Master's Thesis (in Finnish), Univ. Helsinki},
	author = {Linnainmaa, Seppo},
	year = {1970},
	pages = {6--7}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	language = {en},
	author = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
	year = {1986},
	pages = {4},
	file = {Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:/Users/b3020111/Zotero/storage/JQKU8AGJ/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf}
}

@inproceedings{bengio_use_1994,
	title = {Use of genetic programming for the search of a new learning rule for neural networks},
	isbn = {0-7803-1899-4},
	publisher = {IEEE},
	author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn},
	year = {1994},
	pages = {324--327}
}

@article{lillicrap_random_2014,
	title = {Random feedback weights support learning in deep neural networks},
	url = {http://arxiv.org/abs/1411.0247},
	abstract = {The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
	urldate = {2019-08-08},
	journal = {arXiv:1411.0247 [cs, q-bio]},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.0247},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv\:1411.0247 PDF:/Users/b3020111/Zotero/storage/G42ZBI5X/Lillicrap et al. - 2014 - Random feedback weights support learning in deep n.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/9JX7GZX9/1411.html:text/html}
}

@inproceedings{nokland_direct_2016,
	title = {Direct feedback alignment provides learning in deep neural networks},
	author = {Nøkland, Arild},
	year = {2016},
	pages = {1037--1045}
}

@inproceedings{liao_how_2016,
	title = {How important is weight symmetry in backpropagation?},
	author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
	year = {2016}
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2019-08-08},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01852},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1502.01852 PDF:/Users/b3020111/Zotero/storage/X5CNZA7E/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/FU2Y55LH/1502.html:text/html}
}

@article{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2019-08-08},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1703.06870 PDF:/Users/b3020111/Zotero/storage/N7QKA2DD/He et al. - 2017 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ZZY6ZAVB/1703.html:text/html}
}

@misc{waleed_mask_2017,
	title = {Mask {R}-{CNN} for object detection and instance segmentation on {Keras} and {TensorFlow}: matterport/{Mask}\_RCNN},
	copyright = {View license},
	shorttitle = {Mask {R}-{CNN} for object detection and instance segmentation on {Keras} and {TensorFlow}},
	url = {https://github.com/matterport/Mask_RCNN},
	urldate = {2019-08-08},
	publisher = {Matterport, Inc},
	author = {Waleed, Abdulla},
	year = {2017},
	note = {original-date: 2017-10-19T20:28:34Z}
}

@article{long_fully_2014,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1411.4038 [cs]},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4038},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf:/Users/b3020111/Zotero/storage/B4VVGTY3/Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf}
}

@article{badrinarayanan_segnet:_2015,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:/Users/b3020111/Zotero/storage/QI9XLE8U/Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf}
}

@article{chen_semantic_2014,
	title = {Semantic {Image} {Segmentation} with {Deep} {Convolutional} {Nets} and {Fully} {Connected} {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classiﬁcation and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classiﬁcation (also called ”semantic image segmentation”). We show that responses at the ﬁnal layer of DCNNs are not sufﬁciently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efﬁciently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1412.7062 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf:/Users/b3020111/Zotero/storage/E4EEQ3ZR/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf:application/pdf}
}

@article{hariharan_simultaneous_2014,
	title = {Simultaneous {Detection} and {Segmentation}},
	url = {http://arxiv.org/abs/1407.1808},
	abstract = {We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-speciﬁc, topdown ﬁgure-ground predictions to reﬁne our bottom-up proposals. We show a 7 point boost (16\% relative) over our baselines on SDS, a 5 point boost (10\% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1407.1808 [cs]},
	author = {Hariharan, Bharath and Arbeláez, Pablo and Girshick, Ross and Malik, Jitendra},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.1808},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hariharan et al. - 2014 - Simultaneous Detection and Segmentation.pdf:/Users/b3020111/Zotero/storage/ZVZGIU2G/Hariharan et al. - 2014 - Simultaneous Detection and Segmentation.pdf:application/pdf}
}

@article{buhrmester_amazons_2011,
	title = {Amazon's {Mechanical} {Turk}: {A} new source of inexpensive, yet high-quality, data?},
	volume = {6},
	issn = {1745-6916},
	number = {1},
	journal = {Perspectives on psychological science},
	author = {Buhrmester, Michael and Kwang, Tracy and Gosling, Samuel D},
	year = {2011},
	pages = {3--5}
}

@inproceedings{lee_difference_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Difference {Target} {Propagation}},
	isbn = {978-3-319-23528-8},
	abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	editor = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, Vítor and Soares, Carlos and Gama, João and Jorge, Alípio},
	year = {2015},
	keywords = {Deep Neural Network, Hide Layer, Hide Unit, Target Propagation, Test Error},
	pages = {498--515},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/25S9ZBW5/Lee et al. - 2015 - Difference Target Propagation.pdf:application/pdf}
}

@article{dietterich_overfitting_1995,
	title = {Overfitting and undercomputing in machine learning},
	volume = {27},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=212094.212114},
	doi = {10.1145/212094.212114},
	number = {3},
	urldate = {2019-08-12},
	journal = {ACM Computing Surveys},
	author = {Dietterich, Tom},
	month = sep,
	year = {1995},
	pages = {326--327}
}

@article{fu_retinamask:_2019,
	title = {{RetinaMask}: {Learning} to predict masks improves state-of-the-art single-shot detection for free},
	journal = {arXiv preprint arXiv:1901.03353},
	author = {Fu, Cheng-Yang and Shvets, Mykhailo and Berg, Alexander C},
	year = {2019}
}

@article{van_aswegen_morphological_2019,
	title = {Morphological differences between coastal bottlenose dolphin ({Tursiops} aduncus) populations identified using non-invasive stereo-laser photogrammetry},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-019-48419-3},
	doi = {10.1038/s41598-019-48419-3},
	language = {en},
	number = {1},
	urldate = {2019-08-27},
	journal = {Scientific Reports},
	author = {van Aswegen, Martin and Christiansen, Fredrik and Symons, John and Mann, Janet and Nicholson, Krista and Sprogis, Kate and Bejder, Lars},
	month = dec,
	year = {2019},
	pages = {12235}
}

@article{hinton_discovering_2011,
	title = {Discovering {Binary} {Codes} for {Documents} by {Learning} {Deep} {Generative} {Models}: {Topics} in {Cognitive} {Science}(2010)},
	volume = {3},
	issn = {17568757},
	shorttitle = {Discovering {Binary} {Codes} for {Documents} by {Learning} {Deep} {Generative} {Models}},
	url = {http://doi.wiley.com/10.1111/j.1756-8765.2010.01109.x},
	doi = {10.1111/j.1756-8765.2010.01109.x},
	language = {en},
	number = {1},
	urldate = {2019-08-30},
	journal = {Topics in Cognitive Science},
	author = {Hinton, Geoffrey and Salakhutdinov, Ruslan},
	month = jan,
	year = {2011},
	pages = {74--91}
}

@article{hinton_optimal_1983,
	title = {Optimal {Perceptual} {Inference}},
	language = {en},
	author = {Hinton, G E and Sejnowski, T J},
	year = {1983},
	pages = {6},
	file = {Hinton and Sejnowski - Optimal Perceptual Inference.pdf:/Users/b3020111/Zotero/storage/QTT5L8R4/Hinton and Sejnowski - Optimal Perceptual Inference.pdf:application/pdf}
}

@article{xiao_fashion-mnist:_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	url = {http://arxiv.org/abs/1708.07747},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2019-11-30},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/VBYDQTWM/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/KEE89UXE/1708.html:text/html}
}

@inproceedings{akkaynak_sea-thru_2019,
	address = {Long Beach, CA, USA},
	title = {Sea-{Thru}: {A} {Method} for {Removing} {Water} {From} {Underwater} {Images}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Sea-{Thru}},
	url = {https://ieeexplore.ieee.org/document/8954437/},
	doi = {10.1109/CVPR.2019.00178},
	abstract = {Robust recovery of lost colors in underwater images remains a challenging problem. We recently showed that this was partly due to the prevalent use of an atmospheric image formation model for underwater images and proposed a physically accurate model. The revised model showed: 1) the attenuation coefﬁcient of the signal is not uniform across the scene but depends on object range and reﬂectance, 2) the coefﬁcient governing the increase in backscatter with distance differs from the signal attenuation coefﬁcient. Here, we present the ﬁrst method that recovers color with our revised model, using RGBD images. The Sea-thru method estimates backscatter using the dark pixels and their known range information. Then, it uses an estimate of the spatially varying illuminant to obtain the range-dependent attenuation coefﬁcient. Using more than 1,100 images from two optically different water bodies, which we make available, we show that our method with the revised model outperforms those using the atmospheric model. Consistent removal of water will open up large underwater datasets to powerful computer vision and machine learning algorithms, creating exciting opportunities for the future of underwater exploration and conservation.},
	language = {en},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Akkaynak, Derya and Treibitz, Tali},
	month = jun,
	year = {2019},
	pages = {1682--1691},
	file = {Akkaynak and Treibitz - 2019 - Sea-Thru A Method for Removing Water From Underwa.pdf:/Users/b3020111/Zotero/storage/3XD6HGA4/Akkaynak and Treibitz - 2019 - Sea-Thru A Method for Removing Water From Underwa.pdf:application/pdf}
}

@inproceedings{hillman_finscan_2002,
	title = {"{Finscan}", a computer system for photographic identification of marine animals},
	volume = {2},
	doi = {10.1109/IEMBS.2002.1106279},
	abstract = {A system has been developed for computer-assisted photographic identification of marine animals. The system creates and maintains a database of images of dorsal fins or flukes, and the user queries it by entering a new image acquired in the field. The system searches the database for similar images based on the notching pattern of the dorsal fin or fluke, and offers a list of database members ordered by similarity to the query image. A syntactic/semantic string representation method is used for matching and is compared with matching by direct alignment of edge patterns. The system has been tested with data sets of images of several biological species.},
	booktitle = {Proceedings of the {Second} {Joint} 24th {Annual} {Conference} and the {Annual} {Fall} {Meeting} of the {Biomedical} {Engineering} {Society}] [{Engineering} in {Medicine} and {Biology}},
	author = {Hillman, G.R. and Kehtarnavaz, N. and Wursig, B. and Araabi, B. and Gailey, G. and Weller, D. and Mandava, S. and Tagare, H.},
	month = oct,
	year = {2002},
	note = {ISSN: 1094-687X},
	keywords = {Biomedical imaging, Catalogs, computer-assisted photographic identification, Dolphins, dorsal fins, Finscan, flukes, image database, Image databases, image recognition, marine animals, Marine animals, Medical diagnostic imaging, notching pattern, Pattern matching, pattern recognition, photographic identification, string matching, syntactic/semantic string representation method, Testing, Tin, visual databases, Whales, zoology},
	pages = {1065--1066 vol.2},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/Z7F868QQ/1106279.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/MGKTF6VT/Hillman et al. - 2002 - Finscan, a computer system for photographic iden.pdf:application/pdf}
}

@article{weinstein_computer_2018,
	title = {A computer vision for animal ecology},
	volume = {87},
	copyright = {© 2017 The Author. Journal of Animal Ecology © 2017 British Ecological Society},
	issn = {1365-2656},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2656.12780},
	doi = {10.1111/1365-2656.12780},
	abstract = {A central goal of animal ecology is to observe species in the natural world. The cost and challenge of data collection often limit the breadth and scope of ecological study. Ecologists often use image capture to bolster data collection in time and space. However, the ability to process these images remains a bottleneck. Computer vision can greatly increase the efficiency, repeatability and accuracy of image review. Computer vision uses image features, such as colour, shape and texture to infer image content. I provide a brief primer on ecological computer vision to outline its goals, tools and applications to animal ecology. I reviewed 187 existing applications of computer vision and divided articles into ecological description, counting and identity tasks. I discuss recommendations for enhancing the collaboration between ecologists and computer scientists and highlight areas for future growth of automated image analysis.},
	language = {en},
	number = {3},
	urldate = {2020-02-12},
	journal = {Journal of Animal Ecology},
	author = {Weinstein, Ben G.},
	year = {2018},
	keywords = {automation, camera traps, ecological monitoring, images, unmanned aerial vehicles},
	pages = {533--545},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/GZY4PVE6/Weinstein - 2018 - A computer vision for animal ecology.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/WNXSDRJR/1365-2656.html:text/html}
}

@misc{gilman_computer-assisted_2016,
	title = {Computer-assisted recognition of dolphin individuals using dorsal fin pigmentations - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/7804460},
	urldate = {2020-02-12},
	author = {Gilman, Andrew and Hupman, Krista and Stockin, Karen and Pawley, Matthew D M},
	year = {2016},
	file = {Computer-assisted recognition of dolphin individuals using dorsal fin pigmentations - IEEE Conference Publication:/Users/b3020111/Zotero/storage/RRFACPVA/7804460.html:text/html}
}

@inproceedings{bouma_individual_2018-1,
	title = {Individual {Common} {Dolphin} {Identification} {Via} {Metric} {Embedding} {Learning}},
	doi = {10.1109/IVCNZ.2018.8634778},
	abstract = {Photo-identification (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin fin photograph captured in the field to a catalogue of known individuals. We examine this problem in the context of open-set recognition and utilise a triplet loss function to learn a compact representation of fin images in a Euclidean embedding, where the Euclidean distance metric represents fin similarity. We show that this compact representation can be successfully learnt from a fairly small (in deep learning context) training set and still generalise well to out-of-sample identities (completely new dolphin individuals), with top-1 and top-5 test set (37 individuals) accuracy of 90.5 ± 2 and 93.6 ± 1 percent. In the presence of 1200 distractors, top-1 accuracy dropped by 12\%; however, top-5 accuracy saw only a 2.8\% drop.},
	booktitle = {2018 {International} {Conference} on {Image} and {Vision} {Computing} {New} {Zealand} ({IVCNZ})},
	author = {Bouma, Soren and Pawley, Matthew D.M and Hupman, Krista and Gilman, Andrew},
	month = nov,
	year = {2018},
	note = {ISSN: 2151-2191},
	keywords = {compact representation, deep learning context, dolphin identification via metric embedding learning, dolphin individuals, Dolphins, ecological sciences, Euclidean distance metric, Euclidean embedding, fin similarity, image capture, image matching, image recognition, image registration, image segmentation, known individuals, laborious manual process, learning (artificial intelligence), Measurement, open-set recognition, photo-identification, pose estimation, social structure, Sociology, Statistics, Task analysis, top-5 test set accuracy, Training, Training data, triplet loss function},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/3WWIAWWK/8634778.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/3YYHZJAM/Bouma et al. - 2018 - Individual Common Dolphin Identification Via Metri.pdf:application/pdf}
}

@misc{haimeh_haimehfinfindr_2020,
	title = {haimeh/{finFindR}},
	url = {https://github.com/haimeh/finFindR},
	abstract = {An application for dorsal fin image recognition and cataloguing},
	urldate = {2020-02-12},
	author = {haimeh},
	month = feb,
	year = {2020},
	note = {original-date: 2017-10-22T05:19:57Z},
	keywords = {app, cpp, dolphin, r, recognition}
}

@misc{noauthor_hierarchical_nodate,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}: {Journal} of the {American} {Statistical} {Association}: {Vol} 58, {No} 301},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
	urldate = {2020-02-12},
	file = {Hierarchical Grouping to Optimize an Objective Function\: Journal of the American Statistical Association\: Vol 58, No 301:/Users/b3020111/Zotero/storage/UI4B2BBW/01621459.1963.html:text/html}
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} {Unified} {Embedding} for {Face} {Recognition} and {Clustering}},
	shorttitle = {{FaceNet}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
	urldate = {2020-02-12},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	pages = {815--823},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/RWHINAPE/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/X3ESPCJN/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html:text/html}
}

@misc{noauthor_wildbookorgibeis-curvrank-module_2020,
	title = {{WildbookOrg}/ibeis-curvrank-module},
	copyright = {Apache-2.0},
	url = {https://github.com/WildbookOrg/ibeis-curvrank-module},
	abstract = {Python module that wraps https://github.com/hjweide/dolphin-identification},
	urldate = {2020-02-12},
	publisher = {Wildbook Organization},
	month = jan,
	year = {2020},
	note = {original-date: 2018-03-14T16:49:31Z}
}

@article{weideman_integral_2017,
	title = {Integral {Curvature} {Representation} and {Matching} {Algorithms} for {Identification} of {Dolphins} and {Whales}},
	url = {http://arxiv.org/abs/1708.07785},
	abstract = {We address the problem of identifying individual cetaceans from images showing the trailing edge of their fins. Given the trailing edge from an unknown individual, we produce a ranking of known individuals from a database. The nicks and notches along the trailing edge define an individual's unique signature. We define a representation based on integral curvature that is robust to changes in viewpoint and pose, and captures the pattern of nicks and notches in a local neighborhood at multiple scales. We explore two ranking methods that use this representation. The first uses a dynamic programming time-warping algorithm to align two representations, and interprets the alignment cost as a measure of similarity. This algorithm also exploits learned spatial weights to downweight matches from regions of unstable curvature. The second interprets the representation as a feature descriptor. Feature keypoints are defined at the local extrema of the representation. Descriptors for the set of known individuals are stored in a tree structure, which allows us to perform queries given the descriptors from an unknown trailing edge. We evaluate the top-k accuracy on two real-world datasets to demonstrate the effectiveness of the curvature representation, achieving top-1 accuracy scores of approximately 95\% and 80\% for bottlenose dolphins and humpback whales, respectively.},
	urldate = {2020-02-12},
	journal = {arXiv:1708.07785 [cs]},
	author = {Weideman, Hendrik J. and Jablons, Zachary M. and Holmberg, Jason and Flynn, Kiirsten and Calambokidis, John and Tyson, Reny B. and Allen, Jason B. and Wells, Randall S. and Hupman, Krista and Urian, Kim and Stewart, Charles V.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07785},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/C6JAW9XJ/Weideman et al. - 2017 - Integral Curvature Representation and Matching Alg.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/8E3NGLCW/1708.html:text/html}
}

@misc{mann_mann-urian-google_2019,
	title = {Mann-{Urian}-{Google} {Cloud} {AI}.pdf},
	url = {https://drive.google.com/file/d/1nLXJiSjGCgvjW54UTd46EU64oJcKS1f_/view?usp=sharing&usp=embed_facebook},
	urldate = {2020-02-12},
	journal = {Google Docs},
	author = {Mann, Janet},
	year = {2019},
	file = {Snapshot:/Users/b3020111/Zotero/storage/UHEITJLN/view.html:text/html}
}

@misc{liang_googles_2018,
	title = {Google's {AI} {Helps} {Researcher} {ID} {Dolphins}},
	url = {https://www.deeperblue.com/googles-ai-helps-researcher-id-dolphins/},
	abstract = {Google's artificial intelligence engineers are collaborating with a university researcher to identify dolphins in the wild.},
	language = {en-US},
	urldate = {2020-02-12},
	journal = {DeeperBlue.com},
	author = {Liang, John},
	month = oct,
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/GJ9FC95C/googles-ai-helps-researcher-id-dolphins.html:text/html}
}

@misc{georgetown_university_is_2018,
	title = {Is {That} ‘{Jimmy} {Carter}’ or ‘{Barbara} {Bush}?’ {Google} {Designs}, {Professor} {Uses} {Artificial} {Intelligence} to {Track} {Wildlife}},
	shorttitle = {Is {That} ‘{Jimmy} {Carter}’ or ‘{Barbara} {Bush}?},
	url = {https://www.georgetown.edu/news/is-that-jimmy-carter-or-barbara-bush-google-designs-professor-uses-artificial-intelligence-to-track-wildlife/},
	abstract = {Janet Mann, professor of biology and psychology, collaborates with Google’s artificial intelligence engineers on individual identification of wild dolphins through images.},
	language = {en-US},
	urldate = {2020-02-12},
	journal = {Georgetown University},
	author = {Georgetown University},
	month = oct,
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/NB4JSIJQ/is-that-jimmy-carter-or-barbara-bush-google-designs-professor-uses-artificial-intelligence-to-t.html:text/html}
}

@misc{kaggle_humpback_2018,
	title = {Humpback {Whale} {Identification} {Challenge}},
	url = {https://kaggle.com/c/whale-categorization-playground},
	abstract = {Can you identify a whale by the picture of its fluke?},
	language = {en},
	urldate = {2020-02-12},
	author = {Kaggle},
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/PNAXMZVK/whale-categorization-playground.html:text/html}
}

@article{deng_arcface_2019,
	title = {{ArcFace}: {Additive} {Angular} {Margin} {Loss} for {Deep} {Face} {Recognition}},
	shorttitle = {{ArcFace}},
	url = {http://arxiv.org/abs/1801.07698},
	abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
	urldate = {2020-02-12},
	journal = {arXiv:1801.07698 [cs]},
	author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	month = feb,
	year = {2019},
	note = {arXiv: 1801.07698},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/IRNLNN3A/Deng et al. - 2019 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/5UJNLEDI/1801.html:text/html}
}

@article{deng_arcface_2019-1,
	title = {{ArcFace}: {Additive} {Angular} {Margin} {Loss} for {Deep} {Face} {Recognition}},
	shorttitle = {{ArcFace}},
	url = {http://arxiv.org/abs/1801.07698},
	abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
	urldate = {2020-02-12},
	journal = {arXiv:1801.07698 [cs]},
	author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	month = feb,
	year = {2019},
	note = {arXiv: 1801.07698},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/H327VFAM/Deng et al. - 2019 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/PW7Z6QVH/1801.html:text/html}
}

@misc{taigman_deepface_2014,
	title = {{DeepFace}: {Closing} the {Gap} to {Human}-{Level} {Performance} in {Face} {Verification}},
	shorttitle = {{DeepFace}},
	url = {https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/},
	abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing exp...},
	language = {en-US},
	urldate = {2020-02-12},
	journal = {Facebook Research},
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year = {2014},
	file = {Snapshot:/Users/b3020111/Zotero/storage/2QH2BWPA/deepface-closing-the-gap-to-human-level-performance-in-face-verification.html:text/html}
}

@misc{cheeseman_ted_2019,
	title = {Ted {Cheeseman} {Happywhale} presentation-{WMMC}-{Rise} of the {Machines}.pdf},
	url = {https://drive.google.com/file/d/1yRtNKagANZOgpY6852zzUpZmiFXuZmLM/view?usp=sharing&usp=embed_facebook},
	urldate = {2020-02-12},
	journal = {Google Docs},
	author = {Cheeseman, Ted},
	year = {2019},
	file = {Snapshot:/Users/b3020111/Zotero/storage/E6KJSPX9/view.html:text/html}
}

@misc{fisheries_finbase_2018,
	title = {{FinBase} {Photo}-{Identification} {Database} {System} {\textbar} {NOAA} {Fisheries}},
	url = {https://www.fisheries.noaa.gov/national/marine-mammal-protection/finbase-photo-identification-database-system},
	abstract = {A database system consists of a collection of subdirectories (associated with image and file storage) and front- and back-end Microsoft Access databases},
	language = {en},
	urldate = {2020-02-12},
	journal = {NOAA},
	author = {Fisheries, NOAA},
	month = feb,
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/9JNLCLAR/finbase-photo-identification-database-system.html:text/html}
}

@misc{thompson_finfindrpdf_2019,
	title = {{finFindR}.pdf},
	url = {https://drive.google.com/file/d/1jgbnAH2_C0DoUe8HidJfP13yfhtqz2gB/view?usp=sharing&usp=embed_facebook},
	urldate = {2020-02-12},
	journal = {Google Docs},
	author = {Thompson, Jaime},
	year = {2019},
	file = {Snapshot:/Users/b3020111/Zotero/storage/87LSWBKE/view.html:text/html}
}

@article{ward_hierarchical_1963,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}},
	volume = {58},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
	doi = {10.1080/01621459.1963.10500845},
	abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {\textgreater} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n − 1 mutually exclusive sets by considering the union of all possible n(n − 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
	number = {301},
	urldate = {2020-02-12},
	journal = {Journal of the American Statistical Association},
	author = {Ward, Joe H.},
	month = mar,
	year = {1963},
	pages = {236--244},
	file = {Snapshot:/Users/b3020111/Zotero/storage/FC6PSAID/01621459.1963.html:text/html}
}

@article{hale_unsupervised_2012,
	title = {Unsupervised {Threshold} for {Automatic} {Extraction} of {Dolphin} {Dorsal} {Fin} {Outlines} from {Digital} {Photographs} in {DARWIN} ({Digital} {Analysis} and {Recognition} of {Whale} {Images} on a {Network})},
	url = {http://arxiv.org/abs/1202.4107},
	abstract = {At least two software packages---DARWIN, Eckerd College, and FinScan, Texas A\&M---exist to facilitate the identification of cetaceans---whales, dolphins, porpoises---based upon the naturally occurring features along the edges of their dorsal fins. Such identification is useful for biological studies of population, social interaction, migration, etc. The process whereby fin outlines are extracted in current fin-recognition software packages is manually intensive and represents a major user input bottleneck: it is both time consuming and visually fatiguing. This research aims to develop automated methods (employing unsupervised thresholding and morphological processing techniques) to extract cetacean dorsal fin outlines from digital photographs thereby reducing manual user input. Ideally, automatic outline generation will improve the overall user experience and improve the ability of the software to correctly identify cetaceans. Various transformations from color to gray space were examined to determine which produced a grayscale image in which a suitable threshold could be easily identified. To assist with unsupervised thresholding, a new metric was developed to evaluate the jaggedness of figures ("pixelarity") in an image after thresholding. The metric indicates how cleanly a threshold segments background and foreground elements and hence provides a good measure of the quality of a given threshold. This research results in successful extractions in roughly 93\% of images, and significantly reduces user-input time.},
	urldate = {2020-02-12},
	journal = {arXiv:1202.4107 [cs]},
	author = {Hale, Scott A.},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.4107},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, H.5.2, I.4.6},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/LN3QTPEQ/Hale - 2012 - Unsupervised Threshold for Automatic Extraction of.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/HS82CUNS/1202.html:text/html}
}