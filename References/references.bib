
@article{urian_recommendations_2015,
	title = {Recommendations for photo-identification methods used in capture-recapture models with cetaceans},
	volume = {31},
	issn = {08240469},
	url = {http://doi.wiley.com/10.1111/mms.12141},
	doi = {10.1111/mms.12141},
	language = {en},
	number = {1},
	urldate = {2019-01-08},
	journal = {Marine Mammal Science},
	author = {Urian, Kim and Gorgone, Antoinette and Read, Andrew and Balmer, Brian and Wells, Randall S. and Berggren, Per and Durban, John and Eguchi, Tomoharu and Rayment, William and Hammond, Philip S.},
	month = jan,
	year = {2015},
	keywords = {photo-id, cetaceans, dolphins},
	pages = {298--321},
	file = {Submitted Version:/Users/b3020111/Zotero/storage/WWTMTPAX/Urian et al. - 2015 - Recommendations for photo-identification methods u.pdf:application/pdf}
}

@article{van_bressem_visual_2018,
	title = {Visual health assessment of white-beaked dolphins off the coast of {Northumberland}, {North} {Sea}, using underwater photography},
	volume = {34},
	issn = {08240469},
	url = {http://doi.wiley.com/10.1111/mms.12501},
	doi = {10.1111/mms.12501},
	language = {en},
	number = {4},
	urldate = {2019-01-08},
	journal = {Marine Mammal Science},
	author = {Van Bressem, Marie-Françoise and Burville, Ben and Sharpe, Matt and Berggren, Per and Van Waerebeek, Koen},
	month = oct,
	year = {2018},
	keywords = {underwater, health},
	pages = {1119--1133},
	file = {Submitted Version:/Users/b3020111/Zotero/storage/XAEYFGHB/Van Bressem et al. - 2018 - Visual health assessment of white-beaked dolphins .pdf:application/pdf}
}

@inproceedings{quinonez_using_2019,
	title = {Using {Convolutional} {Neural} {Networks} to {Recognition} of {Dolphin} {Images}},
	isbn = {978-3-030-01171-0},
	abstract = {Classification of specific objects through Convolutional Neural Networks (CNN) has become an interesting research line in the area from information processing and machine learning, main idea is training a image dataset to perform the classifying a given pattern. In this work, a new dataset with 2504 images was introduced, the method used to train the networks was transfer learning to recognition of dolphin images. For this purpose, two models were used: Inception V3 and Inception ResNet V2 to train on TensorFlow platform with different images, corresponding to the four main classes: dolphin, dolphin\_pod, open\_sea, and seabirds. The paper ends with a critical discussion of the experimental results.},
	booktitle = {Trends and {Applications} in {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Quiñonez, Yadira and Zatarain, Oscar and Lizarraga, Carmen and Peraza, Juan},
	editor = {Mejia, Jezreel and Muñoz, Mirna and Rocha, Alvaro and Peña, Adriana and Pérez-Cisneros, Marco},
	year = {2019},
	pages = {236--245},
	file = {Quiñonez et al. - 2019 - Using Convolutional Neural Networks to Recognition.pdf:/Users/b3020111/Zotero/storage/R54YVDT3/Quiñonez et al. - 2019 - Using Convolutional Neural Networks to Recognition.pdf:application/pdf}
}

@inproceedings{bouma_individual_2018,
	title = {Individual common dolphin identification via metric embedding learning},
	abstract = {Photo-identiﬁcation (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin ﬁn photograph captured in the ﬁeld to a catalogue of known individuals.},
	language = {en},
	author = {Bouma, Soren and Pawley, Matthew D M and Hupman, Krista and Gilman, Andrew},
	year = {2018},
	pages = {7},
	file = {Bouma et al. - Individual common dolphin identiﬁcation via metric.pdf:/Users/b3020111/Dropbox/University/PhD/Data and Cetacians/Bouma et al. - Individual common dolphin identiﬁcation via metric.pdf:application/pdf}
}

@article{berger-wolf_wildbook:_2017,
	title = {Wildbook: {Crowdsourcing}, computer vision, and data science for conservation},
	shorttitle = {Wildbook},
	url = {http://arxiv.org/abs/1710.08880},
	abstract = {Photographs, taken by field scientists, tourists, automated cameras, and incidental photographers, are the most abundant source of data on wildlife today. Wildbook is an autonomous computational system that starts from massive collections of images and, by detecting various species of animals and identifying individuals, combined with sophisticated data management, turns them into high resolution information database, enabling scientific inquiry, conservation, and citizen science. We have built Wildbooks for whales (flukebook.org), sharks (whaleshark.org), two species of zebras (Grevy's and plains), and several others. In January 2016, Wildbook enabled the first ever full species (the endangered Grevy's zebra) census using photographs taken by ordinary citizens in Kenya. The resulting numbers are now the official species census used by IUCN Red List: http://www.iucnredlist.org/details/7950/0. In 2016, Wildbook partnered up with WWF to build Wildbook for Sea Turtles, Internet of Turtles (IoT), as well as systems for seals and lynx. Most recently, we have demonstrated that we can now use publicly available social media images to count and track wild animals. In this paper we present and discuss both the impact and challenges that the use of crowdsourced images can have on wildlife conservation.},
	urldate = {2019-01-08},
	journal = {arXiv:1710.08880 [cs]},
	author = {Berger-Wolf, Tanya Y. and Rubenstein, Daniel I. and Stewart, Charles V. and Holmberg, Jason A. and Parham, Jason and Menon, Sreejith and Crall, Jonathan and Van Oast, Jon and Kiciman, Emre and Joppa, Lucas},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.08880},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv\:1710.08880 PDF:/Users/b3020111/Zotero/storage/SNQUXM4C/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf:application/pdf;arXiv\:1710.08880 PDF:/Users/b3020111/Zotero/storage/7XVBYQTL/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/GTMS4JR9/1710.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/UG5YRIIF/1710.html:text/html}
}

@inproceedings{truskinger_visualizing_2018,
	address = {Amsterdam, Netherlands},
	title = {Visualizing five decades of environmental acoustic data},
	url = {https://eprints.qut.edu.au/122730/},
	abstract = {Monitoring the environment with acoustic sensors is now practical; sensors are sold as commercial devices, storage is cheap, and the field of ecoacoustics is recognized as an effective way to scale monitoring of the environment. However, a pressing challenge faced in many eScience projects is how to manage, analyze, and visualize very large data so that scientists can benefit, with ecoacoustic data presenting its own particular challenges.     This paper presents a new zoomable interactive visualization interface for the exploration of environmental audio data. The interface is a new tool in the Acoustic Workbench, an ecoacoustics software platform built for managing environmental audio data. This Google Maps like interface for audio data, enables zooming in and out of audio data by incorporating specialized, multiresolution, visual representations of audio data into the workbench website. The ‘zooming’ visualization allows scientists to surface the structure, detail, and patterns in content that would otherwise be opaque to them, from scales of seconds through to weeks of data. The Ecosounds instance of the Acoustic Workbench contains 52 years (108 TB) of audio data, from 1016 locations, which results in a 180 million-tile, 8.3 terapixel visualization.   The design and implementation of this novel big audio data visualization is presented along with some design considerations for storing visualization tiles.},
	booktitle = {14th {eScience} {IEEE} {International} {Conference}},
	publisher = {IEEE  Computer Society},
	author = {Truskinger, Anthony and Brereton, Margot and Roe, Paul},
	year = {2018},
	keywords = {Application file formats}
}

@inproceedings{roe_catching_2018,
	title = {Catching {Toad} {Calls} in the {Cloud}: {Commodity} {Edge} {Computing} for {Flexible} {Analysis} of {Big} {Sound} {Data}},
	isbn = {1-5386-9156-6},
	booktitle = {2018 {IEEE} 14th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Roe, Paul and Ferroudj, Meriem and Towsey, Michael and Schwarzkopf, Lin},
	year = {2018},
	pages = {67--74}
}

@article{oswald_tool_2007,
	title = {A tool for real-time acoustic species identification of delphinid whistles},
	volume = {122},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2743157},
	doi = {10.1121/1.2743157},
	language = {en},
	number = {1},
	urldate = {2019-01-08},
	journal = {The Journal of the Acoustical Society of America},
	author = {Oswald, Julie N. and Rankin, Shannon and Barlow, Jay and Lammers, Marc O.},
	month = jul,
	year = {2007},
	pages = {587--595}
}

@article{pan_survey_2010,
	title = {A survey on transfer learning},
	volume = {22},
	number = {10},
	journal = {IEEE Transactions on knowledge and data engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	pages = {1345--1359}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2019-01-08},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, resnet50},
	file = {arXiv\:1512.03385 PDF:/Users/b3020111/Zotero/storage/U79QHJ2E/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv\:1512.03385 PDF:/Users/b3020111/Zotero/storage/7STEMDJW/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/RMK4EIHR/1512.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/QE5S4JJE/1512.html:text/html;He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/Users/b3020111/Zotero/storage/AMDEJUSK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2019-01-08},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.01497 PDF:/Users/b3020111/Zotero/storage/8UESWG2A/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/MXAH4FRB/1506.html:text/html}
}

@article{liu_ssd:_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2019-01-08},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
	file = {arXiv\:1512.02325 PDF:/Users/b3020111/Zotero/storage/W27GTLB9/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ISCZ9YLM/1512.html:text/html}
}

@article{huang_speed/accuracy_2016,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	url = {http://arxiv.org/abs/1611.10012},
	abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
	urldate = {2019-01-08},
	journal = {arXiv:1611.10012 [cs]},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.10012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.10012 PDF:/Users/b3020111/Zotero/storage/TPUHAW9Q/Huang et al. - 2016 - Speedaccuracy trade-offs for modern convolutional.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/6PSRKK4Z/1611.html:text/html}
}

@inproceedings{lim_automated_2018,
	title = {Automated {Interpretation} of {Seafloor} {Visual} {Maps} {Obtained} {Using} {Underwater} {Robots}},
	isbn = {1-5386-1654-8},
	booktitle = {2018 {OCEANS}-{MTS}/{IEEE} {Kobe} {Techno}-{Oceans} ({OTO})},
	publisher = {IEEE},
	author = {Lim, Jin Wei and Prügel-Bennett, Adam and Thornton, Blair},
	year = {2018},
	pages = {1--8}
}

@inproceedings{abadi_tensorflow:_2016,
	title = {Tensorflow: a system for large-scale machine learning.},
	volume = {16},
	booktitle = {{OSDI}},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael},
	year = {2016},
	pages = {265--283}
}

@inproceedings{wu_mobile_nodate,
	title = {A {Mobile} {Application} for {Dog} {Breed} {Detection} and {Recognition} based on {Deep} {Learning}},
	abstract = {Deep learning provides the ability to train algorithms (models) that can tackle the problems of data classification and prediction based on deriving (learning) knowledge from raw data. Convolutional Neural Networks (CNNs) provides one commonly used approach for image classification and detection. In this work we describe a CNN-based method for detecting dogs in potentially complex images and subsequently consider the identification of the type/breed of dogs. The results achieve nearly 85\% accuracy for breed classification for a set of 50 classes of dogs and 64\% accuracy for 120 other less common dog types. An iOS application and associated big data processing infrastructure utilizing a variety of GPUs was used to support the image classification algorithms.},
	language = {en},
	author = {Wu, Fang and Chen, Wenbin and Sinnott, Richard O},
	pages = {10},
	file = {Wu et al. - A Mobile Application for Dog Breed Detection and R.pdf:/Users/b3020111/Dropbox/University/PhD/Data and Cetacians/Wu et al. - A Mobile Application for Dog Breed Detection and R.pdf:application/pdf}
}

@article{elsken_neural_2018,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	shorttitle = {Neural {Architecture} {Search}},
	url = {http://arxiv.org/abs/1808.05377},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	urldate = {2019-01-08},
	journal = {arXiv:1808.5377 null},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.5377},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1808.5377 PDF:/Users/b3020111/Zotero/storage/U7VMLSNG/Elsken et al. - 2018 - Neural Architecture Search A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/753P6DD5/1808.html:text/html}
}

@article{breuel_effects_2015,
	title = {The {Effects} of {Hyperparameters} on {SGD} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1508.02788},
	abstract = {The performance of neural network classifiers is determined by a number of hyperparameters, including learning rate, batch size, and depth. A number of attempts have been made to explore these parameters in the literature, and at times, to develop methods for optimizing them. However, exploration of parameter spaces has often been limited. In this note, I report the results of large scale experiments exploring these different parameters and their interactions.},
	urldate = {2019-01-08},
	journal = {arXiv:1508.02788 [cs]},
	author = {Breuel, Thomas M.},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.02788},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, K.3.2},
	file = {arXiv\:1508.02788 PDF:/Users/b3020111/Zotero/storage/3R9PU3QP/Breuel - 2015 - The Effects of Hyperparameters on SGD Training of .pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/XIN5IH2B/1508.html:text/html}
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2019-01-08},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1609.04747 PDF:/Users/b3020111/Zotero/storage/9MQR2SCP/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/62QFN8PH/1609.html:text/html}
}

@article{loshchilov_sgdr:_2016,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	urldate = {2019-01-08},
	journal = {arXiv:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.03983},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv\:1608.03983 PDF:/Users/b3020111/Zotero/storage/43KPKN54/Loshchilov and Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ZGNZ6WS6/1608.html:text/html}
}

@article{jaderberg_population_2017,
	title = {Population {Based} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.09846},
	abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present {\textbackslash}emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
	urldate = {2019-01-08},
	journal = {arXiv:1711.09846 [cs]},
	author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09846},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1711.09846 PDF:/Users/b3020111/Zotero/storage/7MFTI52F/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/BIR9BCMK/1711.html:text/html}
}

@article{huang_snapshot_2017,
	title = {Snapshot {Ensembles}: {Train} 1, get {M} for free},
	shorttitle = {Snapshot {Ensembles}},
	url = {http://arxiv.org/abs/1704.00109},
	abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
	urldate = {2019-01-08},
	journal = {arXiv:1704.00109 [cs]},
	author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
	month = mar,
	year = {2017},
	note = {arXiv: 1704.00109},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1704.00109 PDF:/Users/b3020111/Zotero/storage/RJ7LY6XI/Huang et al. - 2017 - Snapshot Ensembles Train 1, get M for free.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/LQY9MDXI/1704.html:text/html}
}

@article{smith_cyclical_2015,
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.01186},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	urldate = {2019-01-08},
	journal = {arXiv:1506.01186 [cs]},
	author = {Smith, Leslie N.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01186},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1506.01186 PDF:/Users/b3020111/Zotero/storage/BM9FYJNA/Smith - 2015 - Cyclical Learning Rates for Training Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/UGUJ42DH/1506.html:text/html}
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2019-01-08},
	journal = {arXiv:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.06993},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1608.06993 PDF:/Users/b3020111/Zotero/storage/K3U93XP6/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/WIUXMLVY/1608.html:text/html}
}

@article{zhang_mixup:_2017,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2019-01-08},
	journal = {arXiv:1710.09412 [cs, stat]},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09412},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.09412 PDF:/Users/b3020111/Zotero/storage/GJMG7XN2/Zhang et al. - 2017 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/GVN2VDY8/1710.html:text/html}
}

@article{devries_improved_2017,
	title = {Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}},
	url = {http://arxiv.org/abs/1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
	urldate = {2019-01-08},
	journal = {arXiv:1708.04552 [cs]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.04552},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1708.04552 PDF:/Users/b3020111/Zotero/storage/URI6BI5T/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/K735KXT3/1708.html:text/html}
}

@article{elsken_simple_2017,
	title = {Simple {And} {Efficient} {Architecture} {Search} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.04528},
	abstract = {Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6\% in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5\%.},
	urldate = {2019-01-08},
	journal = {arXiv:1711.04528 [cs, stat]},
	author = {Elsken, Thomas and Metzen, Jan-Hendrik and Hutter, Frank},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.04528},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1711.04528 PDF:/Users/b3020111/Zotero/storage/843UYA4I/Elsken et al. - 2017 - Simple And Efficient Architecture Search for Convo.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/GBEBMT6I/1711.html:text/html}
}

@article{he_bag_2018,
	title = {Bag of {Tricks} for {Image} {Classification} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.01187},
	abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
	urldate = {2019-01-08},
	journal = {arXiv:1812.01187 [cs]},
	author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01187},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1812.01187 PDF:/Users/b3020111/Zotero/storage/8FH5QY53/He et al. - 2018 - Bag of Tricks for Image Classification with Convol.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/M9KGD67C/1812.html:text/html}
}

@inproceedings{karnowski_dolphin_2015,
	address = {Waikoloa, HI, USA},
	title = {Dolphin {Detection} and {Tracking}},
	isbn = {978-0-7695-5469-3},
	url = {http://ieeexplore.ieee.org/document/7046814/},
	doi = {10.1109/WACVW.2015.10},
	urldate = {2019-01-14},
	booktitle = {2015 {IEEE} {Winter} {Applications} and {Computer} {Vision} {Workshops}},
	publisher = {IEEE},
	author = {Karnowski, Jeremy and Hutchins, Edwin and Johnson, Christine},
	month = jan,
	year = {2015},
	pages = {51--56}
}

@article{yahn_how_2019,
	title = {How to tell them apart? {Discriminating} tropical blackfish species using fin and body measurements from photographs taken at sea: {DISCRIMINATING} {TROPICAL} {BLACKFISH} {SPECIES}},
	issn = {08240469},
	shorttitle = {How to tell them apart?},
	url = {http://doi.wiley.com/10.1111/mms.12584},
	doi = {10.1111/mms.12584},
	language = {en},
	urldate = {2019-02-04},
	journal = {Marine Mammal Science},
	author = {Yahn, Shelby N. and Baird, Robin W. and Mahaffy, Sabre D. and Webster, Daniel L.},
	month = jan,
	year = {2019}
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2019-02-11},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1405.0312 PDF:/Users/b3020111/Zotero/storage/V4BHGBCE/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/4S98T37Y/1405.html:text/html}
}

@article{liu_mobile_2017,
	title = {Mobile {Video} {Object} {Detection} with {Temporally}-{Aware} {Feature} {Maps}},
	url = {http://arxiv.org/abs/1711.06368},
	abstract = {This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.},
	urldate = {2019-03-06},
	journal = {arXiv:1711.06368 [cs]},
	author = {Liu, Mason and Zhu, Menglong},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06368},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1711.06368 PDF:/Users/b3020111/Zotero/storage/JYZEHKYN/Liu and Zhu - 2017 - Mobile Video Object Detection with Temporally-Awar.pdf:application/pdf;arXiv\:1711.06368 PDF:/Users/b3020111/Zotero/storage/GBUBLB8I/Liu and Zhu - 2017 - Mobile Video Object Detection with Temporally-Awar.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/T9XUGQYL/1711.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/9Y23LQU2/1711.html:text/html}
}

@inproceedings{patterson_fine-grained_2005,
	title = {Fine-grained activity recognition by aggregating abstract object usage},
	doi = {10.1109/ISWC.2005.22},
	abstract = {In this paper we present results related to achieving finegrained activity recognition for context-aware computing applications. We examine the advantages and challenges of reasoning with globally unique object instances detected by an RFID glove. We present a sequence of increasingly powerful probabilistic graphical models for activity recognition. We show the advantages of adding additional complexity and conclude with a model that can reason tractably about aggregated object instances and gracefully generalizes from object instances to their classes by using abstraction smoothing. We apply these models to data collected from a morning household routine.},
	booktitle = {Ninth {IEEE} {International} {Symposium} on {Wearable} {Computers} ({ISWC}'05)},
	author = {Patterson, D. J. and Fox, D. and Kautz, H. and Philipose, M.},
	month = oct,
	year = {2005},
	keywords = {abstract object usage, abstraction smoothing, Character recognition, computer vision, context-aware computing RFID glove, fine-grained activity recognition, Inference algorithms, inference mechanisms, Machine vision, mobile computing, Multimodal sensors, Object detection, planning (artificial intelligence), probabilistic graphical model, radiofrequency identification, Radiofrequency identification, Robustness, Sensor phenomena and characterization, Wearable computers, Wearable sensors},
	pages = {44--51},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/DSTJVV2V/1550785.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/IIMGYJ6S/Patterson et al. - 2005 - Fine-grained activity recognition by aggregating a.pdf:application/pdf}
}

@article{hughes_automated_2017,
	title = {Automated {Visual} {Fin} {Identification} of {Individual} {Great} {White} {Sharks}},
	volume = {122},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-016-0961-y},
	doi = {10.1007/s11263-016-0961-y},
	abstract = {This paper discusses the automated visual identiﬁcation of individual great white sharks from dorsal ﬁn imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained ﬁn images. To the best of our knowledge this line of work establishes the ﬁrst fully automated contour-based visual ID system in the ﬁeld of animal biometrics. The approach put forward appreciates shark ﬁns as textureless, ﬂexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we ﬁrst introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust ﬁn detection. Secondly, we show that combinatorial, scale-space selective ﬁngerprinting can successfully encode ﬁn individuality. We then measure the species-speciﬁc distribution of visual individuality along the ﬁn contour via an embedding into a global ‘ﬁn space’. Exploiting this domain, we ﬁnally propose a non-linear model for individual animal recognition and combine all approaches into a ﬁne-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail.},
	language = {en},
	number = {3},
	urldate = {2019-03-14},
	journal = {International Journal of Computer Vision},
	author = {Hughes, Benjamin and Burghardt, Tilo},
	month = may,
	year = {2017},
	pages = {542--557},
	file = {Hughes and Burghardt - 2017 - Automated Visual Fin Identification of Individual .pdf:/Users/b3020111/Zotero/storage/R5KW3Y26/Hughes and Burghardt - 2017 - Automated Visual Fin Identification of Individual .pdf:application/pdf}
}

@inproceedings{van_horn_building_2015,
	address = {Boston, MA, USA},
	title = {Building a bird recognition app and large scale dataset with citizen scientists: {The} fine print in fine-grained dataset collection},
	isbn = {978-1-4673-6964-0},
	shorttitle = {Building a bird recognition app and large scale dataset with citizen scientists},
	url = {http://ieeexplore.ieee.org/document/7298658/},
	doi = {10.1109/CVPR.2015.7298658},
	abstract = {From these results, we can see that there are clear distinctions between the two different worker pools. Citizen scientists are clearly more capable at labeling ﬁne-grained categories than MTurkers. However, the raw throughput of MTurk means that you can ﬁnish annotating your dataset sooner than when using citizen scientists. If the annotation task does not require much domain knowledge (such as part annotation), then MTurkers can perform on par with citizen scientists. Gathering ﬁne-grained category labels with MTurk should be done with care, as we have shown that naive averaging of labels does not converge to the correct label. Finally, the cost savings of using citizen scientists can be signiﬁcant when the number of annotation tasks grows.},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Van Horn, Grant and Branson, Steve and Farrell, Ryan and Haber, Scott and Barry, Jessie and Ipeirotis, Panos and Perona, Pietro and Belongie, Serge},
	month = jun,
	year = {2015},
	pages = {595--604},
	file = {Van Horn et al. - 2015 - Building a bird recognition app and large scale da.pdf:/Users/b3020111/Zotero/storage/6QZ4QHGX/Van Horn et al. - 2015 - Building a bird recognition app and large scale da.pdf:application/pdf}
}

@inproceedings{di_style_2013,
	address = {OR, USA},
	title = {Style {Finder}: {Fine}-{Grained} {Clothing} {Style} {Detection} and {Retrieval}},
	isbn = {978-0-7695-4990-3},
	shorttitle = {Style {Finder}},
	url = {http://ieeexplore.ieee.org/document/6595844/},
	doi = {10.1109/CVPRW.2013.6},
	abstract = {With the rapid proliferation of smartphones and tablet computers, search has moved beyond text to other modalities like images and voice. For many applications like Fashion, visual search offers a compelling interface that can capture stylistic visual elements beyond color and pattern that cannot be as easily described using text. However, extracting and matching such attributes remains an extremely challenging task due to high variability and deformability of clothing items. In this paper, we propose a ﬁne-grained learning model and multimedia retrieval framework to address this problem. First, an attribute vocabulary is constructed using human annotations obtained on a novel ﬁnegrained clothing dataset. This vocabulary is then used to train a ﬁne-grained visual recognition system for clothing styles. We report benchmark recognition and retrieval results on Women’s Fashion Coat Dataset and illustrate potential mobile applications for attribute-based multimedia retrieval of clothing items and image annotation.},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Di, Wei and Wah, Catherine and Bhardwaj, Anurag and Piramuthu, Robinson and Sundaresan, Neel},
	month = jun,
	year = {2013},
	pages = {8--13},
	file = {Di et al. - 2013 - Style Finder Fine-Grained Clothing Style Detectio.pdf:/Users/b3020111/Zotero/storage/BMFGKBV8/Di et al. - 2013 - Style Finder Fine-Grained Clothing Style Detectio.pdf:application/pdf}
}

@article{sinha_face_2006,
	title = {Face {Recognition} by {Humans}: {Nineteen} {Results} {All} {Computer} {Vision} {Researchers} {Should} {Know} {About}},
	volume = {94},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Face {Recognition} by {Humans}},
	url = {http://ieeexplore.ieee.org/document/4052483/},
	doi = {10.1109/JPROC.2006.884093},
	abstract = {A key goal of computer vision researchers is to create automated face recognition systems that can equal, and eventually surpass, human performance. To this end, it is imperative that computational researchers know of the key findings from experimental studies of face recognition by humans. These findings provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities. In this paper, we present what we believe are 19 basic results, with implications for the design of computational systems. Each result is described briefly and appropriate pointers are provided to permit an in-depth study of any particular result.},
	language = {en},
	number = {11},
	urldate = {2019-03-14},
	journal = {Proceedings of the IEEE},
	author = {Sinha, P. and Balas, B. and Ostrovsky, Y. and Russell, R.},
	month = nov,
	year = {2006},
	pages = {1948--1962},
	file = {Sinha et al. - 2006 - Face Recognition by Humans Nineteen Results All C.pdf:/Users/b3020111/Zotero/storage/L2NCZB59/Sinha et al. - 2006 - Face Recognition by Humans Nineteen Results All C.pdf:application/pdf}
}

@book{noauthor_computer_2014,
	address = {New York},
	edition = {1st edition},
	series = {Lecture notes in computer science},
	title = {Computer vision - {ECCV} 2014: 13th {European} {Conference}, {Zurich}, {Switzerland}, {September} 6-12, 2014, {Proceedings}, part i},
	isbn = {978-3-319-10589-5},
	shorttitle = {Computer vision - {ECCV} 2014},
	language = {en},
	number = {8689},
	publisher = {Springer},
	year = {2014},
	file = {2014 - Computer vision - ECCV 2014 13th European Confere.pdf:/Users/b3020111/Zotero/storage/VADUBVSN/2014 - Computer vision - ECCV 2014 13th European Confere.pdf:application/pdf}
}

@inproceedings{yang_large-scale_2015,
	address = {Boston, MA, USA},
	title = {A large-scale car dataset for fine-grained categorization and verification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299023/},
	doi = {10.1109/CVPR.2015.7299023},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
	month = jun,
	year = {2015},
	pages = {3973--3981},
	file = {Yang et al. - 2015 - A large-scale car dataset for fine-grained categor.pdf:/Users/b3020111/Zotero/storage/X57PHTKL/Yang et al. - 2015 - A large-scale car dataset for fine-grained categor.pdf:application/pdf}
}

@book{institute_of_electrical_and_electronics_engineers_2009_2009,
	address = {Piscataway, NJ},
	title = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}: {CVPR} 2009 ; {Miami} [{Beach}], {Florida}, {USA}, 20 - 25 {June} 2009},
	isbn = {978-1-4244-3992-8 978-1-4244-3991-1},
	shorttitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	language = {eng},
	publisher = {IEEE},
	editor = {Institute of Electrical {and} Electronics Engineers and IEEE Computer Society},
	year = {2009}
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	language = {en},
	number = {2},
	urldate = {2019-03-20},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	pages = {303--338},
	file = {Submitted Version:/Users/b3020111/Zotero/storage/Z9XEBVKA/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf}
}

@article{fei-fei_learning_2007,
	title = {Learning generative visual models from few training examples: {An} incremental {Bayesian} approach tested on 101 object categories},
	volume = {106},
	issn = {10773142},
	shorttitle = {Learning generative visual models from few training examples},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206001688},
	doi = {10.1016/j.cviu.2005.09.012},
	language = {en},
	number = {1},
	urldate = {2019-03-20},
	journal = {Computer Vision and Image Understanding},
	author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
	month = apr,
	year = {2007},
	pages = {59--70}
}

@inproceedings{van_horn_inaturalist_2018,
	title = {The inaturalist species classification and detection dataset},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
	year = {2018},
	pages = {8769--8778}
}

@article{uijlings_selective_2013,
	title = {Selective search for object recognition},
	volume = {104},
	number = {2},
	journal = {International journal of computer vision},
	author = {Uijlings, Jasper RR and Van De Sande, Koen EA and Gevers, Theo and Smeulders, Arnold WM},
	year = {2013},
	keywords = {Appearance Model, Colour Space, Exhaustive Search, Object Location, Object Recognition},
	pages = {154--171},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/JAPFEW4A/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf}
}

@inproceedings{farrell_birdlets:_2011,
	title = {Birdlets: {Subordinate} categorization using volumetric primitives and pose-normalized appearance},
	isbn = {1-4577-1102-8},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Farrell, Ryan and Oza, Om and Zhang, Ning and Morariu, Vlad I. and Darrell, Trevor and Davis, Larry S.},
	year = {2011},
	pages = {161--168}
}

@inproceedings{bourdev_poselets:_2009,
	title = {Poselets: {Body} part detectors trained using 3d human pose annotations},
	isbn = {1-4244-4420-9},
	booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Bourdev, Lubomir and Malik, Jitendra},
	year = {2009},
	pages = {1365--1372}
}

@article{felzenszwalb_object_2010,
	title = {Object detection with discriminatively trained part-based models},
	volume = {32},
	number = {9},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
	year = {2010},
	pages = {1627--1645}
}

@inproceedings{parkhi_cats_2012,
	title = {Cats and dogs},
	isbn = {1-4673-1228-2},
	booktitle = {2012 {IEEE} conference on computer vision and pattern recognition},
	publisher = {IEEE},
	author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
	year = {2012},
	pages = {3498--3505}
}

@inproceedings{zhang_deformable_2013,
	title = {Deformable part descriptors for fine-grained recognition and attribute prediction},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Zhang, Ning and Farrell, Ryan and Iandola, Forrest and Darrell, Trevor},
	year = {2013},
	pages = {729--736}
}

@article{belhumeur_localizing_2013,
	title = {Localizing parts of faces using a consensus of exemplars},
	volume = {35},
	number = {12},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Belhumeur, Peter N. and Jacobs, David W. and Kriegman, David J. and Kumar, Neeraj},
	year = {2013},
	pages = {2930--2940}
}

@inproceedings{liu_dog_2012,
	title = {Dog breed classification using part localization},
	booktitle = {European conference on computer vision},
	publisher = {Springer},
	author = {Liu, Jiongxin and Kanazawa, Angjoo and Jacobs, David and Belhumeur, Peter},
	year = {2012},
	pages = {172--185}
}

@inproceedings{gavves_fine-grained_2013,
	title = {Fine-{Grained} {Categorization} by {Alignments}},
	url = {http://openaccess.thecvf.com/content_iccv_2013/html/Gavves_Fine-Grained_Categorization_by_2013_ICCV_paper.html},
	urldate = {2019-04-12},
	author = {Gavves, E. and Fernando, B. and Snoek, C. G. M. and Smeulders, A. W. M. and Tuytelaars, T.},
	year = {2013},
	pages = {1713--1720},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/R336N6YG/Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf:application/pdf;Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf:/Users/b3020111/Zotero/storage/3FNJNV8Y/Gavves et al. - 2013 - Fine-Grained Categorization by Alignments.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/DLKGGEFZ/Gavves_Fine-Grained_Categorization_by_2013_ICCV_paper.html:text/html}
}

@inproceedings{xie_hierarchical_2013,
	address = {Sydney, Australia},
	title = {Hierarchical {Part} {Matching} for {Fine}-{Grained} {Visual} {Categorization}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751314/},
	doi = {10.1109/ICCV.2013.206},
	abstract = {As a special topic in computer vision, ﬁne-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classiﬁcation tasks in which objects have large inter-class variation, the visual concepts in the ﬁne-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difﬁcult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model.},
	language = {en},
	urldate = {2019-04-12},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Xie, Lingxi and Tian, Qi and Hong, Richang and Yan, Shuicheng and Zhang, Bo},
	month = dec,
	year = {2013},
	pages = {1641--1648},
	file = {Xie et al. - 2013 - Hierarchical Part Matching for Fine-Grained Visual.pdf:/Users/b3020111/Zotero/storage/EIXGXSV4/Xie et al. - 2013 - Hierarchical Part Matching for Fine-Grained Visual.pdf:application/pdf}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	pages = {4},
	file = {Paszke et al. - Automatic differentiation in PyTorch.pdf:/Users/b3020111/Zotero/storage/C7IV49XY/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf}
}

@inproceedings{deng_imagenet:_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	urldate = {2019-05-30},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
	file = {Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:/Users/b3020111/Zotero/storage/DRIS6Y3J/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/Users/b3020111/Zotero/storage/ZX3TYXVX/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2019-06-04},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation, beginnings},
	pages = {115--133},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/DN2VLF7L/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2019-06-04},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1502.03167 PDF:/Users/b3020111/Zotero/storage/PXJLUU3K/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv\:1502.03167 PDF:/Users/b3020111/Zotero/storage/JN2GN9LU/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/T6VEIVFW/1502.html:text/html;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/LMBHZT8H/1502.html:text/html}
}

@inproceedings{bottou_large-scale_2010,
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	isbn = {978-3-7908-2604-3},
	abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
	language = {en},
	booktitle = {Proceedings of {COMPSTAT}'2010},
	publisher = {Physica-Verlag HD},
	author = {Bottou, Léon},
	editor = {Lechevallier, Yves and Saporta, Gilbert},
	year = {2010},
	keywords = {efficiency, online learning, stochastic gradient descent},
	pages = {177--186},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/7ITG9H83/Bottou - 2010 - Large-Scale Machine Learning with Stochastic Gradi.pdf:application/pdf}
}

@inproceedings{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN’s), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.},
	booktitle = {Proceedings of the {IEEE}},
	author = {Lecun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	pages = {2278--2324},
	file = {Citeseer - Full Text PDF:/Users/b3020111/Zotero/storage/EJZX7HLS/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf;Citeseer - Snapshot:/Users/b3020111/Zotero/storage/MUKSRDLZ/summary.html:text/html}
}

@article{boureau_theoretical_2010,
	title = {A {Theoretical} {Analysis} of {Feature} {Pooling} in {Visual} {Recognition}},
	abstract = {Many modern visual recognition algorithms incorporate a step of spatial ‘pooling’, where the outputs of several nearby feature detectors are combined into a local or global ‘bag of features’, in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly inﬂuence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.},
	language = {en},
	author = {Boureau, Y-Lan and Ponce, Jean and LeCun, Yann},
	year = {2010},
	pages = {8},
	file = {Boureau et al. - A Theoretical Analysis of Feature Pooling in Visua.pdf:/Users/b3020111/Zotero/storage/BKAPV48R/Boureau et al. - A Theoretical Analysis of Feature Pooling in Visua.pdf:application/pdf}
}

@article{zeiler_stochastic_2013,
	title = {Stochastic {Pooling} for {Regularization} of {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1301.3557},
	abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
	urldate = {2019-06-04},
	journal = {arXiv:1301.3557 [cs, stat]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3557},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1301.3557 PDF:/Users/b3020111/Zotero/storage/HXPYGB2H/Zeiler and Fergus - 2013 - Stochastic Pooling for Regularization of Deep Conv.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/DTNWHC5P/1301.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2019-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Snapshot:/Users/b3020111/Zotero/storage/BHYY9P53/4824-imagenet-classification-with-deep-convolutional-neural-networ.html:text/html}
}

@inproceedings{girshick_rich_2014,
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
	urldate = {2019-06-05},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	pages = {580--587},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/JGNEX6FD/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/9VHKQ9ZC/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html:text/html}
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} {With} {Convolutions}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
	urldate = {2019-06-05},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	pages = {1--9},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/UCTZ9YS6/Szegedy et al. - 2015 - Going Deeper With Convolutions.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/7MY2ALKU/Szegedy_Going_Deeper_With_2015_CVPR_paper.html:text/html}
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2019-06-05},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1504.08083 PDF:/Users/b3020111/Zotero/storage/X6QD9372/Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ZL4KXHHF/1504.html:text/html}
}

@inproceedings{redmon_you_2016,
	address = {Las Vegas, NV, USA},
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	language = {en},
	urldate = {2019-06-05},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2016},
	pages = {779--788},
	file = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:/Users/b3020111/Zotero/storage/UBCXIYVS/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}

@misc{griffin_caltech-256_2007,
	type = {Report or {Paper}},
	title = {Caltech-256 {Object} {Category} {Dataset}},
	url = {http://resolver.caltech.edu/CaltechAUTHORS:CNS-TR-2007-001},
	abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
	urldate = {2019-06-05},
	author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
	month = mar,
	year = {2007},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/5GCVQRFP/Griffin et al. - 2007 - Caltech-256 Object Category Dataset.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/6MHTRL5S/7694.html:text/html}
}

@inproceedings{zhang_part-based_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Part-{Based} {R}-{CNNs} for {Fine}-{Grained} {Category} {Detection}},
	isbn = {978-3-319-10590-1},
	abstract = {Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zhang, Ning and Donahue, Jeff and Girshick, Ross and Darrell, Trevor},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {convolutional models, Fine-grained recognition, object detection},
	pages = {834--849},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/ZUDK3G2G/Zhang et al. - 2014 - Part-Based R-CNNs for Fine-Grained Category Detect.pdf:application/pdf}
}

@article{moore_marine_2008,
	title = {Marine mammals as ecosystem sentinels},
	volume = {89},
	issn = {0022-2372, 1545-1542},
	url = {https://academic.oup.com/jmammal/article-lookup/doi/10.1644/07-MAMM-S-312R1.1},
	doi = {10.1644/07-MAMM-S-312R1.1},
	abstract = {The earth’s climate is changing, possibly at an unprecedented rate. Overall, the planet is warming, sea ice and glaciers are in retreat, sea level is rising, and pollutants are accumulating in the environment and within organisms. These clear physical changes undoubtedly affect marine ecosystems. Species dependent on sea ice, such as the polar bear (Ursus maritimus) and the ringed seal (Phoca hispida), provide the clearest examples of sensitivity to climate change. Responses of cetaceans to climate change are more difﬁcult to discern, but in the eastern North Paciﬁc evidence is emerging that gray whales (Eschrichtius robustus) are delaying their southbound migration, expanding their feeding range along the migration route and northward to Arctic waters, and even remaining in polar waters over winter—all indications that North Paciﬁc and Arctic ecosystems are in transition. To use marine mammals as sentinels of ecosystem change, we must expand our existing research strategies to encompass the decadal and ocean-basin temporal and spatial scales consistent with their natural histories.},
	language = {en},
	number = {3},
	urldate = {2019-08-07},
	journal = {Journal of Mammalogy},
	author = {Moore, Sue E.},
	month = jun,
	year = {2008},
	pages = {534--540},
	file = {Moore - 2008 - Marine mammals as ecosystem sentinels.pdf:/Users/b3020111/Zotero/storage/K3JB8DTI/Moore - 2008 - Marine mammals as ecosystem sentinels.pdf:application/pdf}
}

@article{connor_male_2015,
	title = {Male dolphin alliances in {Shark} {Bay}: changing perspectives in a 30-year study},
	volume = {103},
	issn = {0003-3472},
	shorttitle = {Male dolphin alliances in {Shark} {Bay}},
	url = {http://www.sciencedirect.com/science/article/pii/S0003347215000810},
	doi = {10.1016/j.anbehav.2015.02.019},
	abstract = {Bottlenose dolphins, Tursiops cf. aduncus, in Shark Bay, Western Australia exhibit the most complex alliances known outside of humans. Advances in our understanding of these alliances have occurred with expansions of our study area each decade. In the 1980s, we discovered that males cooperated in stable trios and pairs (first-order alliances) to herd individual oestrous females, and that two such alliances of four to six, sometimes related, individuals (second-order alliances) cooperated against other males in contests over females. The 1990s saw the discovery of a large 14-member second-order alliance whose members exhibited labile first-order alliance formation among nonrelatives. Partner preferences as well as a relationship between first-order alliance stability and consortship rate in this ‘super-alliance’ indicated differentiated relationships. The contrast between the super-alliance and the 1980s alliances suggested two alliance tactics. An expansion of the study area in the 2000s revealed a continuum of second-order alliance sizes in an open social network and no simple relationship between second-order alliance size and alliance stability, but generalized the relationship between first-order alliance stability and consortship rate within second-order alliances. Association preferences and contests involving three second-order alliances indicated the presence of third-order alliances. Second-order alliances may persist for 20 years with stability thwarted by gradual attrition, but underlying flexibility is indicated by observations of individuals joining other alliances, including old males joining young or old second-order alliances. The dolphin research has informed us on the evolution of complex social relationships and large brain evolution in mammals and the ecology of alliance formation. Variation in odontocete brain size and the large radiation of delphinids into a range of habitats holds great promise that further effort to describe their societies will be rewarded with similar advances in our understanding of these important issues.},
	urldate = {2019-08-07},
	journal = {Animal Behaviour},
	author = {Connor, Richard C. and Krützen, Michael},
	month = may,
	year = {2015},
	keywords = {alliance, bottlenose dolphin, social organization, social structure},
	pages = {223--235},
	file = {ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/QRX3NQY5/S0003347215000810.html:text/html}
}

@article{wursig_photographic_1977,
	title = {The {Photographic} {Determination} of {Group} {Size}, {Composition}, and {Stability} of {Coastal} {Porpoises} ({Tursiops} truncatus)},
	volume = {198},
	copyright = {© 1977},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/198/4318/755},
	doi = {10.1126/science.198.4318.755},
	abstract = {During a 21-month study, 53 individual bottle-nosed porpoises were recognized by photographs of their dorsal fins. They traveled in small subgroups (mean size = 15) composed of a stable core of five animals plus other individuals that varied greatly from sighting to sighting.},
	language = {en},
	number = {4318},
	urldate = {2019-08-07},
	journal = {Science},
	author = {Würsig, Bernd and Würsig, Melany},
	month = nov,
	year = {1977},
	pages = {755--756},
	file = {Snapshot:/Users/b3020111/Zotero/storage/DZ3CT76C/tab-pdf.html:text/html}
}

@inproceedings{stewman_iterative_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Iterative 3-{D} {Pose} {Correction} and {Content}-{Based} {Image} {Retrieval} for {Dorsal} {Fin} {Recognition}},
	isbn = {978-3-540-44893-8},
	abstract = {Contour or boundary descriptors may be used in content-based image retrieval to effectively identify appropriate images when image content consists primarily of a single object of interest. The registration of object contours for the purposes of comparison is complicated when the objects of interest are characterized by open contours and when reliable feature points for contour alignment are absent. We present an application that employs an iterative approach to the alignment of open contours for the purposes of image retrieval and demonstrate its success in identifying individual bottlenose dolphins from the profiles of their dorsal fins.},
	language = {en},
	booktitle = {Image {Analysis} and {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Stewman, John and Debure, Kelly and Hale, Scott and Russell, Adam},
	editor = {Campilho, Aurélio and Kamel, Mohamed S.},
	year = {2006},
	keywords = {Active Contour, Bottlenose Dolphin, Feature Point, Humpback Whale, Sperm Whale},
	pages = {648--660}
}

@article{galatius_lagenorhynchus_2016,
	title = {Lagenorhynchus albirostris ({Cetacea}: {Delphinidae})},
	volume = {48},
	issn = {0076-3519},
	shorttitle = {Lagenorhynchus albirostris ({Cetacea}},
	url = {https://academic.oup.com/mspecies/article/48/933/35/2583995},
	doi = {10.1093/mspecies/sew003},
	abstract = {Abstract.  Lagenorhynchus albirostris ( Gray, 1846a ) is a delphinid commonly called the white-beaked dolphin. A robustly built dolphin with black, white, and g},
	language = {en},
	number = {933},
	urldate = {2019-08-07},
	journal = {Mammalian Species},
	author = {Galatius, Anders and Kinze, Carl Christian},
	month = aug,
	year = {2016},
	pages = {35--47},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/ZBZNMHAX/Galatius and Kinze - 2016 - Lagenorhynchus albirostris (Cetacea Delphinidae).pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/6AIVQWB2/2583995.html:text/html}
}

@article{hammond_cetacean_2013,
	title = {Cetacean abundance and distribution in {European} {Atlantic} shelf waters to inform conservation and management},
	volume = {164},
	issn = {0006-3207},
	url = {http://www.sciencedirect.com/science/article/pii/S0006320713001055},
	doi = {https://doi.org/10.1016/j.biocon.2013.04.010},
	abstract = {The European Union (EU) Habitats Directive requires Member States to monitor and maintain at favourable conservation status those species identified to be in need of protection, including all cetaceans. In July 2005 we surveyed the entire EU Atlantic continental shelf to generate robust estimates of abundance for harbour porpoise and other cetacean species. The survey used line transect sampling methods and purpose built data collection equipment designed to minimise bias in estimates of abundance. Shipboard transects covered 19,725km in sea conditions ⩽Beaufort 4 in an area of 1,005,743km2. Aerial transects covered 15,802km in good/moderate conditions (⩽Beaufort 3) in an area of 364,371km2. Thirteen cetacean species were recorded; abundance was estimated for harbour porpoise (375,358; CV=0.197), bottlenose dolphin (16,485; CV=0.422), white-beaked dolphin (16,536; CV=0.303), short-beaked common dolphin (56,221; CV=0.234) and minke whale (18,958; CV=0.347). Abundance in 2005 was similar to that estimated in July 1994 for harbour porpoise, white-beaked dolphin and minke whale in a comparable area. However, model-based density surfaces showed a marked difference in harbour porpoise distribution between 1994 and 2005. Our results allow EU Member States to discharge their responsibilities under the Habitats Directive and inform other international organisations concerning the assessment of conservation status of cetaceans and the impact of bycatch at a large spatial scale. The lack of evidence for a change in harbour porpoise abundance in EU waters as a whole does not exclude the possibility of an impact of bycatch in some areas. Monitoring bycatch and estimation of abundance continue to be essential.},
	journal = {Biological Conservation},
	author = {Hammond, Philip S. and Macleod, Kelly and Berggren, Per and Borchers, David L. and Burt, Louise and Cañadas, Ana and Desportes, Geneviève and Donovan, Greg P. and Gilles, Anita and Gillespie, Douglas and Gordon, Jonathan and Hiby, Lex and Kuklik, Iwona and Leaper, Russell and Lehnert, Kristina and Leopold, Mardik and Lovell, Phil and Øien, Nils and Paxton, Charles G. M. and Ridoux, Vincent and Rogan, Emer and Samarra, Filipa and Scheidat, Meike and Sequeira, Marina and Siebert, Ursula and Skov, Henrik and Swift, René and Tasker, Mark L. and Teilmann, Jonas and Canneyt, Olivier Van and Vázquez, José Antonio},
	month = aug,
	year = {2013},
	keywords = {Bottlenose dolphin, Bycatch, Common dolphin, Conservation status, Habitats Directive, Harbour porpoise, Line transect sampling, Minke whale, North Sea, SCANS, White-beaked dolphin},
	pages = {107 -- 122}
}

@article{khosla_novel_2011,
	title = {Novel {Dataset} for {Fine}-{Grained} {Image} {Categorization}: {Stanford} {Dogs}},
	language = {en},
	author = {Khosla, Aditya and Jayadevaprakash, Nityananda and Yao, Bangpeng and Li, Fei-Fei},
	year = {2011},
	pages = {2},
	file = {Khosla et al. - Novel Dataset for Fine-Grained Image Categorizatio.pdf:/Users/b3020111/Zotero/storage/Z6NWN3YS/Khosla et al. - Novel Dataset for Fine-Grained Image Categorizatio.pdf:application/pdf}
}

@misc{welinder_caltech-ucsd_2010,
	type = {Report or {Paper}},
	title = {Caltech-{UCSD} {Birds} 200},
	url = {http://resolver.caltech.edu/CaltechAUTHORS:20111026-155425465},
	abstract = {Caltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated with 200 bird species. It was created to enable the study of subordinate categorization, which is not possible with other popular datasets that focus on basic level categories (such as PASCAL VOC, Caltech-101, etc). The images were downloaded from the website Flickr and filtered by workers on Amazon Mechanical Turk. Each image is annotated with a bounding box, a rough bird segmentation, and a set of attribute labels.},
	urldate = {2019-08-07},
	author = {Welinder, Peter and Branson, Steve and Mita, Takeshi and Wah, Catherine and Schroff, Florian and Belongie, Serge and Perona, Pietro},
	month = sep,
	year = {2010},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/G9FNXHGZ/Welinder et al. - 2010 - Caltech-UCSD Birds 200.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/PGPKCN6N/27468.html:text/html}
}

@misc{wah_caltech-ucsd_2011,
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset} - {CaltechAUTHORS}},
	url = {https://authors.library.caltech.edu/27452/},
	urldate = {2019-08-07},
	author = {Wah, C and Branson, S and Welinder, P and Perona, P and Belongie, S},
	year = {2011},
	file = {The Caltech-UCSD Birds-200-2011 Dataset - CaltechAUTHORS:/Users/b3020111/Zotero/storage/7DLXYRVN/27452.html:text/html}
}

@article{trotter_northumberland_2019,
	title = {The {Northumberland} {Dolphin} {Dataset}: {A} {Multimedia} {Individual} {Cetacean} {Dataset} for {Fine}-{Grained} {Categorisation}},
	copyright = {All rights reserved},
	shorttitle = {The {Northumberland} {Dolphin} {Dataset}},
	url = {http://arxiv.org/abs/1908.02669},
	abstract = {Methods for cetacean research include photo-identification (photo-id) and passive acoustic monitoring (PAM) which generate thousands of images per expedition that are currently hand categorised by researchers into the individual dolphins sighted. With the vast amount of data obtained it is crucially important to develop a system that is able to categorise this quickly. The Northumberland Dolphin Dataset (NDD) is an on-going novel dataset project made up of above and below water images of, and spectrograms of whistles from, white-beaked dolphins. These are produced by photo-id and PAM data collection methods applied off the coast of Northumberland, UK. This dataset will aid in building cetacean identification models, reducing the number of human-hours required to categorise images. Example use cases and areas identified for speed up are examined.},
	urldate = {2019-08-08},
	journal = {arXiv:1908.02669 [cs]},
	author = {Trotter, Cameron and Atkinson, Georgia and Sharpe, Matthew and McGough, A. Stephen and Wright, Nick and Berggren, Per},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.02669},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1908.02669 PDF:/Users/b3020111/Zotero/storage/THL6KSXC/Trotter et al. - 2019 - The Northumberland Dolphin Dataset A Multimedia I.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/TWQ2L5L7/1908.html:text/html}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2019-08-08},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444}
}

@incollection{hecht-nielsen_iii.3_1992,
	title = {{III}.3 - {Theory} of the {Backpropagation} {Neural} {Network}**{Based} on “nonindent” by {Robert} {Hecht}-{Nielsen}, which appeared in {Proceedings} of the {International} {Joint} {Conference} on {Neural} {Networks} 1, 593–611, {June} 1989. © 1989 {IEEE}.},
	isbn = {978-0-12-741252-8},
	url = {http://www.sciencedirect.com/science/article/pii/B9780127412528500108},
	abstract = {This chapter presents a survey of the elementary theory of the basic backpropagation neural network architecture, covering the areas of architectural design, performance measurement, function approximation capability, and learning. The survey includes a formulation of the backpropagation neural network architecture to make it a valid neural network and a proof that the backpropagation mean squared error function exists and is differentiable. Also included in the survey is a theorem showing that any L2 function can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. An appendix presents a speculative neurophysiological model illustrating the way in which the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of cerebral cortex. One of the crucial decisions in the design of the backpropagation architecture is the selection of a sigmoidal activation function.},
	urldate = {2019-08-08},
	booktitle = {Neural {Networks} for {Perception}},
	publisher = {Academic Press},
	author = {Hecht-nielsen, ROBERT},
	editor = {Wechsler, Harry},
	month = jan,
	year = {1992},
	doi = {10.1016/B978-0-12-741252-8.50010-8},
	pages = {65--93},
	file = {ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/EU4UZNPJ/B9780127412528500108.html:text/html}
}

@article{hartigan_algorithm_1979,
	title = {Algorithm {AS} 136: {A} {K}-{Means} {Clustering} {Algorithm}},
	volume = {28},
	issn = {0035-9254},
	shorttitle = {Algorithm {AS} 136},
	url = {https://www.jstor.org/stable/2346830},
	doi = {10.2307/2346830},
	number = {1},
	urldate = {2019-08-08},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Hartigan, J. A. and Wong, M. A.},
	year = {1979},
	pages = {100--108}
}

@incollection{bottou_tradeoffs_2008,
	title = {The {Tradeoffs} of {Large} {Scale} {Learning}},
	url = {http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf},
	urldate = {2019-08-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Bottou, Léon and Bousquet, Olivier},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {161--168},
	file = {NIPS Full Text PDF:/Users/b3020111/Zotero/storage/TUEX7IP8/Bottou and Bousquet - 2008 - The Tradeoffs of Large Scale Learning.pdf:application/pdf;NIPS Snapshot:/Users/b3020111/Zotero/storage/XLK98IPG/3323-the-tradeoffs-of-large-scale-learning.html:text/html}
}

@article{tieleman_lecture_2012,
	title = {Lecture 6.5-rmsprop: {Divide} the gradient by a running average of its recent magnitude},
	volume = {4},
	number = {2},
	journal = {COURSERA: Neural networks for machine learning},
	author = {Tieleman, Tijmen and Hinton, Geoffrey},
	year = {2012},
	pages = {26--31}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2019-08-08},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/Users/b3020111/Zotero/storage/B6RJQ8WY/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/7UDECC6K/1412.html:text/html}
}

@article{reddi_convergence_2019,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	url = {http://arxiv.org/abs/1904.09237},
	abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
	urldate = {2019-08-08},
	journal = {arXiv:1904.09237 [cs, math, stat]},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09237},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv\:1904.09237 PDF:/Users/b3020111/Zotero/storage/AMQ8GBCR/Reddi et al. - 2019 - On the Convergence of Adam and Beyond.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/U9SZ6EY7/1904.html:text/html}
}

@article{qian_momentum_1999,
	title = {On the momentum term in gradient descent learning algorithms},
	volume = {12},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608098001166},
	doi = {10.1016/S0893-6080(98)00116-6},
	abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
	number = {1},
	urldate = {2019-08-08},
	journal = {Neural Networks},
	author = {Qian, Ning},
	month = jan,
	year = {1999},
	keywords = {Critical damping, Damped harmonic oscillator, Gradient descent learning algorithm, Learning rate, Momentum, Speed of convergence},
	pages = {145--151},
	file = {ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/C5KJ3QL6/S0893608098001166.html:text/html}
}

@incollection{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf},
	urldate = {2019-08-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2933--2941},
	file = {NIPS Full Text PDF:/Users/b3020111/Zotero/storage/FDV754IE/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf:application/pdf;NIPS Snapshot:/Users/b3020111/Zotero/storage/EEX376Q9/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimiza.html:text/html}
}

@article{choromanska_loss_2015,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	language = {en},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gerard Ben and LeCun, Yann},
	year = {2015},
	pages = {13},
	file = {Choromanska et al. - The Loss Surfaces of Multilayer Networks.pdf:/Users/b3020111/Zotero/storage/ZLYBNI7R/Choromanska et al. - The Loss Surfaces of Multilayer Networks.pdf:application/pdf}
}

@article{alber_backprop_2018,
	title = {Backprop {Evolution}},
	url = {http://arxiv.org/abs/1808.02822},
	abstract = {The back-propagation algorithm is the cornerstone of deep learning. Despite its importance, few variations of the algorithm have been attempted. This work presents an approach to discover new variations of the back-propagation equation. We use a domain speciﬁc language to describe update equations as a list of primitive functions. An evolution-based method is used to discover new propagation rules that maximize the generalization performance after a few epochs of training. We ﬁnd several update equations that can train faster with short training times than standard back-propagation, and perform similar as standard back-propagation at convergence.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1808.02822 [cs, stat]},
	author = {Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.02822},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Alber et al. - 2018 - Backprop Evolution.pdf:/Users/b3020111/Zotero/storage/R9F77IAP/Alber et al. - 2018 - Backprop Evolution.pdf:application/pdf}
}

@article{linnainmaa_representation_1970,
	title = {The representation of the cumulative rounding error of an algorithm as a {Taylor} expansion of the local rounding errors},
	journal = {Master's Thesis (in Finnish), Univ. Helsinki},
	author = {Linnainmaa, Seppo},
	year = {1970},
	pages = {6--7}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	language = {en},
	author = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
	year = {1986},
	pages = {4},
	file = {Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:/Users/b3020111/Zotero/storage/JQKU8AGJ/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf}
}

@inproceedings{bengio_use_1994,
	title = {Use of genetic programming for the search of a new learning rule for neural networks},
	isbn = {0-7803-1899-4},
	publisher = {IEEE},
	author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn},
	year = {1994},
	pages = {324--327}
}

@article{lillicrap_random_2014,
	title = {Random feedback weights support learning in deep neural networks},
	url = {http://arxiv.org/abs/1411.0247},
	abstract = {The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
	urldate = {2019-08-08},
	journal = {arXiv:1411.0247 [cs, q-bio]},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.0247},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv\:1411.0247 PDF:/Users/b3020111/Zotero/storage/G42ZBI5X/Lillicrap et al. - 2014 - Random feedback weights support learning in deep n.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/9JX7GZX9/1411.html:text/html}
}

@inproceedings{nokland_direct_2016,
	title = {Direct feedback alignment provides learning in deep neural networks},
	author = {Nøkland, Arild},
	year = {2016},
	pages = {1037--1045}
}

@inproceedings{liao_how_2016,
	title = {How important is weight symmetry in backpropagation?},
	author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
	year = {2016}
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2019-08-08},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01852},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1502.01852 PDF:/Users/b3020111/Zotero/storage/X5CNZA7E/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/FU2Y55LH/1502.html:text/html}
}

@article{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2019-08-08},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1703.06870 PDF:/Users/b3020111/Zotero/storage/N7QKA2DD/He et al. - 2017 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/ZZY6ZAVB/1703.html:text/html}
}

@misc{waleed_mask_2017,
	title = {Mask {R}-{CNN} for object detection and instance segmentation on {Keras} and {TensorFlow}},
	copyright = {View license},
	shorttitle = {Mask {R}-{CNN} for object detection and instance segmentation on {Keras} and {TensorFlow}},
	url = {https://github.com/matterport/Mask_RCNN},
	urldate = {2019-08-08},
	publisher = {Matterport, Inc},
	author = {Waleed, Abdulla},
	year = {2017},
	note = {original-date: 2017-10-19T20:28:34Z}
}

@article{long_fully_2014,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1411.4038 [cs]},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4038},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf:/Users/b3020111/Zotero/storage/B4VVGTY3/Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf}
}

@article{badrinarayanan_segnet:_2015,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:/Users/b3020111/Zotero/storage/QI9XLE8U/Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf}
}

@article{chen_semantic_2014,
	title = {Semantic {Image} {Segmentation} with {Deep} {Convolutional} {Nets} and {Fully} {Connected} {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classiﬁcation and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classiﬁcation (also called ”semantic image segmentation”). We show that responses at the ﬁnal layer of DCNNs are not sufﬁciently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efﬁciently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1412.7062 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf:/Users/b3020111/Zotero/storage/E4EEQ3ZR/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutiona.pdf:application/pdf}
}

@article{hariharan_simultaneous_2014,
	title = {Simultaneous {Detection} and {Segmentation}},
	url = {http://arxiv.org/abs/1407.1808},
	abstract = {We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-speciﬁc, topdown ﬁgure-ground predictions to reﬁne our bottom-up proposals. We show a 7 point boost (16\% relative) over our baselines on SDS, a 5 point boost (10\% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.},
	language = {en},
	urldate = {2019-08-08},
	journal = {arXiv:1407.1808 [cs]},
	author = {Hariharan, Bharath and Arbeláez, Pablo and Girshick, Ross and Malik, Jitendra},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.1808},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hariharan et al. - 2014 - Simultaneous Detection and Segmentation.pdf:/Users/b3020111/Zotero/storage/ZVZGIU2G/Hariharan et al. - 2014 - Simultaneous Detection and Segmentation.pdf:application/pdf}
}

@article{buhrmester_amazons_2011,
	title = {Amazon's {Mechanical} {Turk}: {A} new source of inexpensive, yet high-quality, data?},
	volume = {6},
	issn = {1745-6916},
	number = {1},
	journal = {Perspectives on psychological science},
	author = {Buhrmester, Michael and Kwang, Tracy and Gosling, Samuel D},
	year = {2011},
	pages = {3--5}
}

@inproceedings{lee_difference_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Difference {Target} {Propagation}},
	isbn = {978-3-319-23528-8},
	abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	editor = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, Vítor and Soares, Carlos and Gama, João and Jorge, Alípio},
	year = {2015},
	keywords = {Deep Neural Network, Hide Layer, Hide Unit, Target Propagation, Test Error},
	pages = {498--515},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/25S9ZBW5/Lee et al. - 2015 - Difference Target Propagation.pdf:application/pdf}
}

@article{dietterich_overfitting_1995,
	title = {Overfitting and undercomputing in machine learning},
	volume = {27},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=212094.212114},
	doi = {10.1145/212094.212114},
	number = {3},
	urldate = {2019-08-12},
	journal = {ACM Computing Surveys},
	author = {Dietterich, Tom},
	month = sep,
	year = {1995},
	pages = {326--327}
}

@article{fu_retinamask:_2019,
	title = {{RetinaMask}: {Learning} to predict masks improves state-of-the-art single-shot detection for free},
	url = {https://arxiv.org/pdf/1901.03353.pdf},
	journal = {arXiv preprint arXiv:1901.03353},
	author = {Fu, Cheng-Yang and Shvets, Mykhailo and Berg, Alexander C},
	year = {2019}
}

@article{van_aswegen_morphological_2019,
	title = {Morphological differences between coastal bottlenose dolphin ({Tursiops} aduncus) populations identified using non-invasive stereo-laser photogrammetry},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-019-48419-3},
	doi = {10.1038/s41598-019-48419-3},
	language = {en},
	number = {1},
	urldate = {2019-08-27},
	journal = {Scientific Reports},
	author = {van Aswegen, Martin and Christiansen, Fredrik and Symons, John and Mann, Janet and Nicholson, Krista and Sprogis, Kate and Bejder, Lars},
	month = dec,
	year = {2019},
	pages = {12235}
}

@article{hinton_discovering_2011,
	title = {Discovering {Binary} {Codes} for {Documents} by {Learning} {Deep} {Generative} {Models}: {Topics} in {Cognitive} {Science}(2010)},
	volume = {3},
	issn = {17568757},
	shorttitle = {Discovering {Binary} {Codes} for {Documents} by {Learning} {Deep} {Generative} {Models}},
	url = {http://doi.wiley.com/10.1111/j.1756-8765.2010.01109.x},
	doi = {10.1111/j.1756-8765.2010.01109.x},
	language = {en},
	number = {1},
	urldate = {2019-08-30},
	journal = {Topics in Cognitive Science},
	author = {Hinton, Geoffrey and Salakhutdinov, Ruslan},
	month = jan,
	year = {2011},
	pages = {74--91}
}

@article{hinton_optimal_1983,
	title = {Optimal {Perceptual} {Inference}},
	language = {en},
	author = {Hinton, G E and Sejnowski, T J},
	year = {1983},
	pages = {6},
	file = {Hinton and Sejnowski - Optimal Perceptual Inference.pdf:/Users/b3020111/Zotero/storage/QTT5L8R4/Hinton and Sejnowski - Optimal Perceptual Inference.pdf:application/pdf}
}

@article{xiao_fashion-mnist:_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	url = {http://arxiv.org/abs/1708.07747},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2019-11-30},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/VBYDQTWM/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/KEE89UXE/1708.html:text/html}
}

@inproceedings{akkaynak_sea-thru_2019,
	address = {Long Beach, CA, USA},
	title = {Sea-{Thru}: {A} {Method} for {Removing} {Water} {From} {Underwater} {Images}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Sea-{Thru}},
	url = {https://ieeexplore.ieee.org/document/8954437/},
	doi = {10.1109/CVPR.2019.00178},
	abstract = {Robust recovery of lost colors in underwater images remains a challenging problem. We recently showed that this was partly due to the prevalent use of an atmospheric image formation model for underwater images and proposed a physically accurate model. The revised model showed: 1) the attenuation coefﬁcient of the signal is not uniform across the scene but depends on object range and reﬂectance, 2) the coefﬁcient governing the increase in backscatter with distance differs from the signal attenuation coefﬁcient. Here, we present the ﬁrst method that recovers color with our revised model, using RGBD images. The Sea-thru method estimates backscatter using the dark pixels and their known range information. Then, it uses an estimate of the spatially varying illuminant to obtain the range-dependent attenuation coefﬁcient. Using more than 1,100 images from two optically different water bodies, which we make available, we show that our method with the revised model outperforms those using the atmospheric model. Consistent removal of water will open up large underwater datasets to powerful computer vision and machine learning algorithms, creating exciting opportunities for the future of underwater exploration and conservation.},
	language = {en},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Akkaynak, Derya and Treibitz, Tali},
	month = jun,
	year = {2019},
	pages = {1682--1691},
	file = {Akkaynak and Treibitz - 2019 - Sea-Thru A Method for Removing Water From Underwa.pdf:/Users/b3020111/Zotero/storage/3XD6HGA4/Akkaynak and Treibitz - 2019 - Sea-Thru A Method for Removing Water From Underwa.pdf:application/pdf}
}

@inproceedings{hillman_finscan_2002,
	title = {"{Finscan}", a computer system for photographic identification of marine animals},
	volume = {2},
	doi = {10.1109/IEMBS.2002.1106279},
	abstract = {A system has been developed for computer-assisted photographic identification of marine animals. The system creates and maintains a database of images of dorsal fins or flukes, and the user queries it by entering a new image acquired in the field. The system searches the database for similar images based on the notching pattern of the dorsal fin or fluke, and offers a list of database members ordered by similarity to the query image. A syntactic/semantic string representation method is used for matching and is compared with matching by direct alignment of edge patterns. The system has been tested with data sets of images of several biological species.},
	booktitle = {Proceedings of the {Second} {Joint} 24th {Annual} {Conference} and the {Annual} {Fall} {Meeting} of the {Biomedical} {Engineering} {Society}] [{Engineering} in {Medicine} and {Biology}},
	author = {Hillman, G.R. and Kehtarnavaz, N. and Wursig, B. and Araabi, B. and Gailey, G. and Weller, D. and Mandava, S. and Tagare, H.},
	month = oct,
	year = {2002},
	note = {ISSN: 1094-687X},
	keywords = {Biomedical imaging, Catalogs, computer-assisted photographic identification, Dolphins, dorsal fins, Finscan, flukes, image database, Image databases, image recognition, marine animals, Marine animals, Medical diagnostic imaging, notching pattern, Pattern matching, pattern recognition, photographic identification, string matching, syntactic/semantic string representation method, Testing, Tin, visual databases, Whales, zoology},
	pages = {1065--1066 vol.2},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/Z7F868QQ/1106279.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/MGKTF6VT/Hillman et al. - 2002 - Finscan, a computer system for photographic iden.pdf:application/pdf}
}

@article{weinstein_computer_2018,
	title = {A computer vision for animal ecology},
	volume = {87},
	copyright = {© 2017 The Author. Journal of Animal Ecology © 2017 British Ecological Society},
	issn = {1365-2656},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2656.12780},
	doi = {10.1111/1365-2656.12780},
	abstract = {A central goal of animal ecology is to observe species in the natural world. The cost and challenge of data collection often limit the breadth and scope of ecological study. Ecologists often use image capture to bolster data collection in time and space. However, the ability to process these images remains a bottleneck. Computer vision can greatly increase the efficiency, repeatability and accuracy of image review. Computer vision uses image features, such as colour, shape and texture to infer image content. I provide a brief primer on ecological computer vision to outline its goals, tools and applications to animal ecology. I reviewed 187 existing applications of computer vision and divided articles into ecological description, counting and identity tasks. I discuss recommendations for enhancing the collaboration between ecologists and computer scientists and highlight areas for future growth of automated image analysis.},
	language = {en},
	number = {3},
	urldate = {2020-02-12},
	journal = {Journal of Animal Ecology},
	author = {Weinstein, Ben G.},
	year = {2018},
	keywords = {automation, camera traps, ecological monitoring, images, unmanned aerial vehicles},
	pages = {533--545},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/GZY4PVE6/Weinstein - 2018 - A computer vision for animal ecology.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/WNXSDRJR/1365-2656.html:text/html}
}

@misc{gilman_computer-assisted_2016,
	title = {Computer-assisted recognition of dolphin individuals using dorsal fin pigmentations - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/7804460},
	urldate = {2020-02-12},
	author = {Gilman, Andrew and Hupman, Krista and Stockin, Karen and Pawley, Matthew D M},
	year = {2016},
	file = {Computer-assisted recognition of dolphin individuals using dorsal fin pigmentations - IEEE Conference Publication:/Users/b3020111/Zotero/storage/RRFACPVA/7804460.html:text/html}
}

@inproceedings{bouma_individual_2018-1,
	title = {Individual {Common} {Dolphin} {Identification} {Via} {Metric} {Embedding} {Learning}},
	doi = {10.1109/IVCNZ.2018.8634778},
	abstract = {Photo-identification (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin fin photograph captured in the field to a catalogue of known individuals. We examine this problem in the context of open-set recognition and utilise a triplet loss function to learn a compact representation of fin images in a Euclidean embedding, where the Euclidean distance metric represents fin similarity. We show that this compact representation can be successfully learnt from a fairly small (in deep learning context) training set and still generalise well to out-of-sample identities (completely new dolphin individuals), with top-1 and top-5 test set (37 individuals) accuracy of 90.5 ± 2 and 93.6 ± 1 percent. In the presence of 1200 distractors, top-1 accuracy dropped by 12\%; however, top-5 accuracy saw only a 2.8\% drop.},
	booktitle = {2018 {International} {Conference} on {Image} and {Vision} {Computing} {New} {Zealand} ({IVCNZ})},
	author = {Bouma, Soren and Pawley, Matthew D.M and Hupman, Krista and Gilman, Andrew},
	month = nov,
	year = {2018},
	note = {ISSN: 2151-2191},
	keywords = {social structure, Dolphins, image recognition, compact representation, deep learning context, dolphin identification via metric embedding learning, dolphin individuals, ecological sciences, Euclidean distance metric, Euclidean embedding, fin similarity, image capture, image matching, image registration, image segmentation, known individuals, laborious manual process, learning (artificial intelligence), Measurement, open-set recognition, photo-identification, pose estimation, Sociology, Statistics, Task analysis, top-5 test set accuracy, Training, Training data, triplet loss function},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/3WWIAWWK/8634778.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/3YYHZJAM/Bouma et al. - 2018 - Individual Common Dolphin Identification Via Metri.pdf:application/pdf}
}

@misc{haimeh_haimehfinfindr_2020,
	title = {haimeh/{finFindR}},
	url = {https://github.com/haimeh/finFindR},
	abstract = {An application for dorsal fin image recognition and cataloguing},
	urldate = {2020-02-12},
	author = {haimeh},
	month = feb,
	year = {2020},
	note = {original-date: 2017-10-22T05:19:57Z},
	keywords = {app, cpp, dolphin, r, recognition}
}

@misc{noauthor_hierarchical_nodate,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}: {Journal} of the {American} {Statistical} {Association}: {Vol} 58, {No} 301},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
	urldate = {2020-02-12},
	file = {Hierarchical Grouping to Optimize an Objective Function\: Journal of the American Statistical Association\: Vol 58, No 301:/Users/b3020111/Zotero/storage/UI4B2BBW/01621459.1963.html:text/html}
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} {Unified} {Embedding} for {Face} {Recognition} and {Clustering}},
	shorttitle = {{FaceNet}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
	urldate = {2020-02-12},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	pages = {815--823},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/RWHINAPE/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/X3ESPCJN/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html:text/html}
}

@misc{noauthor_wildbookorgibeis-curvrank-module_2020,
	title = {{WildbookOrg}/ibeis-curvrank-module},
	copyright = {Apache-2.0},
	url = {https://github.com/WildbookOrg/ibeis-curvrank-module},
	abstract = {Python module that wraps https://github.com/hjweide/dolphin-identification},
	urldate = {2020-02-12},
	publisher = {Wildbook Organization},
	month = jan,
	year = {2020},
	note = {original-date: 2018-03-14T16:49:31Z}
}

@article{weideman_integral_2017,
	title = {Integral {Curvature} {Representation} and {Matching} {Algorithms} for {Identification} of {Dolphins} and {Whales}},
	url = {http://arxiv.org/abs/1708.07785},
	abstract = {We address the problem of identifying individual cetaceans from images showing the trailing edge of their fins. Given the trailing edge from an unknown individual, we produce a ranking of known individuals from a database. The nicks and notches along the trailing edge define an individual's unique signature. We define a representation based on integral curvature that is robust to changes in viewpoint and pose, and captures the pattern of nicks and notches in a local neighborhood at multiple scales. We explore two ranking methods that use this representation. The first uses a dynamic programming time-warping algorithm to align two representations, and interprets the alignment cost as a measure of similarity. This algorithm also exploits learned spatial weights to downweight matches from regions of unstable curvature. The second interprets the representation as a feature descriptor. Feature keypoints are defined at the local extrema of the representation. Descriptors for the set of known individuals are stored in a tree structure, which allows us to perform queries given the descriptors from an unknown trailing edge. We evaluate the top-k accuracy on two real-world datasets to demonstrate the effectiveness of the curvature representation, achieving top-1 accuracy scores of approximately 95\% and 80\% for bottlenose dolphins and humpback whales, respectively.},
	urldate = {2020-02-12},
	journal = {arXiv:1708.07785 [cs]},
	author = {Weideman, Hendrik J. and Jablons, Zachary M. and Holmberg, Jason and Flynn, Kiirsten and Calambokidis, John and Tyson, Reny B. and Allen, Jason B. and Wells, Randall S. and Hupman, Krista and Urian, Kim and Stewart, Charles V.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07785},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/C6JAW9XJ/Weideman et al. - 2017 - Integral Curvature Representation and Matching Alg.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/8E3NGLCW/1708.html:text/html}
}

@misc{mann_mann-urian-google_2019,
	title = {Mann-{Urian}-{Google} {Cloud} {AI}.pdf},
	url = {https://drive.google.com/file/d/1nLXJiSjGCgvjW54UTd46EU64oJcKS1f_/view?usp=sharing&usp=embed_facebook},
	urldate = {2020-02-12},
	journal = {Google Docs},
	author = {Mann, Janet},
	year = {2019},
	file = {Snapshot:/Users/b3020111/Zotero/storage/UHEITJLN/view.html:text/html}
}

@misc{liang_googles_2018,
	title = {Google's {AI} {Helps} {Researcher} {ID} {Dolphins}},
	url = {https://www.deeperblue.com/googles-ai-helps-researcher-id-dolphins/},
	abstract = {Google's artificial intelligence engineers are collaborating with a university researcher to identify dolphins in the wild.},
	language = {en-US},
	urldate = {2020-02-12},
	journal = {DeeperBlue.com},
	author = {Liang, John},
	month = oct,
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/GJ9FC95C/googles-ai-helps-researcher-id-dolphins.html:text/html}
}

@misc{georgetown_university_is_2018,
	title = {Is {That} ‘{Jimmy} {Carter}’ or ‘{Barbara} {Bush}?’ {Google} {Designs}, {Professor} {Uses} {Artificial} {Intelligence} to {Track} {Wildlife}},
	shorttitle = {Is {That} ‘{Jimmy} {Carter}’ or ‘{Barbara} {Bush}?},
	url = {https://www.georgetown.edu/news/is-that-jimmy-carter-or-barbara-bush-google-designs-professor-uses-artificial-intelligence-to-track-wildlife/},
	abstract = {Janet Mann, professor of biology and psychology, collaborates with Google’s artificial intelligence engineers on individual identification of wild dolphins through images.},
	language = {en-US},
	urldate = {2020-02-12},
	journal = {Georgetown University},
	author = {Georgetown University},
	month = oct,
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/NB4JSIJQ/is-that-jimmy-carter-or-barbara-bush-google-designs-professor-uses-artificial-intelligence-to-t.html:text/html}
}

@misc{kaggle_humpback_2018,
	title = {Humpback {Whale} {Identification} {Challenge}},
	url = {https://kaggle.com/c/whale-categorization-playground},
	abstract = {Can you identify a whale by the picture of its fluke?},
	language = {en},
	urldate = {2020-02-12},
	author = {Kaggle},
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/PNAXMZVK/whale-categorization-playground.html:text/html}
}

@article{deng_arcface_2019,
	title = {{ArcFace}: {Additive} {Angular} {Margin} {Loss} for {Deep} {Face} {Recognition}},
	shorttitle = {{ArcFace}},
	url = {http://arxiv.org/abs/1801.07698},
	abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
	urldate = {2020-02-12},
	journal = {arXiv:1801.07698 [cs]},
	author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	month = feb,
	year = {2019},
	note = {arXiv: 1801.07698},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/IRNLNN3A/Deng et al. - 2019 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/5UJNLEDI/1801.html:text/html}
}

@article{deng_arcface_2019-1,
	title = {{ArcFace}: {Additive} {Angular} {Margin} {Loss} for {Deep} {Face} {Recognition}},
	shorttitle = {{ArcFace}},
	url = {http://arxiv.org/abs/1801.07698},
	abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
	urldate = {2020-02-12},
	journal = {arXiv:1801.07698 [cs]},
	author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	month = feb,
	year = {2019},
	note = {arXiv: 1801.07698},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/H327VFAM/Deng et al. - 2019 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/PW7Z6QVH/1801.html:text/html}
}

@misc{taigman_deepface_2014,
	title = {{DeepFace}: {Closing} the {Gap} to {Human}-{Level} {Performance} in {Face} {Verification}},
	shorttitle = {{DeepFace}},
	url = {https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/},
	abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing exp...},
	language = {en-US},
	urldate = {2020-02-12},
	journal = {Facebook Research},
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year = {2014},
	file = {Snapshot:/Users/b3020111/Zotero/storage/2QH2BWPA/deepface-closing-the-gap-to-human-level-performance-in-face-verification.html:text/html}
}

@misc{cheeseman_ted_2019,
	title = {Ted {Cheeseman} {Happywhale} presentation-{WMMC}-{Rise} of the {Machines}.pdf},
	url = {https://drive.google.com/file/d/1yRtNKagANZOgpY6852zzUpZmiFXuZmLM/view?usp=sharing&usp=embed_facebook},
	urldate = {2020-02-12},
	journal = {Google Docs},
	author = {Cheeseman, Ted},
	year = {2019},
	file = {Snapshot:/Users/b3020111/Zotero/storage/E6KJSPX9/view.html:text/html}
}

@misc{fisheries_finbase_2018,
	title = {{FinBase} {Photo}-{Identification} {Database} {System} {\textbar} {NOAA} {Fisheries}},
	url = {https://www.fisheries.noaa.gov/national/marine-mammal-protection/finbase-photo-identification-database-system},
	abstract = {A database system consists of a collection of subdirectories (associated with image and file storage) and front- and back-end Microsoft Access databases},
	language = {en},
	urldate = {2020-02-12},
	journal = {NOAA},
	author = {Fisheries, NOAA},
	month = feb,
	year = {2018},
	file = {Snapshot:/Users/b3020111/Zotero/storage/9JNLCLAR/finbase-photo-identification-database-system.html:text/html}
}

@misc{thompson_finfindrpdf_2019,
	title = {{finFindR}.pdf},
	url = {https://drive.google.com/file/d/1jgbnAH2_C0DoUe8HidJfP13yfhtqz2gB/view?usp=sharing&usp=embed_facebook},
	urldate = {2020-02-12},
	journal = {Google Docs},
	author = {Thompson, Jaime},
	year = {2019},
	file = {Snapshot:/Users/b3020111/Zotero/storage/87LSWBKE/view.html:text/html}
}

@article{ward_hierarchical_1963,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}},
	volume = {58},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
	doi = {10.1080/01621459.1963.10500845},
	abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {\textgreater} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n − 1 mutually exclusive sets by considering the union of all possible n(n − 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
	number = {301},
	urldate = {2020-02-12},
	journal = {Journal of the American Statistical Association},
	author = {Ward, Joe H.},
	month = mar,
	year = {1963},
	pages = {236--244},
	file = {Snapshot:/Users/b3020111/Zotero/storage/FC6PSAID/01621459.1963.html:text/html}
}

@article{hale_unsupervised_2012,
	title = {Unsupervised {Threshold} for {Automatic} {Extraction} of {Dolphin} {Dorsal} {Fin} {Outlines} from {Digital} {Photographs} in {DARWIN} ({Digital} {Analysis} and {Recognition} of {Whale} {Images} on a {Network})},
	url = {http://arxiv.org/abs/1202.4107},
	abstract = {At least two software packages---DARWIN, Eckerd College, and FinScan, Texas A\&M---exist to facilitate the identification of cetaceans---whales, dolphins, porpoises---based upon the naturally occurring features along the edges of their dorsal fins. Such identification is useful for biological studies of population, social interaction, migration, etc. The process whereby fin outlines are extracted in current fin-recognition software packages is manually intensive and represents a major user input bottleneck: it is both time consuming and visually fatiguing. This research aims to develop automated methods (employing unsupervised thresholding and morphological processing techniques) to extract cetacean dorsal fin outlines from digital photographs thereby reducing manual user input. Ideally, automatic outline generation will improve the overall user experience and improve the ability of the software to correctly identify cetaceans. Various transformations from color to gray space were examined to determine which produced a grayscale image in which a suitable threshold could be easily identified. To assist with unsupervised thresholding, a new metric was developed to evaluate the jaggedness of figures ("pixelarity") in an image after thresholding. The metric indicates how cleanly a threshold segments background and foreground elements and hence provides a good measure of the quality of a given threshold. This research results in successful extractions in roughly 93\% of images, and significantly reduces user-input time.},
	urldate = {2020-02-12},
	journal = {arXiv:1202.4107 [cs]},
	author = {Hale, Scott A.},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.4107},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, H.5.2, I.4.6},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/LN3QTPEQ/Hale - 2012 - Unsupervised Threshold for Automatic Extraction of.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/HS82CUNS/1202.html:text/html}
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
	language = {en},
	number = {2},
	urldate = {2020-04-09},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv: 1610.02391},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {336--359},
	file = {Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:/Users/b3020111/Zotero/storage/GMTDMNX8/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf}
}

@article{dumoulin_guide_2018,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	language = {en},
	urldate = {2020-04-22},
	journal = {arXiv:1603.07285 [cs, stat]},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = jan,
	year = {2018},
	note = {arXiv: 1603.07285},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf:/Users/b3020111/Zotero/storage/A86KZPMN/Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf:application/pdf}
}

@misc{dumoulin_160307285_2018,
	title = {[1603.07285] {A} guide to convolution arithmetic for deep learning},
	url = {https://arxiv.org/abs/1603.07285},
	urldate = {2020-04-22},
	author = {Dumoulin, V and Visin, F},
	year = {2018},
	file = {[1603.07285] A guide to convolution arithmetic for deep learning:/Users/b3020111/Zotero/storage/D59VT2UB/1603.html:text/html}
}

@inproceedings{wilber_animal_2013,
	title = {Animal recognition in the {Mojave} {Desert}: {Vision} tools for field biologists},
	shorttitle = {Animal recognition in the {Mojave} {Desert}},
	doi = {10.1109/WACV.2013.6475020},
	abstract = {The outreach of computer vision to non-traditional areas has enormous potential to enable new ways of solving real world problems. One such problem is how to incorporate technology in the effort to protect endangered and threatened species in the wild. This paper presents a snapshot of our interdisciplinary team's ongoing work in the Mojave Desert to build vision tools for field biologists to study the currently threatened Desert Tortoise and Mohave Ground Squirrel. Animal population studies in natural habitats present new recognition challenges for computer vision, where open set testing and access to just limited computing resources lead us to algorithms that diverge from common practices. We introduce a novel algorithm for animal classification that addresses the open set nature of this problem and is suitable for implementation on a smartphone. Further, we look at a simple model for object recognition applied to the problem of individual species identification. A thorough experimental analysis is provided for real field data collected in the Mojave desert.},
	booktitle = {2013 {IEEE} {Workshop} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Wilber, Michael J. and Scheirer, Walter J. and Leitner, Phil and Heflin, Brian and Zott, James and Reinke, Daniel and Delaney, David K. and Boult, Terrance E.},
	month = jan,
	year = {2013},
	note = {ISSN: 1550-5790},
	keywords = {computer vision, zoology, Sociology, Statistics, Training data, animal classification, animal population studies, animal recognition, Animals, biology computing, Computer vision, desert tortoise, field biologists, image classification, individual species identification, interdisciplinary team, Mohave ground squirrel, Mojave desert, natural habitats, nontraditional areas, object recognition, smart phones, smartphone, Support vector machines, vision tools},
	pages = {206--213},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/RNDZ2RWM/6475020.html:text/html;IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/Y9HLV52F/Wilber et al. - 2013 - Animal recognition in the Mojave Desert Vision to.pdf:application/pdf}
}

@article{beery_iwildcam_2019,
	title = {The {iWildCam} 2019 {Challenge} {Dataset}},
	url = {http://arxiv.org/abs/1907.07617},
	abstract = {Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to different areas we are faced with an interesting problem: how do you classify a species in a new region that you may not have seen in previous training data? In order to tackle this problem, we have prepared a dataset and challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. We use the Caltech Camera Traps dataset, collected from the American Southwest, as training data. We add a new dataset from the American Northwest, curated from data provided by the Idaho Department of Fish and Game (IDFG), as our test dataset. The test data has some class overlap with the training data, some species are found in both datasets, but there are both species seen during training that are not seen during test and vice versa. To help fill the gaps in the training species, we allow competitors to utilize transfer learning from two alternate domains: human-curated images from iNaturalist and synthetic images from Microsoft's TrapCam-AirSim simulation environment.},
	urldate = {2020-05-15},
	journal = {arXiv:1907.07617 [cs]},
	author = {Beery, Sara and Morris, Dan and Perona, Pietro},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.07617},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/K4HQZZHJ/Beery et al. - 2019 - The iWildCam 2019 Challenge Dataset.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/9DHRMU4D/1907.html:text/html}
}

@inproceedings{anwar_invariant_2015,
	address = {Swansea},
	title = {Invariant {Image}-{Based} {Species} {Classification} of {Butterflies} and {Reef} {Fish}},
	isbn = {978-1-901725-57-5},
	url = {http://www.bmva.org/bmvc/2015/mvab/papers/paper005/index.html},
	doi = {10.5244/C.29.MVAB.5},
	abstract = {We propose a framework for species-based image classiﬁcation of butterﬂies and reef ﬁsh. To support such image-based classiﬁcation, we use an image representation which enriches the famous bag-of-visual words (BoVWs) model with spatial information. This image representation is developed by encoding the global geometric relationships of visual words in the 2D image plane in a scale- and rotation-invariant manner. In this way, invariance is achieved to the most common variations found in the images of these animals as they can be imaged at different image locations, exhibit various in-plane orientations and have various scales in the images. The images in our butterﬂy and reef ﬁsh datasets belong to 30 species of each animal. We achieve better classiﬁcation rates on both the datasets than the ordinary BoVWs model while still being invariant to the mentioned image variations. Our proposed image-based classiﬁcation framework for butterﬂy and reef ﬁsh species can be considered as a helpful tool for scientiﬁc research, conversation and education.},
	language = {en},
	urldate = {2020-05-15},
	booktitle = {Procedings of the {Machine} {Vision} of {Animals} and their {Behaviour} {Workshop} 2015},
	publisher = {British Machine Vision Association},
	author = {Anwar, Hafeez and Zambanini, Sebastian and Kampel, Martin},
	year = {2015},
	pages = {5.1--5.8},
	file = {Anwar et al. - 2015 - Invariant Image-Based Species Classification of Bu.pdf:/Users/b3020111/Zotero/storage/3HAHU6CR/Anwar et al. - 2015 - Invariant Image-Based Species Classification of Bu.pdf:application/pdf}
}

@inproceedings{vetrova_hidden_2018,
	address = {Auckland, New Zealand},
	title = {Hidden {Features}: {Experiments} with {Feature} {Transfer} for {Fine}-{Grained} {Multi}-{Class} and {One}-{Class} {Image} {Categorization}},
	isbn = {978-1-72810-125-5},
	shorttitle = {Hidden {Features}},
	url = {https://ieeexplore.ieee.org/document/8634790/},
	doi = {10.1109/IVCNZ.2018.8634790},
	abstract = {Can we apply out-of-the box feature transfer using pre-trained convolutional neural networks in ﬁne-grained multiclass image categorization tasks? What is the effect of (a) domainspeciﬁc ﬁne-tuning and (b) a special-purpose network architecture designed and trained speciﬁcally for the target domain? How do these approaches perform in one-class classiﬁcation? We investigate these questions by tackling two biological object recognition tasks: classiﬁcation of “cryptic” plants of genus Coprosma and identiﬁcation of New Zealand moth species. We compare results based on out-of-the-box features extracted using a pre-trained state-of-the-art network to those obtained by ﬁnetuning to the target domain, and also evaluate features learned using a simple Siamese network trained only on data from the target domain. For each extracted feature set, we test a number of classiﬁers, e.g., support vector machines. In addition to multiclass classiﬁcation, we also consider one-class classiﬁcation, a scenario that is particularly relevant to biosecurity applications. In the multi-class setting, we ﬁnd that out-of-the-box lowlevel features extracted from the generic pre-trained network yield high accuracy (90.76\%) when coupled with a simple LDA classiﬁer. Fine-tuning improves accuracy only slightly (to 91.6\%). Interestingly, features extracted from the much simpler Siamese network trained on data from the target domain lead to comparable results (90.8\%). In the one-class classiﬁcation setting, we note high variability in the area under the ROC curve across feature sets, opening up the possibility of considering an ensemble approach.},
	language = {en},
	urldate = {2020-06-12},
	booktitle = {2018 {International} {Conference} on {Image} and {Vision} {Computing} {New} {Zealand} ({IVCNZ})},
	publisher = {IEEE},
	author = {Vetrova, Varvara and Coup, Sheldon and Frank, Eibe and Cree, Michael J.},
	month = nov,
	year = {2018},
	pages = {1--6},
	file = {Vetrova et al. - 2018 - Hidden Features Experiments with Feature Transfer.pdf:/Users/b3020111/Zotero/storage/Y2692WBU/Vetrova et al. - 2018 - Hidden Features Experiments with Feature Transfer.pdf:application/pdf}
}

@article{tian_fcos_2019,
	title = {{FCOS}: {Fully} {Convolutional} {One}-{Stage} {Object} {Detection}},
	shorttitle = {{FCOS}},
	url = {http://arxiv.org/abs/1904.01355},
	abstract = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7\% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1},
	urldate = {2020-06-24},
	journal = {arXiv:1904.01355 [cs]},
	author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
	month = aug,
	year = {2019},
	note = {arXiv: 1904.01355},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/FXJ66A66/Tian et al. - 2019 - FCOS Fully Convolutional One-Stage Object Detecti.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/SVAB2LB8/1904.html:text/html}
}

@article{ravoor_deep_2020,
	title = {Deep {Learning} {Methods} for {Multi}-{Species} {Animal} {Re}-identification and {Tracking} – a {Survey}},
	abstract = {Technology has an important part to play in wildlife and ecosystem conservation, and can vastly reduce time and effort spent in the associated tasks. Deep learning methods for computer vision in particular show good performance on a variety of tasks; animal detection and classification using deep learning networks are widely used to assist ecological studies. A related challenge is tracking animal movement over multiple cameras. For effective animal movement tracking, it is necessary to distinguish between individuals of the same species to correctly identify an individual moving between two cameras. Such problems could potentially be solved through animal re-identification methods. In this paper, the applicability of existing animal re-identification techniques for fully automated individual animal tracking in a cross-camera setup is explored. Recent developments in animal re-identification in the context of open-set recognition of individuals, and the extension of these systems to multiple species is examined. Some of the best performing human re-identification and object tracking systems are also reviewed in view of extending ideas within them to individual animal tracking. The survey concludes by presenting common trends in re-identification methods, lists a few challenges in the domain and recommends possible solutions.},
	language = {en},
	journal = {Computer Science Review},
	author = {Ravoor, Prashanth C},
	year = {2020},
	pages = {10},
	file = {Ravoor - 2020 - Deep Learning Methods for Multi-Species Animal Re-.pdf:/Users/b3020111/Zotero/storage/46HM5H3A/Ravoor - 2020 - Deep Learning Methods for Multi-Species Animal Re-.pdf:application/pdf}
}

@article{schneider_past_2019,
	title = {Past, present and future approaches using computer vision for animal re-identification from camera trap data},
	volume = {10},
	issn = {2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13133},
	doi = {10.1111/2041-210X.13133},
	abstract = {The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.},
	language = {en},
	number = {4},
	urldate = {2020-10-01},
	journal = {Methods in Ecology and Evolution},
	author = {Schneider, Stefan and Taylor, Graham W. and Linquist, Stefan and Kremer, Stefan C.},
	year = {2019},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13133},
	keywords = {computer vision, object detection, camera traps, animal reidentification, convolutional networks, deep learning, density estimation, monitoring},
	pages = {461--470},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/H2D82NL5/Schneider et al. - 2019 - Past, present and future approaches using computer.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/QQJH5XV8/2041-210X.html:text/html}
}

@article{atapour-abarghouei_kings_2019,
	title = {A {Kings} {Ransom} for {Encryption}: {Ransomware} {Classification} using {Augmented} {One}-{Shot} {Learning} and {Bayesian} {Approximation}},
	shorttitle = {A {Kings} {Ransom} for {Encryption}},
	url = {http://arxiv.org/abs/1908.06750},
	abstract = {Newly emerging variants of ransomware pose an ever-growing threat to computer systems governing every aspect of modern life through the handling and analysis of big data. While various recent security-based approaches have focused on detecting and classifying ransomware at the network or system level, easy-to-use post-infection ransomware classiﬁcation for the lay user has not been attempted before. In this paper, we investigate the possibility of classifying the ransomware a system is infected with simply based on a screenshot of the splash screen or the ransom note captured using a consumer camera commonly found in any modern mobile device. To train and evaluate our system, we create a sample dataset of the splash screens of 50 well-known ransomware variants. In our dataset, only a single training image is available per ransomware. Instead of creating a large training dataset of ransomware screenshots, we simulate screenshot capture conditions via carefully designed data augmentation techniques, enabling simple and efﬁcient oneshot learning. Moreover, using model uncertainty obtained via Bayesian approximation, we ensure special input cases such as unrelated non-ransomware images and previously-unseen ransomware variants are correctly identiﬁed for special handling and not mis-classiﬁed. Extensive experimental evaluation demonstrates the efﬁcacy of our work, with accuracy levels of up to 93.6\% for ransomware classiﬁcation.},
	language = {en},
	urldate = {2020-10-05},
	journal = {arXiv:1908.06750 [cs]},
	author = {Atapour-Abarghouei, Amir and Bonner, Stephen and McGough, Andrew Stephen},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.06750},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Atapour-Abarghouei et al. - 2019 - A Kings Ransom for Encryption Ransomware Classifi.pdf:/Users/b3020111/Zotero/storage/5SY5REE3/Atapour-Abarghouei et al. - 2019 - A Kings Ransom for Encryption Ransomware Classifi.pdf:application/pdf}
}

@article{zhong_random_2017,
	title = {Random {Erasing} {Data} {Augmentation}},
	url = {http://arxiv.org/abs/1708.04896},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	urldate = {2020-10-06},
	journal = {arXiv:1708.04896 [cs]},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.04896},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/9GQEJ45U/Zhong et al. - 2017 - Random Erasing Data Augmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/P4ZDV8KM/1708.html:text/html}
}

@inproceedings{nita_cnn-based_2020,
	address = {Online Only, United Kingdom},
	title = {{CNN}-based object detection and segmentation for maritime domain awareness},
	isbn = {978-1-5106-3899-0 978-1-5106-3900-3},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11543/2573287/CNN-based-object-detection-and-segmentation-for-maritime-domain-awareness/10.1117/12.2573287.full},
	doi = {10.1117/12.2573287},
	abstract = {Deep learning algorithms have been proven to be a powerful tool in image and video processing for security and surveillance operations. In a maritime environment, the fusion of electro-optical sensor data with human intelligence plays an important role to counter the security issues. For instance, the situational awareness can be enhanced through an automated system that generates reports on ship identity and signature together with detecting the changes on naval vessels activity. To date, various studies have been set out to explore the performance of deep neural networks using a ship signature database. In the current study, we investigate the Mask R-CNN method to address not only the naval vessel detection using bounding boxes, but also obtaining their segmentation masks. We train and validate the model on data captured by an on-board camera covering the visible spectral band under various weather and light conditions. The experimental results show that Mask RCNN provides high conﬁdence scores on challenging scenarios with a mean average precision of 86.4\%. However, the precision of the segmentation mask is slightly deteriorated when the ships are adjacent to the border of the captured scene. Moreover, the network tested on thermal images indicates a decrease in detection and segmentation performance since the training data distribution is not representative enough.},
	language = {en},
	urldate = {2020-10-08},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} in {Defense} {Applications} {II}},
	publisher = {SPIE},
	author = {Nita, Cornelia and Vandewal, Marijke},
	editor = {Dijk, Judith},
	month = sep,
	year = {2020},
	pages = {4},
	file = {Nita and Vandewal - 2020 - CNN-based object detection and segmentation for ma.pdf:/Users/b3020111/Zotero/storage/B5G87CVY/Nita and Vandewal - 2020 - CNN-based object detection and segmentation for ma.pdf:application/pdf}
}

@article{luo_multiple_2017,
	title = {Multiple {Object} {Tracking}: {A} {Literature} {Review}},
	shorttitle = {Multiple {Object} {Tracking}},
	url = {http://arxiv.org/abs/1409.7618},
	abstract = {Multiple Object Tracking (MOT) is an important computer vision problem which has gained increasing attention due to its academic and commercial potential. Although different kinds of approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the ﬁrst comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in a multiple object tracking system, including formulation, categorization, key principles, evaluation of an MOT are discussed. 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks. 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative comparisons. We also point to some interesting discoveries by analyzing these results. 4) We provide a discussion about issues of MOT research, as well as some interesting directions which could possibly become potential research effort in the future.},
	language = {en},
	urldate = {2020-10-09},
	journal = {arXiv:1409.7618 [cs]},
	author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Zhao, Xiaowei and Kim, Tae-Kyun},
	month = may,
	year = {2017},
	note = {arXiv: 1409.7618},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8},
	file = {Luo et al. - 2017 - Multiple Object Tracking A Literature Review.pdf:/Users/b3020111/Zotero/storage/SKL85HAK/Luo et al. - 2017 - Multiple Object Tracking A Literature Review.pdf:application/pdf}
}

@article{konovalov_individual_nodate,
	title = {Individual {Minke} {Whale} {Recognition} {Using} {Deep} {Learning} {Convolutional} {Neural} {Networks}},
	abstract = {The only known predictable aggregation of dwarf minke whales (Balaenoptera acutorostrata subsp.) occurs in the Australian offshore waters of the northern Great Barrier Reef in May-August each year. The identification of individual whales is required for research on the whales’ population characteristics and for monitoring the potential impacts of tourism activities, including commercial swims with the whales. At present, it is not cost-effective for researchers to manually process and analyze the tens of thousands of underwater images collated after each observation/tourist season, and a large data base of historical non-identified imagery exists. This study reports the first proof of concept for recognizing individual dwarf minke whales using the Deep Learning Convolutional Neural Networks (CNN).The “off-the-shelf” Image net-trained VGG16 CNN was used as the feature-encoder of the per-pixel sematic segmentation Automatic Minke Whale Recognizer (AMWR). The most frequently photographed whale in a sample of 76 individual whales (MW1020) was identified in 179 images out of the total 1320 images provided. Training and image augmentation procedures were developed to compensate for the small number of available images. The trained AMWR achieved 93\% prediction accuracy on the testing subset of 36 positive/MW1020 and 228 negative/not-MW1020 images, where each negative image contained at least one of the other 75 whales. Furthermore on the test subset, AMWR achieved 74\% precision, 80\% recall, and 4\% false-positive rate, making the presented approach comparable or better to other state-of-the-art individual animal recognition results.},
	language = {en},
	journal = {Journal of Geoscience and Environment Protection},
	author = {Konovalov, Dmitry A and Hillcoat, Suzanne and Williams, Genevieve and Birtles, R Alastair and Gardiner, Naomi and Curnock, Matthew I},
	pages = {12},
	file = {Konovalov et al. - Individual Minke Whale Recognition Using Deep Lear.pdf:/Users/b3020111/Zotero/storage/6BYIW2GN/Konovalov et al. - Individual Minke Whale Recognition Using Deep Lear.pdf:application/pdf}
}

@article{franklin_photo-identification_2020,
	title = {Photo-identification of individual {Southern} {Hemisphere} humpback whales ({Megaptera} novaeangliae) using all available natural marks:: managing the potential for misidentification},
	volume = {21},
	issn = {2312-2706},
	shorttitle = {Photo-identification of individual {Southern} {Hemisphere} humpback whales ({Megaptera} novaeangliae) using all available natural marks},
	url = {https://journal.iwc.int/index.php/jcrm/article/view/186},
	doi = {10.47536/jcrm.v21i1.186},
	language = {en},
	number = {1},
	urldate = {2020-10-15},
	journal = {J. Cetacean Res. Manage.},
	author = {Franklin, Wally and Franklin, Trish and Harrison, Peter and Brooks, Lyndon},
	month = oct,
	year = {2020},
	pages = {71--83},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/E7JJPTSI/Franklin et al. - 2020 - Photo-identification of individual Southern Hemisp.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/UKXRIFR6/186.html:text/html}
}

@article{lee_backbone_2020,
	title = {Backbone {Alignment} and {Cascade} {Tiny} {Object} {Detecting} {Techniques} for {Dolphin} {Detection} and {Classification}},
	volume = {advpub},
	doi = {10.1587/transfun.2020EAP1054},
	abstract = {Automatic tracking and classification are essential for studying the behaviors of wild animals. Owing to dynamic far-shooting photos, the occlusion problem, protective coloration, the background noise is irregular interference for designing a computerized algorithm for reducing human labeling resources. Moreover, wild dolphin images are hard-acquired by on-the-spot investigations, which takes a lot of waiting time and hardly sets the fixed camera to automatic monitoring dolphins on the ocean in several days. It is challenging tasks to detect well and classify a dolphin from polluted photos by a single famous deep learning method in a small dataset. Therefore, in this study, we propose a generic Cascade Small Object Detection (CSOD) algorithm for dolphin detection to handle small object problems and develop visualization to backbone based classification (V2BC) for removing noise, highlighting features of dolphin and classifying the name of dolphin. The architecture of CSOD consists of the P-net and the F-net. The P-net uses the crude Yolov3 detector to be a core network to predict all the regions of interest (ROIs) at lower resolution images. Then, the F-net, which is more robust, is applied to capture the ROIs from high-resolution photos to solve single detector problems. Moreover, a visualization to backbone based classification (V2BC) method focuses on extracting significant regions of occluded dolphin and design significant post-processing by referencing the backbone of dolphins to facilitate for classification. Compared to the state of the art methods, including faster-rcnn, yolov3 detection and Alexnet, the Vgg, and the Resnet classification. All experiments show that the proposed algorithm based on CSOD and V2BC has an excellent performance in dolphin detection and classification. Consequently, compared to the related works of classification, the accuracy of the proposed designation is over 14\% higher. Moreover, our proposed CSOD detection system has 42\% higher performance than that of the original Yolov3 architecture.},
	journal = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
	author = {Lee, Yih-Cherng and Hsu, Hung-Wei and Ding, Jian-Jiun and Hou, Wen and Chou, Lien-Shiang and Chang, Ronald Y.},
	year = {2020},
	keywords = {object detection, image classification, deep learning, object segmentation, rotation measurement}
}

@article{lee_backbone_2020-1,
	title = {Backbone {Alignment} and {Cascade} {Tiny} {Object} {Detecting} {Techniques} for {Dolphin} {Detection} and {Classification}},
	volume = {advpub},
	doi = {10.1587/transfun.2020EAP1054},
	abstract = {Automatic tracking and classification are essential for studying the behaviors of wild animals. Owing to dynamic far-shooting photos, the occlusion problem, protective coloration, the background noise is irregular interference for designing a computerized algorithm for reducing human labeling resources. Moreover, wild dolphin images are hard-acquired by on-the-spot investigations, which takes a lot of waiting time and hardly sets the fixed camera to automatic monitoring dolphins on the ocean in several days. It is challenging tasks to detect well and classify a dolphin from polluted photos by a single famous deep learning method in a small dataset. Therefore, in this study, we propose a generic Cascade Small Object Detection (CSOD) algorithm for dolphin detection to handle small object problems and develop visualization to backbone based classification (V2BC) for removing noise, highlighting features of dolphin and classifying the name of dolphin. The architecture of CSOD consists of the P-net and the F-net. The P-net uses the crude Yolov3 detector to be a core network to predict all the regions of interest (ROIs) at lower resolution images. Then, the F-net, which is more robust, is applied to capture the ROIs from high-resolution photos to solve single detector problems. Moreover, a visualization to backbone based classification (V2BC) method focuses on extracting significant regions of occluded dolphin and design significant post-processing by referencing the backbone of dolphins to facilitate for classification. Compared to the state of the art methods, including faster-rcnn, yolov3 detection and Alexnet, the Vgg, and the Resnet classification. All experiments show that the proposed algorithm based on CSOD and V2BC has an excellent performance in dolphin detection and classification. Consequently, compared to the related works of classification, the accuracy of the proposed designation is over 14\% higher. Moreover, our proposed CSOD detection system has 42\% higher performance than that of the original Yolov3 architecture.},
	journal = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
	author = {Lee, Yih-Cherng and Hsu, Hung-Wei and Ding, Jian-Jiun and Hou, Wen and Chou, Lien-Shiang and Chang, Ronald Y.},
	year = {2020},
	keywords = {object detection, image classification, deep learning, object segmentation, rotation measurement},
	file = {J-Stage - Snapshot:/Users/b3020111/Zotero/storage/RT8I68XZ/en.html:text/html;Lee et al. - 2020 - Backbone Alignment and Cascade Tiny Object Detecti.pdf:/Users/b3020111/Zotero/storage/IP34FZ8L/Lee et al. - 2020 - Backbone Alignment and Cascade Tiny Object Detecti.pdf:application/pdf}
}

@inproceedings{hsu_dolphin_2018,
	title = {Dolphin {Recognition} with {Adaptive} {Hybrid} {Saliency} {Detection} for {Deep} {Learning} {Based} on {DenseNet} {Recognition}},
	doi = {10.1109/APCCAS.2018.8605718},
	abstract = {Dolphin identification is important for wildlife conservation. Since identifying dolphins from thousands of images manually takes tremendous time, it is important to develop an automatic dolphin identification algorithm. In this paper, a high accurate deep learning based dolphin identification algorithm is proposed. We presented an advanced approach, called hybrid saliency method, for feature extraction and efficiently integrate several well-known techniques to make dolphins distinguishable. With the proposed techniques, we can avoid the background part (e.g. the sea water) to affect the identification results, which is usually a problem of most convolutional neural network based methods. Simulations show that the proposed algorithm can well identify a dolphin in most cases and it can achieve the accuracy rate of 85\% even if there are 40 dolphins to be distinguished.},
	booktitle = {2018 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems} ({APCCAS})},
	author = {Hsu, Hung-Wei and Lee, Yih-Cherng and Ding, Jian-Jiun and Chang, Ronald Y.},
	month = oct,
	year = {2018},
	keywords = {computer vision, Dolphins, image recognition, Testing, Whales, zoology, learning (artificial intelligence), photo-identification, Training, biology computing, acoustic signal detection, adaptive hybrid saliency detection, automatic dolphin identification algorithm, convolutional neural networks, DenseNet recognition, dolphin recognition, feature extraction, feedforward neural nets, high accurate deep learning, hybrid saliency method, identification results, Image color analysis, image sensors, marine vertebrate, photography, Prediction algorithms, saliency map, Sea surface, tremendous time, wildlife conservation},
	pages = {455--458},
	file = {IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/D3RUMQE2/Hsu et al. - 2018 - Dolphin Recognition with Adaptive Hybrid Saliency .pdf:application/pdf}
}

@article{clapham_automated_nodate,
	title = {Automated facial recognition for wildlife that lack unique markings: {A} deep learning approach for brown bears},
	volume = {n/a},
	copyright = {© 2020 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
	issn = {2045-7758},
	shorttitle = {Automated facial recognition for wildlife that lack unique markings},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.6840},
	doi = {https://doi.org/10.1002/ece3.6840},
	abstract = {Emerging technologies support a new era of applied wildlife research, generating data on scales from individuals to populations. Computer vision methods can process large datasets generated through image-based techniques by automating the detection and identification of species and individuals. With the exception of primates, however, there are no objective visual methods of individual identification for species that lack unique and consistent body markings. We apply deep learning approaches of facial recognition using object detection, landmark detection, a similarity comparison network, and an support vector machine-based classifier to identify individuals in a representative species, the brown bear Ursus arctos. Our open-source application, BearID, detects a bear’s face in an image, rotates and extracts the face, creates an “embedding” for the face, and uses the embedding to classify the individual. We trained and tested the application using labeled images of 132 known individuals collected from British Columbia, Canada, and Alaska, USA. Based on 4,674 images, with an 80/20\% split for training and testing, respectively, we achieved a facial detection (ability to find a face) average precision of 0.98 and an individual classification (ability to identify the individual) accuracy of 83.9\%. BearID and its annotated source code provide a replicable methodology for applying deep learning methods of facial recognition applicable to many other species that lack distinguishing markings. Further analyses of performance should focus on the influence of certain parameters on recognition accuracy, such as age and body size. Combining BearID with camera trapping could facilitate fine-scale behavioral research such as individual spatiotemporal activity patterns, and a cost-effective method of population monitoring through mark–recapture studies, with implications for species and landscape conservation and management. Applications to practical conservation include identifying problem individuals in human–wildlife conflicts, and evaluating the intrapopulation variation in efficacy of conservation strategies, such as wildlife crossings.},
	language = {en},
	number = {n/a},
	urldate = {2020-11-09},
	journal = {Ecology and Evolution},
	author = {Clapham, Melanie and Miller, Ed and Nguyen, Mary and Darimont, Chris T.},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.6840},
	keywords = {deep learning, face recognition, grizzly bear, individual ID, machine learning, wildlife monitoring},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/3RIUER68/Clapham et al. - Automated facial recognition for wildlife that lac.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/KJLUNNCP/ece3.html:text/html}
}

@article{lee_potato_2020,
	title = {Potato {Detection} and {Segmentation} {Based} on {Mask} {R}-{CNN}},
	issn = {2234-1862},
	url = {https://doi.org/10.1007/s42853-020-00063-w},
	doi = {10.1007/s42853-020-00063-w},
	abstract = {Potatoes are similar in color and size to soil and its clods. They are mostly irregular in the shape as well. Therefore, it is not easy to distinguish potatoes from the soil surface background only with machine vision. This study applied Mask R-CNN, one of the object recognition technologies using deep learning to detect potatoes. The size of object in pixel was obtained on individual potato, and they will be used to predict the yield of potatoes.},
	language = {en},
	urldate = {2020-11-13},
	journal = {Journal of Biosystems Engineering},
	author = {Lee, Hyeon-Seung and Shin, Beom-Soo},
	month = oct,
	year = {2020},
	file = {Springer Full Text PDF:/Users/b3020111/Zotero/storage/D677D2ZD/Lee and Shin - 2020 - Potato Detection and Segmentation Based on Mask R-.pdf:application/pdf}
}

@misc{sharma_image_2019,
	title = {Image {Segmentation} {Python} {\textbar} {Implementation} of {Mask} {R}-{CNN} {\textbar} https://analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/},
	url = {https://www.analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/},
	abstract = {An introduction to image segmentation. In this article learn about Mask R-CNN framework for image segmentation and implementation of mask r-cnn in python.},
	urldate = {2020-12-03},
	journal = {Analytics Vidhya},
	author = {Sharma, Pulkit},
	month = jul,
	year = {2019},
	keywords = {instance segmentation, semantic segmentation},
	file = {Snapshot:/Users/b3020111/Zotero/storage/TAILWK7V/computer-vision-implementing-mask-r-cnn-image-segmentation.html:text/html}
}

@article{neven_instance_2019,
	title = {Instance {Segmentation} by {Jointly} {Optimizing} {Spatial} {Embeddings} and {Clustering} {Bandwidth}},
	url = {http://arxiv.org/abs/1906.11109},
	abstract = {Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5{\textbackslash}\% improvement over Mask R-CNN) at more than 10 fps on 2MP images. Code will be available at https://github.com/davyneven/SpatialEmbeddings .},
	urldate = {2020-12-03},
	journal = {arXiv:1906.11109 [cs]},
	author = {Neven, Davy and De Brabandere, Bert and Proesmans, Marc and Van Gool, Luc},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.11109},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/CPXAEZ6F/Neven et al. - 2019 - Instance Segmentation by Jointly Optimizing Spatia.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/HFUVFTJX/1906.html:text/html}
}

@article{neven_instance_2019-1,
	title = {Instance {Segmentation} by {Jointly} {Optimizing} {Spatial} {Embeddings} and {Clustering} {Bandwidth}},
	url = {http://arxiv.org/abs/1906.11109},
	abstract = {Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5{\textbackslash}\% improvement over Mask R-CNN) at more than 10 fps on 2MP images. Code will be available at https://github.com/davyneven/SpatialEmbeddings .},
	urldate = {2020-12-03},
	journal = {arXiv:1906.11109 [cs]},
	author = {Neven, Davy and De Brabandere, Bert and Proesmans, Marc and Van Gool, Luc},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.11109},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/VN5L9724/Neven et al. - 2019 - Instance Segmentation by Jointly Optimizing Spatia.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/84GF37UV/1906.html:text/html}
}

@article{zhou_bottom-up_2019,
	title = {Bottom-up {Object} {Detection} by {Grouping} {Extreme} and {Center} {Points}},
	url = {http://arxiv.org/abs/1901.08043},
	abstract = {With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2\% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9\%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6\% Mask AP.},
	urldate = {2020-12-03},
	journal = {arXiv:1901.08043 [cs]},
	author = {Zhou, Xingyi and Zhuo, Jiacheng and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv: 1901.08043},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/PI4LK8RY/Zhou et al. - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/H8ZKZJPX/1901.html:text/html}
}

@article{xu_explicit_2019,
	title = {Explicit {Shape} {Encoding} for {Real}-{Time} {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1908.04067},
	abstract = {In this paper, we propose a novel top-down instance segmentation framework based on explicit shape encoding, named {\textbackslash}textbf\{ESE-Seg\}. It largely reduces the computational consumption of the instance segmentation by explicitly decoding the multiple object shapes with tensor operations, thus performs the instance segmentation at almost the same speed as the object detection. ESE-Seg is based on a novel shape signature Inner-center Radius (IR), Chebyshev polynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3 outperforms the Mask R-CNN on Pascal VOC 2012 at mAP\${\textasciicircum}r\$@0.5 while 7 times faster.},
	urldate = {2020-12-03},
	journal = {arXiv:1908.04067 [cs]},
	author = {Xu, Wenqiang and Wang, Haiyang and Qi, Fubo and Lu, Cewu},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.04067},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/EUYDL8GS/Xu et al. - 2019 - Explicit Shape Encoding for Real-Time Instance Seg.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/CRLEY4M7/1908.html:text/html}
}

@article{xie_polarmask_2020,
	title = {{PolarMask}: {Single} {Shot} {Instance} {Segmentation} with {Polar} {Representation}},
	shorttitle = {{PolarMask}},
	url = {http://arxiv.org/abs/1909.13226},
	abstract = {In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used as a mask prediction module for instance segmentation, by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9\% in mask mAP with single-model and single-scale training/testing on challenging COCO dataset. For the first time, we demonstrate a much simpler and flexible instance segmentation framework achieving competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation tasks. Code is available at: github.com/xieenze/PolarMask.},
	urldate = {2020-12-03},
	journal = {arXiv:1909.13226 [cs]},
	author = {Xie, Enze and Sun, Peize and Song, Xiaoge and Wang, Wenhai and Liang, Ding and Shen, Chunhua and Luo, Ping},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.13226},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/YSBR8T8P/Xie et al. - 2020 - PolarMask Single Shot Instance Segmentation with .pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/LTXTJWBD/1909.html:text/html}
}

@article{riaz_fouriernet_2020,
	title = {{FourierNet}: {Compact} mask representation for instance segmentation using differentiable shape decoders},
	shorttitle = {{FourierNet}},
	url = {http://arxiv.org/abs/2002.02709},
	abstract = {We present FourierNet, a single shot, anchor-free, fully convolutional instance segmentation method that predicts a shape vector. Consequently, this shape vector is converted into the masks' contour points using a fast numerical transform. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which manages the automatic weight balancing of the shape vector's coefficients. We used the Fourier series as a shape encoder because of its coefficient interpretability and fast implementation. FourierNet shows promising results compared to polygon representation methods, achieving 30.6 mAP on the MS COCO 2017 benchmark. At lower image resolutions, it runs at 26.6 FPS with 24.3 mAP. It reaches 23.3 mAP using just eight parameters to represent the mask (note that at least four parameters are needed for bounding box prediction only). Qualitative analysis shows that suppressing a reasonable proportion of higher frequencies of Fourier series, still generates meaningful masks. These results validate our understanding that lower frequency components hold higher information for the segmentation task, and therefore, we can achieve a compressed representation. Code is available at: github.com/cogsys-tuebingen/FourierNet.},
	urldate = {2020-12-03},
	journal = {arXiv:2002.02709 [cs, eess]},
	author = {Riaz, Hamd ul Moqeet and Benbarka, Nuri and Zell, Andreas},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.02709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/MYWEG2S8/Riaz et al. - 2020 - FourierNet Compact mask representation for instan.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/DDBVEJ76/2002.html:text/html}
}

@misc{bolya_190402689_2019,
	title = {[1904.02689] {YOLACT}: {Real}-time {Instance} {Segmentation}},
	url = {https://arxiv.org/abs/1904.02689},
	urldate = {2020-12-03},
	author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Jae Lee, Yong},
	year = {2019},
	file = {[1904.02689] YOLACT\: Real-time Instance Segmentation:/Users/b3020111/Zotero/storage/68TLV3JW/1904.html:text/html}
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2020-12-03},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/DA3GYMQL/Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/KL6PM7WN/1804.html:text/html}
}

@article{chen_blendmask_2020,
	title = {{BlendMask}: {Top}-{Down} {Meets} {Bottom}-{Up} for {Instance} {Segmentation}},
	shorttitle = {{BlendMask}},
	url = {http://arxiv.org/abs/2001.00309},
	abstract = {Instance segmentation is one of the fundamental vision tasks. Recently, fully convolutional instance segmentation methods have drawn much attention as they are often simpler and more efficient than two-stage approaches like Mask R-CNN. To date, almost all such approaches fall behind the two-stage Mask R-CNN method in mask precision when models have similar computation complexity, leaving great room for improvement. In this work, we achieve improved mask prediction by effectively combining instance-level information with semantic information with lower-level fine-granularity. Our main contribution is a blender module which draws inspiration from both top-down and bottom-up instance segmentation approaches. The proposed BlendMask can effectively predict dense per-pixel position-sensitive instance features with very few channels, and learn attention maps for each instance with merely one convolution layer, thus being fast in inference. BlendMask can be easily incorporated with the state-of-the-art one-stage detection frameworks and outperforms Mask R-CNN under the same training schedule while being 20\% faster. A light-weight version of BlendMask achieves \$ 34.2\% \$ mAP at 25 FPS evaluated on a single 1080Ti GPU card. Because of its simplicity and efficacy, we hope that our BlendMask could serve as a simple yet strong baseline for a wide range of instance-wise prediction tasks. Code is available at https://git.io/AdelaiDet},
	urldate = {2020-12-03},
	journal = {arXiv:2001.00309 [cs]},
	author = {Chen, Hao and Sun, Kunyang and Tian, Zhi and Shen, Chunhua and Huang, Yongming and Yan, Youliang},
	month = apr,
	year = {2020},
	note = {arXiv: 2001.00309},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/Q958CAMQ/Chen et al. - 2020 - BlendMask Top-Down Meets Bottom-Up for Instance S.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/7NFT9NBD/2001.html:text/html}
}

@article{wang_solo_2020,
	title = {{SOLO}: {Segmenting} {Objects} by {Locations}},
	shorttitle = {{SOLO}},
	url = {http://arxiv.org/abs/1912.04488},
	abstract = {We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.},
	urldate = {2020-12-03},
	journal = {arXiv:1912.04488 [cs]},
	author = {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.04488},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/6N3RNKSH/Wang et al. - 2020 - SOLO Segmenting Objects by Locations.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/VCL7S5P8/1912.html:text/html}
}

@article{wang_solov2_2020,
	title = {{SOLOv2}: {Dynamic} and {Fast} {Instance} {Segmentation}},
	shorttitle = {{SOLOv2}},
	url = {http://arxiv.org/abs/2003.10152},
	abstract = {In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1\% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: https://git.io/AdelaiDet},
	urldate = {2020-12-03},
	journal = {arXiv:2003.10152 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},
	month = oct,
	year = {2020},
	note = {arXiv: 2003.10152},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/B3WFVJFR/Wang et al. - 2020 - SOLOv2 Dynamic and Fast Instance Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/K7K9WBEM/2003.html:text/html}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2020-12-03},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556
version: 6},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/AJ495QIF/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/5KI73VRB/1409.html:text/html}
}

@inproceedings{zhao_comparing_2018,
	title = {Comparing {U}-{Net} convolutional network with mask {R}-{CNN} in the performances of pomegranate tree canopy segmentation},
	volume = {10780},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10780/107801J/Comparing-U-Net-convolutional-network-with-mask-R-CNN-in/10.1117/12.2325570.short},
	doi = {10.1117/12.2325570},
	abstract = {In the last decade, technologies of unmanned aerial vehicles (UAVs) and small imaging sensors have achieved a significant improvement in terms of equipment cost, operation cost and image quality. These low-cost platforms provide flexible access to high resolution visible and multispectral images. As a result, many studies have been conducted regarding the applications in precision agriculture, such as water stress detection, nutrient status detection, yield prediction, etc. Different from traditional satellite low-resolution images, high-resolution UAVbased images allow much more freedom in image post-processing. For example, the very first procedure in post-processing is pixel classification, or image segmentation for extracting region of interest(ROI). With the very high resolution, it becomes possible to classify pixels from a UAV-based image, yet it is still a challenge to conduct pixel classification using traditional remote sensing features such as vegetation indices (VIs), especially considering various changes during the growing season such as light intensity, crop size, crop color etc. Thanks to the development of deep learning technologies, it provides a general framework to solve this problem. In this study, we proposed to use deep learning methods to conduct image segmentation. We created our data set of pomegranate trees by flying an off-shelf commercial camera at 30 meters above the ground around noon, during the whole growing season from the beginning of April to the middle of October 2017. We then trained and tested two convolutional network based methods U-Net and Mask R-CNN using this data set. Finally, we compared their performances with our dataset aerial images of pomegranate trees. [Tiebiao- add a sentence to summarize the findings and their implications to precision agriculture]},
	urldate = {2020-12-03},
	booktitle = {Multispectral, {Hyperspectral}, and {Ultraspectral} {Remote} {Sensing} {Technology}, {Techniques} and {Applications} {VII}},
	publisher = {International Society for Optics and Photonics},
	author = {Zhao, Tiebiao and Yang, Yonghuan and Niu, Haoyu and Wang, Dong and Chen, YangQuan},
	month = oct,
	year = {2018},
	pages = {107801J},
	file = {Snapshot:/Users/b3020111/Zotero/storage/C9RNN8AM/12.2325570.html:text/html}
}

@inproceedings{nie_inshore_2018,
	title = {Inshore {Ship} {Detection} {Based} on {Mask} {R}-{CNN}},
	doi = {10.1109/IGARSS.2018.8519123},
	abstract = {Inshore ship detection is a popular research domain for optical remote sensing image understanding with many applications in harbor management. However, recent approaches on inshore ship detection depend heavily on hand-crafted features, which need a complicated procedure. In this paper, we propose a new method to achieve inshore ship detection based on Mask R-CNN. We introduce Soft-Non-Maximum Suppression (Soft-NMS) into our framework to improve the robustness to nearby inshore ships. Both battleships and merchantships can be detected in our framework. Furthermore, our framework can also obtain the binary masks of inshore ships. Experimental results on a dataset collected from Google Earth have quantitatively and qualitatively demonstrated the effectiveness of our approach.},
	booktitle = {{IGARSS} 2018 - 2018 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Nie, S. and Jiang, Z. and Zhang, H. and Cai, B. and Yao, Y.},
	month = jul,
	year = {2018},
	note = {ISSN: 2153-7003},
	keywords = {Object detection, object detection, feature extraction, binary masks, Feature extraction, geophysical image processing, Google Earth, hand-crafted features, harbor management, Image segmentation, inshore ship, inshore ship detection, Marine vehicles, mask R-CNN, Mask R-CNN, nearby inshore ships, oceanographic techniques, optical remote sensing image understanding, Proposals, remote sensing, Remote sensing, remote sensing images, Shape, ships, soft-NMS, soft-nonmaximum suppression},
	pages = {693--696},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/8429LIIC/8519123.html:text/html}
}

@article{qiao_cattle_2019,
	title = {Cattle segmentation and contour extraction based on {Mask} {R}-{CNN} for precision livestock farming},
	volume = {165},
	issn = {0168-1699},
	url = {http://www.sciencedirect.com/science/article/pii/S0168169919304077},
	doi = {10.1016/j.compag.2019.104958},
	abstract = {In precision livestock farming, computer vision based approaches have been widely used to obtain individual cattle health and welfare information such as body condition score, live weight, activity behaviours. For this, precisely segmenting each cattle image from its background is a prerequisite, which is an important step towards obtaining real-time individual cattle information. In this paper, an instance segmentation approach based on a Mask R-CNN deep learning framework is proposed to solve cattle instance segmentation and contour extraction problems in a real feedlot environment. The proposed approach consists of the following steps: key frame extraction (detect the huge cattle motion frames), image enhancement (reduce the illumination and shadow influence), cattle segmentation and body contour extraction. We trained and tested the proposed approach on a challenging cattle image dataset. According to the experimental results, the proposed approach can render fairly desirable cattle segmentation performance with 0.92 Mean Pixel Accuracy (MPA) and achieve contour extraction with an Average Distance Error (ADE) of 33.56 pixel, which is better than that of the state-of-the-art SharpMask and DeepMask instance segmentation methods.},
	language = {en},
	urldate = {2020-12-03},
	journal = {Computers and Electronics in Agriculture},
	author = {Qiao, Yongliang and Truman, Matthew and Sukkarieh, Salah},
	month = oct,
	year = {2019},
	keywords = {Mask R-CNN, Cattle contour, Deep learning, Instance segmentation, Precision livestock farming},
	pages = {104958},
	file = {ScienceDirect Full Text PDF:/Users/b3020111/Zotero/storage/PH5XZFR2/Qiao et al. - 2019 - Cattle segmentation and contour extraction based o.pdf:application/pdf;ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/ZPUZEVSF/S0168169919304077.html:text/html}
}

@article{burke_deblending_2019,
	title = {Deblending and classifying astronomical sources with {Mask} {R}-{CNN} deep learning},
	volume = {490},
	issn = {0035-8711},
	url = {https://academic.oup.com/mnras/article/490/3/3952/5585422},
	doi = {10.1093/mnras/stz2845},
	abstract = {ABSTRACT. We apply a new deep learning technique to detect, classify, and deblend sources in multiband astronomical images. We train and evaluate the performanc},
	language = {en},
	number = {3},
	urldate = {2020-12-03},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Burke, Colin J. and Aleo, Patrick D. and Chen, Yu-Ching and Liu, Xin and Peterson, John R. and Sembroski, Glenn H. and Lin, Joshua Yao-Yu},
	month = dec,
	year = {2019},
	note = {Publisher: Oxford Academic},
	pages = {3952--3965},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/UK34TRF7/Burke et al. - 2019 - Deblending and classifying astronomical sources wi.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/J6DQJE6W/5585422.html:text/html}
}

@inproceedings{anantharaman_utilizing_2018,
	title = {Utilizing {Mask} {R}-{CNN} for {Detection} and {Segmentation} of {Oral} {Diseases}},
	doi = {10.1109/BIBM.2018.8621112},
	abstract = {In this paper, we demonstrate the application of Mask-RCNN, the state-of-the-art convolutional neural network algorithm for object detection and segmentation to the oral pathology domain. Mask-RCNN was originally developed for object detection, and object instance segmentation of natural images. With this experiment, we show that Mask-RCNN can also be used in a very specialized area such as oral pathology. While the number of oral diseases are numerous and varied in the form of Thrush, Leukoplakia, Lichenplanus, etc., we limited our scope to the detection and instance segmentation of two of the most commonly occurring conditions, herpes labialis (commonly referred to as “cold sore“ and aphthous ulcer (commonly referred to as “canker sore“ This paper aims at detecting and segmenting cold sores and canker sores only. As always, no computer based detection system can be 100\% reliable. An accurate diagnosis by a trained health care professional is necessary since several conditions of the mouth including oral cancer may mimic canker sores.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Anantharaman, R. and Velazquez, M. and Lee, Y.},
	month = dec,
	year = {2018},
	keywords = {Object detection, object detection, image segmentation, learning (artificial intelligence), Training, Feature extraction, Image segmentation, aphthous ulcer, cancer, canker sore, cold sore ulcer, computer based detection system, convolutional neural nets, Convolutional neural networks, detecting segmenting cold sores, diseases, Diseases, health care, Mask-RCNN, medical image processing, Mouth, Object Detection, object instance segmentation, Object Segmentation, oral cancer, Oral disease, oral diseases, oral pathology domain, patient diagnosis, patient treatment, state-of-the-art convolutional neural network algorithm},
	pages = {2197--2204},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/8QPX8RRR/8621112.html:text/html}
}

@article{chiao_detection_2019,
	title = {Detection and classification the breast tumors using mask {R}-{CNN} on sonograms},
	volume = {98},
	issn = {0025-7974},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6531264/},
	doi = {10.1097/MD.0000000000015200},
	abstract = {Breast cancer is one of the most harmful diseases for women with the highest morbidity. An efficient way to decrease its mortality is to diagnose cancer earlier by screening. Clinically, the best approach of screening for Asian women is ultrasound images combined with biopsies. However, biopsy is invasive and it gets incomprehensive information of the lesion. The aim of this study is to build a model for automatic detection, segmentation, and classification of breast lesions with ultrasound images. Based on deep learning, a technique using Mask regions with convolutional neural network was developed for lesion detection and differentiation between benign and malignant. The mean average precision was 0.75 for the detection and segmentation. The overall accuracy of benign/malignant classification was 85\%. The proposed method provides a comprehensive and noninvasive way to detect and classify breast lesions.},
	number = {19},
	urldate = {2020-12-03},
	journal = {Medicine},
	author = {Chiao, Jui-Ying and Chen, Kuan-Yung and Liao, Ken Ying-Kai and Hsieh, Po-Hsin and Zhang, Geoffrey and Huang, Tzung-Chi},
	month = may,
	year = {2019},
	pmid = {31083152},
	pmcid = {PMC6531264},
	file = {PubMed Central Full Text PDF:/Users/b3020111/Zotero/storage/TWDL3543/Chiao et al. - 2019 - Detection and classification the breast tumors usi.pdf:application/pdf}
}

@inproceedings{rohit_malhotra_autonomous_2018,
	title = {Autonomous {Detection} of {Disruptions} in the {Intensive} {Care} {Unit} {Using} {Deep} {Mask} {R}-{CNN}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018_workshops/w36/html/Malhotra_Autonomous_Detection_of_CVPR_2018_paper.html},
	urldate = {2020-12-03},
	author = {Rohit Malhotra, Kumar and Davoudi, Anis and Siegel, Scott and Bihorac, Azra and Rashidi, Parisa},
	year = {2018},
	pages = {1863--1865},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/ZWQQ3V3W/Rohit Malhotra et al. - 2018 - Autonomous Detection of Disruptions in the Intensi.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/G2IS7HTR/Malhotra_Autonomous_Detection_of_CVPR_2018_paper.html:text/html}
}

@inproceedings{pobar_detection_2019,
	title = {Detection of the leading player in handball scenes using {Mask} {R}-{CNN} and {STIPS}},
	volume = {11041},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11041/110411V/Detection-of-the-leading-player-in-handball-scenes-using-Mask/10.1117/12.2522668.short},
	doi = {10.1117/12.2522668},
	abstract = {In team sports scenes, recorded during training and lessons, it is common to have many players on the court, each with his own ball performing different actions. Our goal is to detect all players in the handball court and determine the leading player who performs the given handball technique such as a shooting at the goal, catching a ball or dribbling. This is a very challenging task for which, apart from an accurate object detector that is able to deal with cluttered scenes with many objects, partially occluded and with bad illumination, additional information is needed to determine the leading player. Therefore, we propose a leading player detector method combining the Mask R-CNN object detector and spatiotemporal interest points, referred to as MR-CNN+STIPs. The performance of the proposed leading player detector is evaluated on a custom sports video dataset acquired during handball training lessons. The performance of the detector in different conditions will be discussed.},
	urldate = {2020-12-03},
	booktitle = {Eleventh {International} {Conference} on {Machine} {Vision} ({ICMV} 2018)},
	publisher = {International Society for Optics and Photonics},
	author = {Pobar, M. and Ivašić-Kos, Marina},
	month = mar,
	year = {2019},
	pages = {110411V},
	file = {Snapshot:/Users/b3020111/Zotero/storage/N52I27HR/12.2522668.html:text/html}
}

@inproceedings{buric_ball_2018,
	title = {Ball {Detection} {Using} {Yolo} and {Mask} {R}-{CNN}},
	doi = {10.1109/CSCI46756.2018.00068},
	abstract = {Many computer vision applications rely on accurate and fast object detection, and in our case, ball detection serves as a prerequisite for action recognition in handball scenes. We compare the performance of two of the state-of-the-art convolutional neural network-based object detectors for the task of ball detection in non-staged, real-world conditions. The comparison is performed in terms of speed and accuracy measures on a dataset comprising custom handball footage and a sample of images obtained from the Internet. The performance of the models is compared with and without additional training with examples from our dataset.},
	booktitle = {2018 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Buric, M. and Pobar, M. and Ivasic-Kos, M.},
	month = dec,
	year = {2018},
	keywords = {computer vision, Object detection, object detection, Task analysis, Training, object recognition, Feature extraction, Image segmentation, convolutional neural nets, action recognition, ball detection, Computer architecture, computer vision applications, convolutional neural network-based object detectors, custom handball footage, dataset, Detectors, handball scenes, Internet, R-CNN},
	pages = {319--323},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/TKZ2ISV5/8947818.html:text/html}
}

@inproceedings{liu_segmentation_2018,
	title = {Segmentation of {Lung} {Nodule} in {CT} {Images} {Based} on {Mask} {R}-{CNN}},
	doi = {10.1109/ICAwST.2018.8517248},
	abstract = {Due to the low-quality of CT images, the lack of annotated data, and the complex shapes of lung nodules, existing methods for lung nodules detection only predict the center of the nodule, whereas the nodule size is a very important diagnostic criteria but is neglected. In this paper, we employed the powerful object detection neural network “Mask R-CNN” for lung nodule segmentation, which provides contour information. Because of the imbalance between positive and negative samples, we trained classification networks based on block. We selected the classification network with the hightest accuracy. The selected classification network was used as the backbone of the image segmentation network-Mask R-CNN, which performs excellently on natural images. Lastly, Mask R-CNN model trained on the COCO data set was fine-tuned to segment pulmonary nodules. The model was tested on the LIDC-IDRI dataset.},
	booktitle = {2018 9th {International} {Conference} on {Awareness} {Science} and {Technology} ({iCAST})},
	author = {Liu, M. and Dong, J. and Dong, X. and Yu, H. and Qi, L.},
	month = sep,
	year = {2018},
	note = {ISSN: 2325-5994},
	keywords = {object detection, image segmentation, Task analysis, deep learning, feature extraction, Image segmentation, Mask R-CNN, cancer, Convolutional neural networks, medical image processing, annotated data, Cancer, COCO dataset, Computed tomography, computerised tomography, CT images, diagnostic criteria, image segmentation network, LIDC-IDRI, LIDC-IDRI dataset, low-quality, lung, Lung, lung nodule segmentation, lung nodules detection, mask R-CNN model, nodule size, object detection neural network, recurrent neural nets, segment pulmonary nodules, selected classification network, Solid modeling},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/b3020111/Zotero/storage/NYE66QZN/8517248.html:text/html;Submitted Version:/Users/b3020111/Zotero/storage/E9JV7XL9/Liu et al. - 2018 - Segmentation of Lung Nodule in CT Images Based on .pdf:application/pdf}
}

@article{chu_deepapple_2020,
	title = {{DeepApple}: {Deep} {Learning}-based {Apple} {Detection} using a {Suppression} {Mask} {R}-{CNN}},
	shorttitle = {{DeepApple}},
	url = {http://arxiv.org/abs/2010.09870},
	abstract = {Robotic apple harvesting has received much research attention in the past few years due to growing shortage and rising cost in labor. One key enabling technology towards automated harvesting is accurate and robust apple detection, which poses great challenges as a result of the complex orchard environment that involves varying lighting conditions and foliage/branch occlusions. This letter reports on the development of a novel deep learning-based apple detection framework named DeepApple. Specifically, we first collect a comprehensive apple orchard dataset for 'Gala' and 'Blondee' apples, using a color camera, under different lighting conditions (sunny vs. overcast and front lighting vs. back lighting). We then develop a novel suppression Mask R-CNN for apple detection, in which a suppression branch is added to the standard Mask R-CNN to suppress non-apple features generated by the original network. Comprehensive evaluations are performed, which show that the developed suppression Mask R-CNN network outperforms state-of-the-art models with a higher F1-score of 0.905 and a detection time of 0.25 second per frame on a standard desktop computer.},
	urldate = {2020-12-03},
	journal = {arXiv:2010.09870 [cs]},
	author = {Chu, Pengyu and Li, Zhaojian and Lammers, Kyle and Lu, Renfu and Liu, Xiaoming},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/DC3A75JF/Chu et al. - 2020 - DeepApple Deep Learning-based Apple Detection usi.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/7Z6MLTUA/2010.html:text/html}
}

@inproceedings{nguyen_hand_2018,
	title = {Hand segmentation under different viewpoints by combination of {Mask} {R}-{CNN} with tracking},
	doi = {10.1109/ACDT.2018.8593130},
	abstract = {This paper presents a new method for hand segmentation from images and video. The method based mainly on an advanced technique for instance segmentation (Mask RCNN) which has been shown very efficient in segmentation task on COCO dataset. However, Mask R-CNN has some limitations. It works on still images, so cannot explore temporal information of the object of interest such as dynamic hand gestures. Second Mask R-CNN usually fails to detect object suffered from motion blur at low resolution as hand. Our proposed method improves Mask R-CNN by integrating a Mean Shift tracker that tracks hands in consecutive frames and removes false alarms. We have also trained another model of Mask R-CNN on cropped regions extended from hand centers to obtain a better accuracy of segmentation. We have evaluated both methods on a self-constructed multi-view dataset of hand gestures and show how robust these methods are to view point changes. Experimental results showed that our method achieved better performance than the original Mask R-CNN under different viewpoints.},
	booktitle = {2018 5th {Asian} {Conference} on {Defense} {Technology} ({ACDT})},
	author = {Nguyen, D. and Le, T. and Tran, T. and Vu, H. and Le, T. and Doan, H.},
	month = oct,
	year = {2018},
	keywords = {computer vision, object detection, image segmentation, learning (artificial intelligence), deep learning, feature extraction, feedforward neural nets, instance segmentation, Feature extraction, Image segmentation, Detectors, dynamic hand gestures, gesture recognition, hand centers, hand segmentation, image motion analysis, Image resolution, Mask RCNN, mean shift tracker, motion blur, neural network, Neural networks, segmentation task, Skin, tracking, Tracking, video signal processing},
	pages = {14--20}
}

@article{temitope_yekeen_novel_2020,
	title = {A novel deep learning instance segmentation model for automated marine oil spill detection},
	volume = {167},
	issn = {0924-2716},
	url = {http://www.sciencedirect.com/science/article/pii/S0924271620301982},
	doi = {10.1016/j.isprsjprs.2020.07.011},
	abstract = {The visual similarity of oil slick and other elements, known as look-alike, affects the reliability of synthetic aperture radar (SAR) images for marine oil spill detection. So far, detection and discrimination of oil spill and look-alike are still limited to the use of traditional machine learning algorithms and semantic segmentation deep learning models with limited accuracy. Thus, this study developed a novel deep learning oil spill detection model using computer vision instance segmentation Mask-Region-based Convolutional Neural Network (Mask R-CNN) model. The model training was conducted using transfer learning on the ResNet 101 on COCO as backbone in combination with Feature Pyramid Network (FPN) architecture for feature extraction at 30 epochs with 0.001 learning rate. Testing of the model was conducted using the least training and validation loss value on the withheld testing images. The model’s performance was evaluated using precision, recall, specificity, IoU, F1-measure and overall accuracy values. Ship detection and segmentation had the highest performance with overall accuracy of 98.3\%. The model equally showed a higher accuracy for oil spill and look-alike detection and segmentation although oil spill detection outperformed look-alike with overall accuracy values of 96.6\% and 91.0\% respectively. The study concluded that the deep learning instance segmentation model performs better than conventional machine learning models and deep learning semantic segmentation models in detection and segmentation.},
	language = {en},
	urldate = {2020-12-03},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Temitope Yekeen, Shamsudeen and Balogun, Abdul‐Lateef and Wan Yusof, Khamaruzaman B.},
	month = sep,
	year = {2020},
	keywords = {Mask R-CNN, Deep learning, Instance segmentation, Detection, Oil spill, SAR},
	pages = {190--200},
	file = {ScienceDirect Full Text PDF:/Users/b3020111/Zotero/storage/D4LDBUY7/Temitope Yekeen et al. - 2020 - A novel deep learning instance segmentation model .pdf:application/pdf}
}

@article{huang_faster_2019,
	title = {Faster {R}-{CNN} for marine organisms detection and recognition using data augmentation},
	volume = {337},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219301274},
	doi = {10.1016/j.neucom.2019.01.084},
	abstract = {Recently, Faster Region-based Convolutional Neural Network (Faster R-CNN) has achieved marvelous accomplishment in object detection and recognition. In this paper, Faster R-CNN is applied to marine organisms detection and recognition. However, the training of Faster R-CNN requires a mass of labeled samples which are difficult to obtain for marine organisms. Therefore, three data augmentation methods dedicated to underwater-imaging are proposed. Specifically, the inverse process of underwater image restoration is used to simulate different marine turbulence environments. Perspective transformation is proposed to simulate different views of camera shooting. Illumination synthesis is used to simulate different marine uneven illuminating environments. The performance of each data augmentation method, together with previous frequently used data augmentation methods are evaluated by Faster R-CNN on the real-world underwater dataset, which validate the effectiveness of the proposed methods for marine organisms detection and recognition.},
	language = {en},
	urldate = {2020-12-03},
	journal = {Neurocomputing},
	author = {Huang, Hai and Zhou, Hao and Yang, Xu and Zhang, Lu and Qi, Lu and Zang, Ai-Yun},
	month = apr,
	year = {2019},
	keywords = {Data augmentation, Detection and recognition, Faster R-CNN, Marine organisms, Underwater-imaging},
	pages = {372--384},
	file = {ScienceDirect Full Text PDF:/Users/b3020111/Zotero/storage/X9ZURVZG/Huang et al. - 2019 - Faster R-CNN for marine organisms detection and re.pdf:application/pdf}
}

@article{nie_attention_2020,
	title = {Attention {Mask} {R}-{CNN} for {Ship} {Detection} and {Segmentation} {From} {Remote} {Sensing} {Images}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2964540},
	abstract = {In recent years, ship detection in satellite remote sensing images has become an important research topic. Most existing methods detect ships by using a rectangular bounding box but do not perform segmentation down to the pixel level. This paper proposes a ship detection and segmentation method based on an improved Mask R-CNN model. Our proposed method can accurately detect and segment ships at the pixel level. By adding a bottom-up structure to the FPN structure of Mask R-CNN, the path between the lower layers and the topmost layer is shortened, allowing the lower layer features to be more effectively utilized at the top layer. In the bottom-up structure, we use channel-wise attention to assign weights in each channel and use the spatial attention mechanism to assign a corresponding weight at each pixel in the feature maps. This allows the feature maps to respond better to the target's features. Using our method, the detection and segmentation mAPs increased from 70.6\% and 62.0\% to 76.1\% and 65.8\%, respectively.},
	journal = {IEEE Access},
	author = {Nie, X. and Duan, M. and Ding, H. and Hu, B. and Wong, E. K.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Object detection, object detection, image segmentation, Computer vision, image classification, object segmentation, Feature extraction, geophysical image processing, Image segmentation, Marine vehicles, remote sensing, Remote sensing, ships, Deep learning, convolutional neural nets, channel-wise attention, feature maps, improved mask R-CNN model, lower layer features, pixel level, satellite remote sensing images, Satellites, segmentation method, ship detection},
	pages = {9325--9334},
	file = {IEEE Xplore Full Text PDF:/Users/b3020111/Zotero/storage/ITVM7QQJ/Nie et al. - 2020 - Attention Mask R-CNN for Ship Detection and Segmen.pdf:application/pdf}
}

@article{hong_trashcan_2020,
	title = {{TrashCan}: {A} {Semantically}-{Segmented} {Dataset} towards {Visual} {Detection} of {Marine} {Debris}},
	shorttitle = {{TrashCan}},
	url = {http://arxiv.org/abs/2007.08097},
	abstract = {This paper presents TrashCan, a large dataset comprised of images of underwater trash collected from a variety of sources, annotated both using bounding boxes and segmentation labels, for development of robust detectors of marine debris. The dataset has two versions, TrashCan-Material and TrashCan-Instance, corresponding to different object class configurations. The eventual goal is to develop efficient and accurate trash detection methods suitable for onboard robot deployment. Along with information about the construction and sourcing of the TrashCan dataset, we present initial results of instance segmentation from Mask R-CNN and object detection from Faster R-CNN. These do not represent the best possible detection results but provides an initial baseline for future work in instance segmentation and object detection on the TrashCan dataset.},
	urldate = {2020-12-03},
	journal = {arXiv:2007.08097 [cs]},
	author = {Hong, Jungseok and Fulton, Michael and Sattar, Junaed},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08097},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/46FENFGR/Hong et al. - 2020 - TrashCan A Semantically-Segmented Dataset towards.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/SUQTT2NA/2007.html:text/html}
}

@misc{wu_detectron2_2020,
	title = {Detectron2},
	copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
	url = {https://github.com/facebookresearch/detectron2},
	abstract = {Detectron2 is FAIR's next-generation platform for object detection and segmentation.},
	urldate = {2020-12-03},
	publisher = {Facebook Research},
	author = {Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
	month = dec,
	year = {2020},
	note = {original-date: 2019-09-05T21:30:20Z}
}

@article{foret_sharpness-aware_2020,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {http://arxiv.org/abs/2010.01412},
	abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization -- including a generalization bound that we prove here -- we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-\{10, 100\}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.},
	urldate = {2021-01-04},
	journal = {arXiv:2010.01412 [cs, stat]},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.01412},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/8PEAU7MP/Foret et al. - 2020 - Sharpness-Aware Minimization for Efficiently Impro.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/8GS7R49A/2010.html:text/html}
}

@article{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	language = {en},
	author = {Krizhevsky, Alex},
	year = {2009},
	pages = {60},
	file = {Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:/Users/b3020111/Zotero/storage/MLP5PYV9/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:application/pdf}
}

@article{maji_fine-grained_2013,
	title = {Fine-{Grained} {Visual} {Classification} of {Aircraft}},
	url = {http://arxiv.org/abs/1306.5151},
	abstract = {This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding.},
	urldate = {2021-01-04},
	journal = {arXiv:1306.5151 [cs]},
	author = {Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.5151},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/2DKUXG54/Maji et al. - 2013 - Fine-Grained Visual Classification of Aircraft.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/79ZBE6K7/1306.html:text/html}
}

@article{swanson_snapshot_2015,
	title = {Snapshot {Serengeti}, high-frequency annotated camera trap images of 40 mammalian species in an {African} savanna},
	volume = {2},
	copyright = {2015 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201526},
	doi = {10.1038/sdata.2015.26},
	abstract = {Camera traps can be used to address large-scale questions in community ecology by providing systematic data on an array of wide-ranging species. We deployed 225 camera traps across 1,125 km2 in Serengeti National Park, Tanzania, to evaluate spatial and temporal inter-species dynamics. The cameras have operated continuously since 2010 and had accumulated 99,241 camera-trap days and produced 1.2 million sets of pictures by 2013. Members of the general public classified the images via the citizen-science website www.snapshotserengeti.org.Multiple users viewed each image and recorded the species, number of individuals, associated behaviours, and presence of young. Over 28,000 registered users contributed 10.8 million classifications. We applied a simple algorithm to aggregate these individual classifications into a final ‘consensus’ dataset, yielding a final classification for each image and a measure of agreement among individual answers. The consensus classifications and raw imagery provide an unparalleled opportunity to investigate multi-species dynamics in an intact ecosystem and a valuable resource for machine-learning and computer-vision research.},
	language = {en},
	number = {1},
	urldate = {2021-01-05},
	journal = {Scientific Data},
	author = {Swanson, Alexandra and Kosmala, Margaret and Lintott, Chris and Simpson, Robert and Smith, Arfon and Packer, Craig},
	month = jun,
	year = {2015},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {150026},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/5NZQ2N83/Swanson et al. - 2015 - Snapshot Serengeti, high-frequency annotated camer.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/JESA7QAS/sdata201526.html:text/html}
}

@article{tabak_machine_2019,
	title = {Machine learning to classify animal species in camera trap images: {Applications} in ecology},
	volume = {10},
	copyright = {© 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society},
	issn = {2041-210X},
	shorttitle = {Machine learning to classify animal species in camera trap images},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13120},
	doi = {https://doi.org/10.1111/2041-210X.13120},
	abstract = {Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
	language = {en},
	number = {4},
	urldate = {2021-01-05},
	journal = {Methods in Ecology and Evolution},
	author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Salvo, Paul A. Di and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
	year = {2019},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13120},
	keywords = {image classification, machine learning, remote sensing, artificial intelligence, camera trap, convolutional neural network, deep neural networks, r package},
	pages = {585--590},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/V6Y6TR3S/Tabak et al. - 2019 - Machine learning to classify animal species in cam.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/976VRBAX/2041-210X.html:text/html}
}

@article{norouzzadeh_automatically_2018,
	title = {Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/25/E5716},
	doi = {10.1073/pnas.1719367115},
	abstract = {Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {\textgreater}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {\textgreater}8.4 y (i.e., {\textgreater}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
	language = {en},
	number = {25},
	urldate = {2021-01-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
	month = jun,
	year = {2018},
	pmid = {29871948},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {deep learning, artificial intelligence, deep neural networks, camera-trap images, wildlife ecology},
	pages = {E5716--E5725},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/EESET394/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describin.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/4M7ZDYL5/E5716.html:text/html}
}

@article{willi_identifying_2019,
	title = {Identifying animal species in camera trap images using deep learning and citizen science},
	volume = {10},
	copyright = {© 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society},
	issn = {2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13099},
	doi = {https://doi.org/10.1111/2041-210X.13099},
	abstract = {Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2\% and 98.0\%, whereas accuracies for identifying specific species were between 88.7\% and 92.7\%. Transferring information from CNNs trained on large datasets (“transfer-learning”) was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3\%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43\% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.},
	language = {en},
	number = {1},
	urldate = {2021-01-05},
	journal = {Methods in Ecology and Evolution},
	author = {Willi, Marco and Pitman, Ross T. and Cardoso, Anabelle W. and Locke, Christina and Swanson, Alexandra and Boyer, Amy and Veldthuis, Marten and Fortson, Lucy},
	year = {2019},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13099},
	keywords = {deep learning, convolutional neural networks, machine learning, camera trap, animal identification, citizen science},
	pages = {80--91},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/G8NR8BHC/Willi et al. - 2019 - Identifying animal species in camera trap images u.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/XHU2HS3R/2041-210X.html:text/html}
}

@article{beery_efficient_2019,
	title = {Efficient {Pipeline} for {Camera} {Trap} {Image} {Review}},
	url = {http://arxiv.org/abs/1907.06772},
	abstract = {Biologists all over the world use camera traps to monitor biodiversity and wildlife population density. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but it has proven difficult to to apply models trained in one region to images collected in different geographic areas. In some cases, accuracy falls off catastrophically in new region, due to both changes in background and the presence of previously-unseen species. We propose a pipeline that takes advantage of a pre-trained general animal detector and a smaller set of labeled images to train a classification model that can efficiently achieve accurate results in a new region.},
	urldate = {2021-01-05},
	journal = {arXiv:1907.06772 [cs]},
	author = {Beery, Sara and Morris, Dan and Yang, Siyu},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06772},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/UDDFUMLZ/Beery et al. - 2019 - Efficient Pipeline for Camera Trap Image Review.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/S7NSR9IC/1907.html:text/html}
}

@article{norouzzadeh_deep_2019,
	title = {A deep active learning system for species identification and counting in camera trap images},
	url = {http://arxiv.org/abs/1910.09716},
	abstract = {Biodiversity conservation depends on accurate, up-to-date information about wildlife population distributions. Motion-activated cameras, also known as camera traps, are a critical tool for population surveys, as they are cheap and non-intrusive. However, extracting useful information from camera trap images is a cumbersome process: a typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical information is often lost due to resource limitations, and critical conservation questions may be answered too slowly to support decision-making. Computer vision is poised to dramatically increase efficiency in image-based biodiversity surveys, and recent studies have harnessed deep learning techniques for automatic information extraction from camera trap images. However, the accuracy of results depends on the amount, quality, and diversity of the data available to train models, and the literature has focused on projects with millions of relevant, labeled training images. Many camera trap projects do not have a large set of labeled images and hence cannot benefit from existing machine learning techniques. Furthermore, even projects that do have labeled data from similar ecosystems have struggled to adopt deep learning methods because image classification models overfit to specific image backgrounds (i.e., camera locations). In this paper, we focus not on automating the labeling of camera trap images, but on accelerating this process. We combine the power of machine intelligence and human intelligence to build a scalable, fast, and accurate active learning system to minimize the manual work required to identify and count animals in camera trap images. Our proposed scheme can match the state of the art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labeling effort by over 99.5\%.},
	urldate = {2021-01-05},
	journal = {arXiv:1910.09716 [cs, eess, stat]},
	author = {Norouzzadeh, Mohammad Sadegh and Morris, Dan and Beery, Sara and Joshi, Neel and Jojic, Nebojsa and Clune, Jeff},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.09716},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/9JRZNE8U/Norouzzadeh et al. - 2019 - A deep active learning system for species identifi.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/CKHKIVY8/1910.html:text/html}
}

@article{rowcliffe_estimating_2008,
	title = {Estimating {Animal} {Density} {Using} {Camera} {Traps} without the {Need} for {Individual} {Recognition}},
	volume = {45},
	issn = {0021-8901},
	url = {https://www.jstor.org/stable/20144086},
	abstract = {1. Density estimation is of fundamental importance in wildlife management. The use of camera traps to estimate animal density has so far been restricted to capture-recapture analysis of species with individually identifiable markings. This study developed a method that eliminates the requirement for individual recognition of animals by modelling the underlying process of contact between animals and cameras. 2. The model provides a factor that linearly scales trapping rate with density, depending on two key biological variables (average animal group size and day range) and two characteristics of the camera sensor (distance and angle within which it detects animals). 3. We tested the approach in an enclosed animal park with known abundances of four species, obtaining accurate estimates in three out of four cases. Inaccuracy in the fourth species was because of biased placement of cameras with respect to the distribution of this species. 4. Synthesis and applications. Subject to unbiased camera placement and accurate measurement of model parameters, this method opens the possibility of reduced labour costs for estimating wildlife density and may make estimation possible where it has not been previously. We provide guidelines on the trapping effort required to obtain reasonably precise estimates.},
	number = {4},
	urldate = {2021-01-05},
	journal = {Journal of Applied Ecology},
	author = {Rowcliffe, J. Marcus and Field, Juliet and Turvey, Samuel T. and Carbone, Chris},
	year = {2008},
	note = {Publisher: [British Ecological Society, Wiley]},
	pages = {1228--1236}
}

@article{centelleghe_use_2020,
	title = {The use of {Unmanned} {Aerial} {Vehicles} ({UAVs}) to sample the blow microbiome of small cetaceans},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235537},
	doi = {10.1371/journal.pone.0235537},
	abstract = {Recent studies describe the use of UAVs in collecting blow samples from large whales to analyze the microbial and viral community in exhaled air. Unfortunately, attempts to collect blow from small cetaceans have not been successful due to their swimming and diving behavior. In order to overcome these limitations, in this study we investigated the application of a specific sampling tool attached to a UAV to analyze the blow from small cetaceans and their respiratory microbiome. Preliminary trials to set up the sampling tool were conducted on a group of 6 bottlenose dolphins (Tursiops truncatus) under human care, housed at Acquario di Genova, with approximately 1 meter distance between the blowing animal and the tool to obtain suitable samples. The same sampling kit, suspended via a 2 meter rope assembled on a waterproof UAV, flying 3 meters above the animals, was used to sample the blows of 5 wild bottlenose dolphins in the Gulf of Ambracia (Greece) and a sperm whale (Physeter macrocephalus) in the southern Tyrrhenian Sea (Italy), to investigate whether this experimental assembly also works for large whale sampling. In order to distinguish between blow-associated microbes and seawater microbes, we pooled 5 seawater samples from the same area where blow samples’ collection were carried out. The the respiratory microbiota was assessed by using the V3-V4 region of the 16S rRNA gene via Illumina Amplicon Sequencing. The pooled water samples contained more bacterial taxa than the blow samples of both wild animals and the sequenced dolphin maintained under human care. The composition of the bacterial community differed between the water samples and between the blow samples of wild cetaceans and that under human care, but these differences may have been mediated by different microbial communities between seawater and aquarium water. The sperm whale’s respiratory microbiome was more similar to the results obtained from wild bottlenose dolphins. Although the number of samples used in this study was limited and sampling and analyses were impaired by several limitations, the results are rather encouraging, as shown by the evident microbial differences between seawater and blow samples, confirmed also by the meta-analysis carried out comparing our results with those obtained in previous studies. Collecting exhaled air from small cetaceans using drones is a challenging process, both logistically and technically. The success in obtaining samples from small cetacean blow in this study in comparison to previous studies is likely due to the distance the sampling kit is suspended from the drone, which reduced the likelihood that the turbulence of the drone propeller interfered with successfully sampling blow, suggested as a factor leading to poor success in previous studies.},
	language = {en},
	number = {7},
	urldate = {2021-01-05},
	journal = {PLOS ONE},
	author = {Centelleghe, Cinzia and Carraro, Lisa and Gonzalvo, Joan and Rosso, Massimiliano and Esposti, Erika and Gili, Claudia and Bonato, Marco and Pedrotti, Davide and Cardazzo, Barbara and Povinelli, Michele and Mazzariol, Sandro},
	month = jul,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Dolphins, Bacteria, Humpback whales, Microbiome, Sea water, Sperm whales, Swimming, Wildlife},
	pages = {e0235537},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/YQ5Z2TM3/Centelleghe et al. - 2020 - The use of Unmanned Aerial Vehicles (UAVs) to samp.pdf:application/pdf}
}

@article{noauthor_iaaam_2018,
	title = {{IAAAM} 2019},
	url = {https://www.vin.com/doc/?id=9032708},
	journal = {VIN.com},
	month = may,
	year = {2018},
	note = {Context Object: ctx\_ver=Z39.88-2004\&rft\_val\_fmt=info\%3Aofi\%2Ffmt\%3Akev\%3Amtx\%3Ajournal\&rft\_id=https://www.vin.com/doc/?id\%3D9032708\&rft.atitle=IAAAM+2019\&rft.jtitle=VIN.com\&rft.date=2018-05-19},
	file = {Occurrence of Antimicrobial-Resistant Escherichia coli in Marine Animals in the North and Baltic Sea\: Preliminary Results - IAAAM 2019 - VIN:/Users/b3020111/Zotero/storage/VRGQFI6U/defaultadv1.html:text/html}
}

@article{fiori_using_2020,
	title = {Using {Unmanned} {Aerial} {Vehicles} ({UAVs}) to assess humpback whale behavioral responses to swim-with interactions in {Vava}’u, {Kingdom} of {Tonga}},
	volume = {28},
	issn = {0966-9582},
	url = {https://doi.org/10.1080/09669582.2020.1758706},
	doi = {10.1080/09669582.2020.1758706},
	abstract = {The Kingdom of Tonga is a one of the few countries worldwide that allow swim-with-whales tourism activities. Most of the tour operators are based in Vava’u archipelago which represents an important breeding ground for Oceania humpback whales (Megaptera novaeangliae). This study represents an assessment of the effects of swimmer approaches on humpback whales’ behaviour using Unmanned Aerial Vehicles (UAVs). UAV flights took place during the 2016 and 2017 whale breeding seasons from onboard research and swim-with-whales vessels. Whales’ behavioural states (resting, travelling, surface-active, socialising, nurturing) were assessed from aerial videos and the proportions of time spent in each state in the presence and absence of swimmers were compared. Whale agonistic behaviours directed towards swimmers and the injury of a swimmer caused by a whale were documented. Results indicate that in-water tourism activities significantly altered the time spent in each behavioural state by humpback whale in Vava’u. Mother-calf pairs decreased the proportion of time spent nurturing, while the time spent travelling increased two-fold when approached by swimmers. These findings indicate a potential energy expenditure increase for humpback whale mothers and their calves in response to swim-with tourism activities in Vava’u. Moreover, whales’ behavioural responses can pose danger of injury to swimmers.},
	number = {11},
	urldate = {2021-01-05},
	journal = {Journal of Sustainable Tourism},
	author = {Fiori, Lorenzo and Martinez, Emmanuelle and Orams, Mark B. and Bollard, Barbara},
	month = nov,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09669582.2020.1758706},
	keywords = {behaviour, drones, impact, injury, swim-with-whales},
	pages = {1743--1761},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/6TU8UHKM/Fiori et al. - 2020 - Using Unmanned Aerial Vehicles (UAVs) to assess hu.pdf:application/pdf}
}

@article{gray_drones_2019,
	title = {Drones and convolutional neural networks facilitate automated and accurate cetacean species identification and photogrammetry},
	volume = {10},
	copyright = {© 2019 The Authors. Methods in Ecology and Evolution © 2019 British Ecological Society},
	issn = {2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13246},
	doi = {https://doi.org/10.1111/2041-210X.13246},
	abstract = {The flourishing application of drones within marine science provides more opportunity to conduct photogrammetric studies on large and varied populations of many different species. While these new platforms are increasing the size and availability of imagery datasets, established photogrammetry methods require considerable manual input, allowing individual bias in techniques to influence measurements, increasing error and magnifying the time required to apply these techniques. Here, we introduce the next generation of photogrammetry methods utilizing a convolutional neural network to demonstrate the potential of a deep learning-based photogrammetry system for automatic species identification and measurement. We then present the same data analysed using conventional techniques to validate our automatic methods. Our results compare favorably across both techniques, correctly predicting whale species with 98\% accuracy (57/58) for humpback whales, minke whales, and blue whales. Ninety percent of automated length measurements were within 5\% of manual measurements, providing sufficient resolution to inform morphometric studies and establish size classes of whales automatically. The results of this study indicate that deep learning techniques applied to survey programs that collect large archives of imagery may help researchers and managers move quickly past analytical bottlenecks and provide more time for abundance estimation, distributional research, and ecological assessments.},
	language = {en},
	number = {9},
	urldate = {2021-01-05},
	journal = {Methods in Ecology and Evolution},
	author = {Gray, Patrick C. and Bierlich, Kevin C. and Mantell, Sydney A. and Friedlaender, Ari S. and Goldbogen, Jeremy A. and Johnston, David W.},
	year = {2019},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13246},
	keywords = {cetaceans, deep learning, convolutional neural network, drones, photogrammetry, population assessments, species identification, unoccupied aerial systems},
	pages = {1490--1500},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/Z9XPXIBS/Gray et al. - 2019 - Drones and convolutional neural networks facilitat.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/6BFVDSXJ/2041-210X.html:text/html}
}

@article{bogucki_applying_2019,
	title = {Applying deep learning to right whale photo identification},
	volume = {33},
	copyright = {© 2018 The Authors. Conservation Biology published by Wiley Periodicals, Inc. on behalf of Society for Conservation Biology.},
	issn = {1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/cobi.13226},
	doi = {https://doi.org/10.1111/cobi.13226},
	abstract = {Photo identification is an important tool for estimating abundance and monitoring population trends over time. However, manually matching photographs to known individuals is time-consuming. Motivated by recent developments in image recognition, we hosted a data science challenge on the crowdsourcing platform Kaggle to automate the identification of endangered North Atlantic right whales (Eubalaena glacialis). The winning solution automatically identified individual whales with 87\% accuracy with a series of convolutional neural networks to identify the region of interest on an image, rotate, crop, and create standardized photographs of uniform size and orientation and then identify the correct individual whale from these passport-like photographs. Recent advances in deep learning coupled with this fully automated workflow have yielded impressive results and have the potential to revolutionize traditional methods for the collection of data on the abundance and distribution of wild populations. Presenting these results to a broad audience should further bridge the gap between the data science and conservation science communities.},
	language = {en},
	number = {3},
	urldate = {2021-01-05},
	journal = {Conservation Biology},
	author = {Bogucki, Robert and Cygan, Marek and Khan, Christin Brangwynne and Klimek, Maciej and Milczek, Jan Kanty and Mucha, Marcin},
	year = {2019},
	note = {\_eprint: https://conbio.onlinelibrary.wiley.com/doi/pdf/10.1111/cobi.13226},
	keywords = {computer vision, convolutional neural networks, machine learning, algorithm, algoritmo, aprendizaje automático, automated image recognition, competencia Kaggle, identificación fotográfica, Kaggle competition, Kaggle 网站竞赛, photo identification, reconocimiento automatizado de imágenes, redes neurales convolucionales, visión computarizada, 卷积神经网络, 机器学习, 照片识别, 算法, 自动图像识别, 计算机视觉},
	pages = {676--684},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/QVLG8KHQ/Bogucki et al. - 2019 - Applying deep learning to right whale photo identi.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/D3IRYY2C/cobi.html:text/html}
}

@article{giles_responses_2020,
	title = {Responses of bottlenose dolphins ({Tursiops} spp.) to small drones},
	volume = {n/a},
	copyright = {© 2020 John Wiley \& Sons, Ltd.},
	issn = {1099-0755},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aqc.3440},
	doi = {https://doi.org/10.1002/aqc.3440},
	abstract = {Recent advances in aerial drones offer new insights into the biology, ecology and behaviour of marine wildlife found on or near the ocean’s surface. While opening up new opportunities for enhanced wildlife monitoring, the impacts of drone sampling and how it might influence interpretations of animal behaviour are only just beginning to be understood. The capacity of drones to record bottlenose dolphin (Tursiops spp.) behaviour was investigated, along with how the presence of a small drone at varying altitudes influences dolphin behaviour. Over 3 years and eight locations, 361 drone flights were completed between altitudes of 5 and 60 m above the ocean. Analyses showed that dolphins were increasingly likely to change behaviour with decreasing drone altitude. A positive correlation was also found between time spent hovering above a group of dolphins and the probability of recording a behavioural response. Dolphin group size also influenced the frequency of an observed behavioural change, displaying a positive correlation between behaviour change and group size. Overall, although drones have the potential to impact coastal dolphins when flown at low altitudes, they represent a useful tool for collecting ecological information on coastal dolphins owing to their convenience, low cost and capacity to observe behaviours underwater. To maximize benefits and minimize impacts, this study suggests that drones should be flown 30 m above coastal bottlenose dolphins.},
	language = {en},
	number = {n/a},
	urldate = {2021-01-05},
	journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
	author = {Giles, Anna B. and Butcher, Paul A. and Colefax, Andrew P. and Pagendam, Dan E. and Mayjor, Maddison and Kelaher, Brendan P.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aqc.3440},
	keywords = {behaviour, coastal, disturbance, drone, mammals, new techniques, ocean, recreation, Tursiops aduncus, Tursiops truncatus},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/GQZC5TAB/Giles et al. - Responses of bottlenose dolphins (Tursiops spp.) t.pdf:application/pdf}
}

@article{ramos_bottlenose_2018,
	title = {Bottlenose {Dolphins} and {Antillean} {Manatees} {Respond} to {Small} {Multi}-{Rotor} {Unmanned} {Aerial} {Systems}},
	volume = {5},
	issn = {2296-7745},
	url = {https://www.frontiersin.org/articles/10.3389/fmars.2018.00316/full},
	doi = {10.3389/fmars.2018.00316},
	abstract = {Unmanned aerial systems (UAS) are powerful tools for research and monitoring of wildlife. However, the effects of these systems on most marine mammals are largely unknown, preventing the establishment of guidelines that will minimize animal disturbance. In this study, we evaluated the behavioral responses of coastal bottlenose dolphins (Tursiops truncatus) and Antillean manatees (Trichechus manatus manatus) to small multi-rotor UAS flight. From 2015 to 2017, we piloted 211 flights using DJI quadcopters (Phantom II Vision +, 3 Professional and 4) to approach and follow animals over shallow-water habitats in Belize. The quadcopters were equipped with high-resolution cameras to observe dolphins during 138 of these flights, and manatees during 73 flights. Aerial video observations of animal behavior were coded and paired with flight data to determine whether animal activity and/or the UAS’s flight patterns caused behavioral changes in exposed animals. Dolphins responded to UAS flight at altitudes of 11–30 m, and responded primarily when they were alone or in small groups. Single dolphins and one pair responded to the UAS by orienting upward and turning towards the aircraft to observe it, before quickly returning to their pre-response activity. A higher number of manatees responded to the UAS, exhibiting strong disturbance in response to the aircraft from 6–104 m. Manatees changed their behavior by fleeing the area and sometimes this elicited the same response in nearby animals. If pursued post-response, manatees repeatedly responded to overhead flight by evading the aircraft’s path. These findings suggest that the invasiveness of UAS varies across individuals, species, and taxa. We conclude that careful exploratory research is needed to determine the impact of multi-rotor UAS flight on diverse species, and to develop best practices aimed at reducing the disturbance to wildlife that may result from their use.},
	language = {English},
	urldate = {2021-01-05},
	journal = {Frontiers in Marine Science},
	author = {Ramos, Eric A. and Maloney, Brigid and Magnasco, Marcelo O. and Reiss, Diana},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {Dolphins, wildlife monitoring, Behavior, Drone, Manatees, marine mammals, UAS, UAV, Unmanned aerial systems, Unmanned Aerial Vehicle},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/GWSACJL4/Ramos et al. - 2018 - Bottlenose Dolphins and Antillean Manatees Respond.pdf:application/pdf}
}

@article{pomeroy_assessing_2015,
	title = {Assessing use of and reaction to unmanned aerial systems in gray and harbor seals during breeding and molt in the {UK1}},
	url = {https://cdnsciencepub.com/doi/abs/10.1139/juvs-2015-0013},
	doi = {10.1139/juvs-2015-0013},
	abstract = {Wildlife biology applications of unmanned aerial systems (UAS) are extensive. Survey, identification, and measurement using UAS equipped with appropriate sensors can now be added to the suite of te...},
	language = {en},
	urldate = {2021-01-05},
	journal = {Journal of Unmanned Vehicle Systems},
	author = {Pomeroy, P. and O'Connor, L. and Davies, P.},
	month = sep,
	year = {2015},
	note = {Publisher: NRC Research Press  http://www.nrcresearchpress.com},
	file = {Full Text:/Users/b3020111/Zotero/storage/DWJ8RHP4/Pomeroy et al. - 2015 - Assessing use of and reaction to unmanned aerial s.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/RUVYTIRS/juvs-2015-0013.html:text/html;Snapshot:/Users/b3020111/Zotero/storage/JQDX97F3/juvs-2015-0013.html:text/html}
}

@article{bevan_measuring_2018,
	title = {Measuring behavioral responses of sea turtles, saltwater crocodiles, and crested terns to drone disturbance to define ethical operating thresholds},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194460},
	doi = {10.1371/journal.pone.0194460},
	abstract = {Drones are being increasingly used in innovative ways to enhance environmental research and conservation. Despite their widespread use for wildlife studies, there are few scientifically justified guidelines that provide minimum distances at which wildlife can be approached to minimize visual and auditory disturbance. These distances are essential to ensure that behavioral and survey data have no observer bias and form the basis of requirements for animal ethics and scientific permit approvals. In the present study, we documented the behaviors of three species of sea turtle (green turtles, Chelonia mydas, flatback turtles, Natator depressus, hawksbill turtles, Eretmochelys imbricata), saltwater crocodiles (Crocodylus porosus), and crested terns (Thalasseus bergii) in response to a small commercially available (1.4 kg) multirotor drone flown in Northern Territory and Western Australia. Sea turtles in nearshore waters off nesting beaches or in foraging habitats exhibited no evasive behaviors (e.g. rapid diving) in response to the drone at or above 20–30 m altitude, and at or above 10 m altitude for juvenile green and hawksbill turtles foraging on shallow, algae-covered reefs. Adult female flatback sea turtles were not deterred by drones flying forward or stationary at 10 m altitude when crawling up the beach to nest or digging a body pit or egg chamber. In contrast, flyovers elicited a range of behaviors from crocodiles, including minor, lateral head movements, fleeing, or complete submergence when a drone was present below 50 m altitude. Similarly, a colony of crested terns resting on a sand-bank displayed disturbance behaviors (e.g. flight response) when a drone was flown below 60 m altitude. The current study demonstrates a variety of behavioral disturbance thresholds for diverse species and should be considered when establishing operating conditions for drones in behavioral and conservation studies.},
	language = {en},
	number = {3},
	urldate = {2021-01-05},
	journal = {PLOS ONE},
	author = {Bevan, Elizabeth and Whiting, Scott and Tucker, Tony and Guinea, Michael and Raith, Andrew and Douglas, Ryan},
	month = mar,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Sea water, Animal behavior, Animal flight, Bird flight, Crocodiles, Habitats, Reefs, Turtles},
	pages = {e0194460},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/ZMAIBCR2/Bevan et al. - 2018 - Measuring behavioral responses of sea turtles, sal.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/TSJN9SWF/article.html:text/html}
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-01-05},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/Z7H8LXTL/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/KVUC9DJ3/1505.html:text/html}
}

@article{reno_sift-based_2019,
	title = {A {SIFT}-based software system for the photo-identification of the {Risso}'s dolphin},
	volume = {50},
	issn = {1574-9541},
	url = {http://www.sciencedirect.com/science/article/pii/S1574954118301377},
	doi = {10.1016/j.ecoinf.2019.01.006},
	abstract = {Photo-identification is a commonly used non-invasive technique that has been profitably employed in biological studies throughout the years. It starts from the assumption that a single individual can be recognized in multiple photos captured at different times by exploiting its unique representative and visible physical qualities such as marks, notches or any other definite feature. Hence, photo-identification is performed to infer knowledge about wild species' spatial and temporal distributions as well as population dynamics, thus providing valuable information especially when the species being investigated is ranked as data deficient. Furthermore, the technological improvements of the last decades and the large availability of devices with powerful computing capabilities are driving the research towards a common goal of enriching bio-ecological studies with innovative computer science approaches. In this scenario, computer vision plays a fundamental role, as it can successfully assist researchers in the analysis of large amounts of data. The aim of this paper is, in fact, to effectively provide a computer vision approach for the photo-identification of the Risso's dolphin, exploiting specific visual cues with a feature-based approach relying on SIFT and SURF feature detectors. The experiments have been conducted on image data acquired in the Gulf of Taranto from 2013 to 2017, conducting a comparative analysis of the performance of both SIFT and SURF, as well as a comparison with the state-of-the-art software DARWIN, and they proved the effectiveness of the proposed approach and suggested its application would be suitable to large scale studies. In conclusion, this paper shows an innovative computer vision application for the identification of unknown Risso's dolphin individuals that relies on a feature-based automated approach. The results suggest that the proposed approach can efficiently assist researchers during the photo-identification task of large amounts of data collected in such a challenging domain.},
	language = {en},
	urldate = {2021-01-07},
	journal = {Ecological Informatics},
	author = {Renò, V. and Dimauro, G. and Labate, G. and Stella, E. and Fanizza, C. and Cipriano, G. and Carlucci, R. and Maglietta, R.},
	month = mar,
	year = {2019},
	keywords = {Computer vision, Cetaceans, Photo-identification, Risso, SIFT features},
	pages = {95--101},
	file = {Renò et al. - 2019 - A SIFT-based software system for the photo-identif.pdf:/Users/b3020111/Zotero/storage/FMKYWWQL/Renò et al. - 2019 - A SIFT-based software system for the photo-identif.pdf:application/pdf;ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/UZ44KDHR/S1574954118301377.html:text/html}
}

@misc{noauthor_sift-based_nodate,
	title = {A {SIFT}-based software system for the photo-identification of the {Risso}'s dolphin {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1574954118301377?token=2C1F301064C2B1DAE9FC3E42617BDC8F3A754A22E9CBAC1EBDA069CDE0EF1E361F59A8D187F071A928E979317304C6B1},
	language = {en},
	urldate = {2021-01-07},
	doi = {10.1016/j.ecoinf.2019.01.006},
	file = {Snapshot:/Users/b3020111/Zotero/storage/34XNP6FW/S1574954118301377.html:text/html}
}

@article{cheney_long-term_2014,
	title = {Long-term trends in the use of a protected area by small cetaceans in relation to changes in population status},
	volume = {2},
	issn = {2351-9894},
	url = {http://www.sciencedirect.com/science/article/pii/S2351989414000250},
	doi = {10.1016/j.gecco.2014.08.010},
	abstract = {The requirement to monitor listed species in European designated sites is challenging for long-lived mobile species that only temporarily occupy protected areas. We use a 21 year time series of bottlenose dolphin photo-identification data to assess trends in abundance and conservation status within a Special Area of Conservation (SAC) in Scotland. Mark–recapture methods were used to estimate annual abundance within the SAC from 1990 to 2010. A Bayesian mark–recapture model with a state-space approach was used to estimate overall population trends using data collected across the populations’ range. Despite inter-annual variability in the number of dolphins within the SAC, there was a {\textgreater}99\% probability that the wider population was stable or increasing. Results indicate that use of the SAC by the wider population has declined. This is the first evidence of long-term trends in the use of an EU protected area by small cetaceans in relation to changes in overall population status. Our results highlight the importance of adapting the survey protocols used in long-term photo-identification studies to maintain high capture probabilities and minimise sampling heterogeneity. Crucially, these data demonstrate the value of collecting data from the wider population to assess the success of protected areas designated for mobile predators.},
	language = {en},
	urldate = {2021-01-07},
	journal = {Global Ecology and Conservation},
	author = {Cheney, Barbara and Corkrey, Ross and Durban, John W. and Grellier, Kate and Hammond, Philip S. and Islas-Villanueva, Valentina and Janik, Vincent M. and Lusseau, Susan M. and Parsons, Kim M. and Quick, Nicola J. and Wilson, Ben and Thompson, Paul M.},
	month = dec,
	year = {2014},
	keywords = {Bottlenose dolphin, Photo-identification, Abundance, Bayesian, Mark–recapture, Special Area of Conservation},
	pages = {118--128},
	file = {ScienceDirect Full Text PDF:/Users/b3020111/Zotero/storage/4B9SGZQW/Cheney et al. - 2014 - Long-term trends in the use of a protected area by.pdf:application/pdf;ScienceDirect Snapshot:/Users/b3020111/Zotero/storage/BC5FW83C/S2351989414000250.html:text/html}
}

@article{trotter_ndd20_2020,
	title = {{NDD20}: {A} large-scale few-shot dolphin dataset for coarse and fine-grained categorisation},
	copyright = {Creative Commons Attribution-NonCommercial 4.0 International Licence (CC-BY-NC)},
	shorttitle = {{NDD20}},
	url = {http://arxiv.org/abs/2005.13359},
	abstract = {We introduce the Northumberland Dolphin Dataset 2020 (NDD20), a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation. All data contained in NDD20 was obtained via manual collection in the North Sea around the Northumberland coastline, UK. We present experimentation using standard deep learning network architecture trained using NDD20 and report baselines results.},
	urldate = {2021-01-07},
	journal = {arXiv:2005.13359 [cs]},
	author = {Trotter, Cameron and Atkinson, Georgia and Sharpe, Matt and Richardson, Kirsten and McGough, A. Stephen and Wright, Nick and Burville, Ben and Berggren, Per},
	month = may,
	year = {2020},
	note = {arXiv: 2005.13359},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/b3020111/Zotero/storage/UZM5449L/Trotter et al. - 2020 - NDD20 A large-scale few-shot dolphin dataset for .pdf:application/pdf;arXiv.org Snapshot:/Users/b3020111/Zotero/storage/HV87KXC6/2005.html:text/html}
}

@techreport{payne_long_1986,
	title = {Long term behavioral studies of the southern right whale ({Eubalaena} australis)},
	url = {https://ballenas.org.ar/descargas/publicaciones-cientificas/1986/15.%20Long%20term%20behavioral%20studies%20of%20the%20southern%20right%20whale%20(Eubalaena%20australis).pdf},
	number = {10},
	author = {Payne, Roger},
	year = {1986},
	pages = {161--167},
	file = {Full Text:/Users/b3020111/Zotero/storage/AV3L6GIQ/Payne - 1986 - Long term behavioral studies of the southern right.pdf:application/pdf}
}

@article{forney_seasonal_1998,
	title = {Seasonal {Patterns} in the {Abundance} and {Distribution} of {California} {Cetaceans}, 1991–1992},
	volume = {14},
	issn = {1748-7692},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1748-7692.1998.tb00737.x},
	doi = {https://doi.org/10.1111/j.1748-7692.1998.tb00737.x},
	abstract = {This study presents a detailed seasonal comparison of the abundance and distribution of cetaceans within 100-150 nmi (185-278 km) of the California coast during 1991 and 1992. The results of a shipboard line-transect survey conducted in July-November 1991 (“summer”) were compared to those from aerial line-transect surveys conducted in March-April 1991 and February-April 1992 (“winter”). Using a confidence-interval-based bootstrap procedure, abundance estimates for six of the eleven species included in the comparison exhibited significant (α= 0.05) differences between the winter and summer surveys. Pacific white-sided dolphins (Lagenorhynchus obliquidens), Risso's dolphins (Grampus griseus), common dolphins (Delphinus spp.), and northern right whale dolphins (Lissodelphis borealis) were significantly more abundant in winter. The abundance of blue whales (Balaenoptera musculuss) and gray whales (Eschrichtius robustus) reflected well-documented migratory patterns. Fin whales (B. physalus) were significantly more abundant during summer. No significant differences in seasonal abundance were identified for Dall's porpoises (Phocoenoides dalli), bottlenose dolphins (Tursiops truncatus), killer whales (Orcinus orca), sperm whales (Physeter macrocephalus), or humpback whales (Megaptera novaeangliae). Significant north/south shifts in distribution were found for Dall's porpoises, common dolphins, and Pacific white-sided dolphins, and significant inshore/offshore differences were identified for northern right whale dolphins and humpback whales.},
	language = {en},
	number = {3},
	urldate = {2021-01-07},
	journal = {Marine Mammal Science},
	author = {Forney, Karin A. and Barlow, Jay},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1748-7692.1998.tb00737.x},
	keywords = {dolphin, abundance, aerial survey, bootstrap, California, cetacean, confidence-interval test, distribution, line transect, North Pacific, porpoise, seasonality, ship survey, whale},
	pages = {460--489},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/WUHD9VM2/Forney and Barlow - 1998 - Seasonal Patterns in the Abundance and Distributio.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/YMGNN47Y/j.1748-7692.1998.tb00737.html:text/html}
}

@article{gibson_using_2020,
	title = {Using social media as a cost-effective resource in the photo-identification of a coastal bottlenose dolphin community},
	volume = {30},
	copyright = {© 2020 Crown Copyright. Aquatic Conservation: Marine and Freshwater Ecosystems published by John Wiley \& Sons, Ltd. This article is published with the permission of the Controller of HMSO and the Queen's Printer for Scotland.},
	issn = {1099-0755},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aqc.3356},
	doi = {https://doi.org/10.1002/aqc.3356},
	abstract = {Bottlenose dolphins encountered around the Irish coast are considered part of a wide-ranging coastal community; however, knowledge on the significance of the north of Ireland for this species is limited by a lack of dedicated effort. Through social media, the opportunity now exists to gather large volumes of citizen science data in the form of high-quality images, potentially extending the spatial and temporal scope of photo-identification studies. The purpose of this study was to investigate social media as a data resource for photo-identification studies and to provide a preliminary assessment of bottlenose dolphins in the north of Ireland. Specifically, the study sought to examine the photo-identification data for spatial clustering. The study identified 54 well-marked individuals and provided evidence of potential year-round occurrence, with successful re-sightings throughout the study period (2007–2016). There was a geographic concentration of re-sightings along the north of Ireland, suggestive of interannual site fidelity. These results provide scientific rationale for strategically targeting the north of Ireland in future research on the Irish coastal community. For effective conservation of the bottlenose dolphin it is imperative that scientific research, and resultant management objectives, consider wide-ranging communities such as the Irish coastal community. Our research highlights data collection via social media as a cost-effective and scientifically valuable tool in the photo-identification of coastal cetaceans. We recommend that this method is used in research on low-density and wide-ranging coastal cetaceans.},
	language = {en},
	number = {8},
	urldate = {2021-01-07},
	journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
	author = {Gibson, Catherine Elizabeth and Williams, David and Dunlop, Rebecca and Beck, Suzanne},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aqc.3356},
	keywords = {monitoring, coastal, distribution, habitats directive},
	pages = {1702--1710},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/XN2HLNEA/Gibson et al. - 2020 - Using social media as a cost-effective resource in.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/QUXUHWLQ/aqc.html:text/html;Snapshot:/Users/b3020111/Zotero/storage/UFEFRKRZ/aqc.html:text/html}
}

@misc{noauthor_integrating_nodate,
	title = {Integrating multiple data sources to assess the distribution and abundance of bottlenose dolphins {Tursiops} truncatus in {Scottish} waters - {Cheney} - 2013 - {Mammal} {Review} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1365-2907.2011.00208.x},
	urldate = {2021-01-07},
	file = {Integrating multiple data sources to assess the di.pdf:/Users/b3020111/Zotero/storage/38ARPEVT/Integrating multiple data sources to assess the di.pdf:application/pdf;Integrating multiple data sources to assess the distribution and abundance of bottlenose dolphins Tursiops truncatus in Scottish waters - Cheney - 2013 - Mammal Review - Wiley Online Library:/Users/b3020111/Zotero/storage/3F3VS4I9/j.1365-2907.2011.00208.html:text/html}
}

@misc{noauthor_integrating_nodate-1,
	title = {Integrating multiple data sources to assess the distribution and abundance of bottlenose dolphins {Tursiops} truncatus in {Scottish} waters - {Cheney} - 2013 - {Mammal} {Review} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1365-2907.2011.00208.x},
	urldate = {2021-01-07}
}

@article{cheney_integrating_2013,
	title = {Integrating multiple data sources to assess the distribution and abundance of bottlenose dolphins {Tursiops} truncatus in {Scottish} waters},
	volume = {43},
	issn = {1365-2907},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2907.2011.00208.x},
	doi = {https://doi.org/10.1111/j.1365-2907.2011.00208.x},
	abstract = {The distribution, movements and abundance of highly mobile marine species such as bottlenose dolphins Tursiops truncatus are best studied at large spatial scales, but previous research effort has generally been focused on relatively small areas, occupied by populations with high site fidelity. We aimed to characterize the distribution, movements and abundance of bottlenose dolphins around the coasts of Scotland, exploring how data from multiple sources could be integrated to build a broader-scale picture of their ecology. We reviewed existing historical data, integrated data from ongoing studies and developed new collaborative studies to describe distribution patterns. We adopted a Bayesian multi-site mark-recapture model to estimate abundance of bottlenose dolphins throughout Scottish coastal waters and quantified movements of individuals between study areas. The majority of sightings of bottlenose dolphins around the Scottish coastline are concentrated on the east and west coasts, but records are rare before the 1990s. Dedicated photo-identification studies in 2006 and 2007 were used to estimate the size of two resident populations: one on the east coast from the Moray Firth to Fife, population estimate 195 [95\% highest posterior density intervals (HPDI): 162–253] and the second in the Hebrides, population estimate 45 (95\% HPDI: 33–66). Interaction parameters demonstrated that the dolphins off the east coast of Scotland are highly mobile, whereas those off the west coast form two discrete communities. We provide the first comprehensive assessment of the abundance of bottlenose dolphins in the inshore waters of Scotland. The combination of dedicated photo-identification studies and opportunistic sightings suggest that a relatively small number of bottlenose dolphins (200–300 individuals) occur regularly in Scottish coastal waters. On both east and west coasts, re-sightings of identifiable individuals indicate that the animals have been using these coastal areas since studies began.},
	language = {en},
	number = {1},
	urldate = {2021-01-07},
	journal = {Mammal Review},
	author = {Cheney, Barbara and Thompson, Paul M. and Ingram, Simon N. and Hammond, Philip S. and Stevick, Peter T. and Durban, John W. and Culloch, Ross M. and Elwen, Simon H. and Mandleberg, Laura and Janik, Vincent M. and Quick, Nicola J. and ISLAS‐Villanueva, Valentina and Robinson, Kevin P. and Costa, Marina and Eisfeld, Sonja M. and Walters, Alice and Phillips, Charlie and Weir, Caroline R. and Evans, Peter G. H. and Anderwald, Pia and Reid, Robert J. and Reid, James B. and Wilson, Ben},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2907.2011.00208.x},
	keywords = {citizen science, cetacean, coastal zone management, renewable energy, spatial ecology},
	pages = {71--88},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/TI3YU2DX/Cheney et al. - 2013 - Integrating multiple data sources to assess the di.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/456HGXNA/j.1365-2907.2011.00208.html:text/html}
}

@article{caldwell_evidence_1955,
	title = {Evidence of {Home} {Range} of an {Atlantic} {Bottlenose} {Dolphin}},
	volume = {36},
	issn = {0022-2372},
	url = {https://doi.org/10.2307/1375913},
	doi = {10.2307/1375913},
	abstract = {An Atlantic bottlenose dolphin, Tursiops truncatus (Montague), with a damaged dorsal fin was observed by the author on November 1, 1953, at approximately 0900 at Cedar Key, Florida. The healed injury, the absence of the upper two-thirds of the fin, served as a natural "tag" and permitted ready identification of this individual, even in the brief period of a normal breathing roll. This dolphin was again seen in mid-afternoon on January 24, 1954, and a third time at 1430 on March 20,1954. The first observation was made in the main ship channel between Atsena Otie and Grassy Keys, approximately 1600 yards SSW of the main pier. On the second observation, the dolphin was again in the main ship channel about 200 yards east of the main pier, and approximately 600 yards WSW of the pier in the same channel on the third sighting. Joining these reference points (Mohr, Amer. Midi. Nat., 37: 233, 1947), an area of some 475,000 square yards is yielded as an estimated minimum home range for this animal at Cedar Key. Further observations would undoubtedly affect this calculation, and may show that the animal is actually restricted to the channel at Cedar Key and does not include some of the shallower flats in its home range, as would be indicated by the theoretical area encompassed by this approximation. On the other hand, it cannot of course be concluded from the small sample that this area actually corresponds to the full home range of this individual.},
	number = {2},
	urldate = {2021-01-07},
	journal = {Journal of Mammalogy},
	author = {Caldwell, David K.},
	month = may,
	year = {1955},
	pages = {304--305},
	file = {Snapshot:/Users/b3020111/Zotero/storage/W756QMF3/960828.html:text/html}
}

@article{schevill_daily_1960,
	title = {Daily {Patrol} of a {Megaptera}},
	volume = {41},
	issn = {00222372},
	url = {https://academic.oup.com/jmammal/article-lookup/doi/10.2307/1376380},
	doi = {10.2307/1376380},
	language = {en},
	number = {2},
	urldate = {2021-01-07},
	journal = {Journal of Mammalogy},
	author = {Schevill, William E. and Backus, Richard H.},
	month = may,
	year = {1960},
	pages = {279},
	file = {Schevill and Backus - 1960 - Daily Patrol of a Megaptera.pdf:/Users/b3020111/Zotero/storage/79AGDK56/Schevill and Backus - 1960 - Daily Patrol of a Megaptera.pdf:application/pdf}
}

@article{sharpe_indian_2019,
	title = {Indian {Ocean} humpback dolphin in the {Menai} {Bay} off the south coast of {Zanzibar}, {East} {Africa} is {Critically} {Endangered}},
	volume = {29},
	copyright = {© 2019 John Wiley \& Sons, Ltd.},
	issn = {1099-0755},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aqc.3221},
	doi = {https://doi.org/10.1002/aqc.3221},
	abstract = {Cetaceans occupying coastal habitats are at high risk of impact from anthropogenic sources which can cause direct mortality or affect long-term health. Monitoring and detecting change require long-term studies and reliable funding, not always available especially in developing countries. Management and conservation of cetaceans must therefore use precautionary methods that allow assessment from limited data sources to identify risk of, and prevent, species extirpation or extinction. IUCN Red List criteria for regional populations was applied to the population of Indian Ocean humpback dolphins (Sousa plumbea) resident in Menai Bay Conservation Area off the south coast of Zanzibar, East Africa which is subjected to unsustainable entanglement rates in gillnet fisheries and unregulated tourism activities. Photographic identification surveys were conducted in 2015 to generate a new abundance estimate from capture–recapture analysis. Mortality estimates were calculated using available data from 1999 to 2002 and a population viability analysis was conducted based on population, species and genus specific parameters. The 2015 abundance estimate for humpback dolphins was 19 (95\% CI 14–25) non-calf individuals, representing a 63\% reduction in abundance since 2002. The population viability analysis baseline scenario predicted chance of extinction at 0.996 (SE 0.002) with the median time to extinction at 36 years. Sensitivity analysis suggested that population recovery would only be possible with a complete prevention of bycatch mortality. The population met the threshold for Critically Endangered for all criteria which could be directly assessed. This conservation assessment highlights the requirement for immediate management action to eliminate bycatch of humpback dolphins to prevent the local extinction of the species.},
	language = {en},
	number = {12},
	urldate = {2021-01-07},
	journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
	author = {Sharpe, Matt and Berggren, Per},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aqc.3221},
	keywords = {coastal, mammals, endangered species, island, marine protected area},
	pages = {2133--2146},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/SI32ZTES/Sharpe and Berggren - 2019 - Indian Ocean humpback dolphin in the Menai Bay off.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/WA9DN78K/aqc.html:text/html}
}

@article{bigg_assessment_1982,
	title = {An {Assessment} of {Killer} {Whale} ({Orcinus} orca) {Stocks} off  {Vancouver} {Island}, {British} {Columbia} {T}},
	language = {en},
	author = {Bigg, Michael},
	year = {1982},
	pages = {12},
	file = {Bigg - An Assessment of Killer Whale (Orcinus orca) Stock.pdf:/Users/b3020111/Zotero/storage/IE2ACJW9/Bigg - An Assessment of Killer Whale (Orcinus orca) Stock.pdf:application/pdf}
}

@article{vanbressem_visual_2018,
	title = {Visual health assessment of white-beaked dolphins off the coast of {Northumberland}, {North} {Sea}, using underwater photography},
	url = {https://core.ac.uk/download/pdf/327362635.pdf},
	journal = {Marine Mammal Science},
	author = {VanBressem, Marie-Fran\{{\textbackslash}c\{c\}\}oise and Burville, Ben and Sharpe, Matt and Berggren, Per and VanWaerebeek, Koen},
	year = {2018},
	file = {Full Text:/Users/b3020111/Zotero/storage/HR5D2K2A/VanBressem et al. - 2018 - Visual health assessment of white-beaked dolphins .pdf:application/pdf}
}

@article{holmberg_estimating_2009,
	title = {Estimating population size, structure, and residency time for whale sharks {Rhincodon} typus through collaborative photo-identification},
	volume = {7},
	issn = {1863-5407, 1613-4796},
	url = {https://www.int-res.com/abstracts/esr/v7/n1/p39-53/},
	doi = {10.3354/esr00186},
	abstract = {Capture-mark-recapture (CMR) data from Ningaloo Marine Park (NMP) in Western Australia have recently been used to study the population dynamics of the local whale shark aggregation. Because nascent research efforts at other aggregation points look to NMP as a model, further analysis of existing modeling approaches is important. We have expanded upon previous studies of NMP whale sharks by estimating CMR survival and recruitment rates as functions of average total length (TL). Our analysis suggests a decline in reported values of TL coincident with marginally increasing abundance among sharks sighted in more than one year (‘returning’) from 1995 to 2008. We found a positive, average returning recruitment rate (λ) of 1.07 yr–1 (0.99 to 1.15, 95\% CI); smaller individuals contributed in larger numbers to recruitment, allowing for population growth accompanied by a decline in median size. We subsequently explored intraseasonal population dynamics with the Open Robust Design (ORD) model structure. Our best-fit model estimated modestly increasing annual abundances between 107 (95\% CI = 90 to 124) and 159 (95\% CI = 127 to 190) for 2004 to 2007, suggesting a short-term increase in total annual abundance. The ORD also estimated an average residency time of 33 d (95\% CI = 31 to 39) and biweekly entry profiles into the study area. Overall, our techniques demonstrate how large aggregations of the species can be modeled to better understand short- and long-term population trends. These results also show the direct scientific benefit from the development of an online, collaborative data management system to increase collection of sighting data for a rare species in conjunction with ecotourism activity.},
	language = {en},
	number = {1},
	urldate = {2021-01-07},
	journal = {Endangered Species Research},
	author = {Holmberg, Jason and Norman, Bradley and Arzoumanian, Zaven},
	month = apr,
	year = {2009},
	keywords = {Mark-recapture, Open Robust Design, Population biology, Rhincodon typus, Survivorship, Transience, Whale shark},
	pages = {39--53},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/9C3QCLEB/Holmberg et al. - 2009 - Estimating population size, structure, and residen.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/KUFM9XAI/p39-53.html:text/html}
}

@article{reisser_photographic_2008,
	title = {Photographic identification of sea turtles: method description and validation, with an estimation of tag loss},
	volume = {5},
	issn = {1863-5407, 1613-4796},
	shorttitle = {Photographic identification of sea turtles},
	url = {https://www.int-res.com/abstracts/esr/v5/n1/p73-82/},
	doi = {10.3354/esr00113},
	abstract = {Recognition of individual sea turtles is mostly achieved by checking artificial tags previously attached to them, a method which is made difficult by the considerable tag loss rate and which requires repeated manipulation of the marked individuals. We describe an individual recognition method for sea turtles of the family Cheloniidae based on a mark-recapture study that relied on both artificial tagging (Inconel tags, style 681) and natural marks (facial profile photographs). Juvenile green Chelonia mydas and hawksbill Eretmochelys imbricata turtles were manually caught at Arvoredo Island, southern Brazil, and through visual comparison of facial profile photographs we were able to identify recaptured individuals with 2, 1, or no artificial tags. Additionally, Bayesian inference based on tag loss information indicated that the way a tag is attached (position and distance from the flipper edge) affects significantly the probability of its loss. We encourage the use of photographic identification (facial profile) as a reliable method for individual recognition in studies of cheloniid turtles.},
	language = {en},
	number = {1},
	urldate = {2021-01-07},
	journal = {Endangered Species Research},
	author = {Reisser, Júlia and Proietti, Maíra and Kinas, Paul and Sazima, Ivan},
	month = sep,
	year = {2008},
	keywords = {Photo-identification, Bayesian inference, Cheloniidae, Sea turtles, Tag loss},
	pages = {73--82},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/NNI36ANW/Reisser et al. - 2008 - Photographic identification of sea turtles method.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/4323J7WD/p73-82.html:text/html}
}

@article{wursig_methods_1990,
	title = {Methods of photo-identification for small cetaceans},
	volume = {12},
	url = {http://www.vliz.be/imisdocs/publications/253951.pdf},
	journal = {Reports of the International Whaling Commission},
	author = {Würsig, Bernd and Jefferson, Thomas A},
	year = {1990},
	pages = {43--52},
	file = {Full Text:/Users/b3020111/Zotero/storage/FJH76VPM/Würsig and Jefferson - 1990 - Methods of photo-identification for small cetacean.pdf:application/pdf}
}

@article{miragliuolo_rissos_2004,
	title = {Risso’s dolphin harassment by pleasure boaters off the island of {Ischia}, central {Mediterranean} {Sea}},
	volume = {15},
	url = {https://www.researchgate.net/profile/Barbara_Mussi/publication/262566339_Risso's_dolphin_harassment_by_pleasure_boaters_off_the_Island_of_Ischia_Central_Mediterranean_Sea/links/0c9605381e63576004000000.pdf},
	journal = {European Research on Cetaceans},
	author = {Miragliuolo, A and Mussi, B and Bearzi, G},
	year = {2004},
	pages = {168--171},
	file = {Full Text:/Users/b3020111/Zotero/storage/PS5LI8ZP/Miragliuolo et al. - 2004 - Risso’s dolphin harassment by pleasure boaters off.pdf:application/pdf}
}

@article{vernazzani_eastern_2013,
	title = {Eastern {South} {Pacific} southern right whale photoidentification catalog reveals behavior and habitat use patterns},
	language = {en},
	journal = {MARINE MAMMAL SCIENCE},
	author = {Vernazzani, Rbara Galletti and Cabrera, Elsa},
	year = {2013},
	pages = {10},
	file = {Vernazzani and Cabrera - 2013 - Eastern South Pacific southern right whale photoid.pdf:/Users/b3020111/Zotero/storage/NS64DLCS/Vernazzani and Cabrera - 2013 - Eastern South Pacific southern right whale photoid.pdf:application/pdf}
}

@article{arnbom_individual_187,
	title = {Individual identification of sperm whales},
	volume = {37},
	number = {20},
	journal = {Report of the International Whaling Commission},
	author = {Arnbom, Tom},
	year = {187},
	pages = {201--204}
}

@article{constantine_abundance_2012,
	title = {Abundance of humpback whales in {Oceania} using photo-identification and microsatellite genotyping},
	volume = {453},
	issn = {0171-8630, 1616-1599},
	url = {https://www.int-res.com/abstracts/meps/v453/p249-261/},
	doi = {10.3354/meps09613},
	abstract = {Estimating the abundance of long-lived, migratory animals is challenging but essential for managing populations. We provide the first abundance estimates of endangered humpback whales Megaptera novaeangliae from their breeding grounds in Oceania, South Pacific. Using fluke photo-identification (1999−2004, n = 660 individuals) and microsatellite genotypes (1999−2005, n = 840 individuals), we estimated abundance with open capture-recapture statistical models. Total Oceania abundance and trends were estimated from 4 primary and 5 secondary sampling sites across the region. Sex-specific genotype data enabled us to account for the difference in capturability of males and females, by doubling male-specific estimates of abundance derived from genotypes. Abundance estimates were congruent between primary- and secondary-region data sets, suggesting that the primary regions are representative of all Oceania. The best estimate of total abundance was 4329 whales (3345−5313) in 2005, from a sex-specific POPAN super-population model, which includes resident whales and those migrating through the surveyed areas. A doubled-male POPAN abundance estimate from 2003 (n = 2941, 95\% CI = 1648−4234) was considered the most plausible for the 4 primary survey areas and was similar to the 2003 doubled-male estimate derived from Pradel capture probabilities (n = 2952, 95\% CI = 2043−4325). Our results confirm that Oceania is the least abundant humpback whale breeding population in the southern hemisphere. Pradel models showed no significant trend in abundance, which contradicts the recovery seen in most other populations throughout the world. Thus we suggest that the whales in this area warrant continued study and management attention.},
	language = {en},
	urldate = {2021-01-07},
	journal = {Marine Ecology Progress Series},
	author = {Constantine, Rochelle and Jackson, Jennifer A. and Steel, Debbie and Baker, C. Scott and Brooks, Lyndon and Burns, Daniel and Clapham, Phillip and Hauser, Nan and Madon, Bénédicte and Mattila, David and Oremus, Marc and Poole, Michael and Robbins, Jooke and Thompson, Kirsten and Garrigue, Claire},
	month = may,
	year = {2012},
	keywords = {Capture-recapture, Endangered species, Genotyping, Megaptera novaeangliae, South Pacific},
	pages = {249--261},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/8ZVJBYRQ/Constantine et al. - 2012 - Abundance of humpback whales in Oceania using phot.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/ERIBKA5R/p249-261.html:text/html}
}

@article{baird_population_2009,
	title = {Population structure of island-associated dolphins: {Evidence} from photo-identification of common bottlenose dolphins ({Tursiops} truncatus) in the main {Hawaiian} {Islands}},
	volume = {25},
	copyright = {No claim to original US government works},
	issn = {1748-7692},
	shorttitle = {Population structure of island-associated dolphins},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1748-7692.2008.00257.x},
	doi = {https://doi.org/10.1111/j.1748-7692.2008.00257.x},
	abstract = {Management agencies often use geopolitical boundaries as proxies for biological boundaries. In Hawaiian waters a single stock is recognized of common bottlenose dolphins, Tursiops truncatus, a species that is found both in open water and near-shore among the main Hawaiian Islands. To assess population structure, we photo-identified 336 distinctive individuals from the main Hawaiian Islands, from 2000 to 2006. Their generally shallow-water distribution, and numerous within-year and between-year resightings within island areas suggest that individuals are resident to the islands, rather than part of an offshore population moving through the area. Comparisons of identifications obtained from Kaua‘i/Ni‘ihau, O‘ahu, the “4-island area,” and the island of Hawai‘i showed no evidence of movements among these island groups, although movements from Kaua‘i to Ni‘ihau and among the “4-islands” were documented. A Bayesian analysis examining the probability of missing movements among island groups, given our sample sizes for different areas, indicates that interisland movement rates are less than 1\% per year with 95\% probability. Our results suggest the existence of multiple demographically independent populations of island-associated common bottlenose dolphins around the main Hawaiian islands.},
	language = {en},
	number = {2},
	urldate = {2021-01-07},
	journal = {Marine Mammal Science},
	author = {Baird, Robin W. and Gorgone, Antoinette M. and McSweeney, Daniel J. and Ligon, Allan D. and Deakos, Mark H. and Webster, Daniel L. and Schorr, Gregory S. and Martien, Karen K. and Salden, Dan R. and Mahaffy, Sabre D.},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1748-7692.2008.00257.x},
	keywords = {bottlenose dolphin, Tursiops truncatus, Hawai‘i, movements, population structure},
	pages = {251--274},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/EVWU9G54/Baird et al. - 2009 - Population structure of island-associated dolphins.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/VLZA9FUN/j.1748-7692.2008.00257.html:text/html}
}

@article{langtimm_survival_2004,
	title = {Survival {Estimates} for {Florida} {Manatees} from the {Photo}-{Identification} of {Individuals}},
	volume = {20},
	issn = {1748-7692},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1748-7692.2004.tb01171.x},
	doi = {https://doi.org/10.1111/j.1748-7692.2004.tb01171.x},
	abstract = {We estimated adult survival probabilities for the endangered Florida manatee (Trichechus manatus latirostris) in four regional populations using photoidentification data and open-population capture-recapture statistical models. The mean annual adult survival probability over the most recent 10-yr period of available estimates was as follows: Northwest - 0.956 (SE 0.007), Upper St. Johns River - 0.960 (0.011), Atlantic Coast - 0.937 (0.008), and Southwest - 0.908 (0.019). Estimates of temporal variance independent of sampling error, calculated from the survival estimates, indicated constant survival in the Upper St. Johns River, true temporal variability in the Northwest and Atlantic Coast, and large sampling variability obscuring estimates for the Southwest. Calf and subadult survival probabilities were estimated for the Upper St. Johns River from the only available data for known-aged individuals: 0.810 (95\% CI 0.727–0.873) for 1st year calves, 0.915 (0.827–0.960) for 2nd year calves, and 0.969 (0.946–0.982) for manatee 3 yr or older. These estimates of survival probabilities and temporal variance, in conjunction with estimates of reproduction probabilities from photoidentification data can be used to model manatee population dynamics, estimate population growth rates, and provide an integrated measure of regional status.},
	language = {en},
	number = {3},
	urldate = {2021-01-07},
	journal = {Marine Mammal Science},
	author = {Langtimm, Catherine A. and Beck, Cathy A. and Edwards, Holly H. and Fick‐Child, Kristin J. and Ackerman, Bruce B. and Barton, Sheri L. and Hartley, Wayne C.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1748-7692.2004.tb01171.x},
	keywords = {photo-identification, capture-recapture, manatee, Program MARK, sighting probabilities, survival probabilities, temporal variance, Trichechus manatus latirostris},
	pages = {438--463},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/TQXXMDQF/Langtimm et al. - 2004 - Survival Estimates for Florida Manatees from the P.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/37W9NLBC/j.1748-7692.2004.tb01171.html:text/html}
}

@article{lockyer_observations_1990,
	title = {Some observations on wound healing and persistence of scars in {Tursiops} truncatus},
	url = {https://archive.iwc.int/pages/download.php?ref=472&size=&ext=pdf&k=6df7fe2a38&alternative=-1&usage=-1&usagecomment=},
	number = {12},
	journal = {Reports of the International Whaling Commission},
	author = {Lockyer, CH and Morris, RJ},
	year = {1990},
	pages = {113--118},
	file = {Full Text:/Users/b3020111/Zotero/storage/ZXL857J5/Lockyer and Morris - Some observations on wound healing and persistence.pdf:application/pdf}
}

@book{mann_cetacean_2000,
	title = {Cetacean {Societies}: {Field} {Studies} of {Dolphins} and {Whales}},
	url = {https://books.google.co.uk/books?hl=en&lr=&id=W-UQNoxMONwC&oi=fnd&pg=PR9&dq=Cetacean+Societies:+Field+Studies+of+Dolphins+and+Whales&ots=WUlSQOoeue&sig=Vb5c_Dql5ux0yH86gHkzPrurzaI#v=onepage&q=Cetacean%20Societies%3A%20Field%20Studies%20of%20Dolphins%20and%20Whales&f=false},
	urldate = {2021-01-07},
	publisher = {University of Chicago Press},
	author = {Mann, Janet and Connor, Richard C. and Tyack, Peter and Whitehead, Hal},
	year = {2000},
	file = {Cetacean Societies\: Field Studies of Dolphins and Whales - Google Books:/Users/b3020111/Zotero/storage/HK8GCY8J/books.html:text/html}
}

@misc{noauthor_photo-identification_nodate,
	title = {Photo-identification {\textbar} {Cetacea} {Association}},
	url = {http://www.associaciocetacea.org/en/research/photo-identification/},
	urldate = {2021-01-07},
	file = {Photo-identification | Cetacea Association:/Users/b3020111/Zotero/storage/SNCNV7D2/photo-identification.html:text/html}
}

@misc{noauthor_photo-identification_2015,
	title = {Photo-identification},
	url = {http://www.associaciocetacea.org/en/research/photo-identification/},
	abstract = {PHOTO-IDENTIFICATION AT ASSOCIACIÓ CETÀCEA Associació Cetàcea carries out the Photo-identification Project: Whales and dolphins along the Catalan coast since 2014. The main goal of the project is t…},
	language = {en-US},
	urldate = {2021-01-07},
	journal = {Cetacea Association},
	month = jun,
	year = {2015}
}

@book{perrin_encyclopedia_2009,
	title = {Encyclopedia of marine mammals},
	isbn = {978-0-12-373553-9},
	publisher = {Academic Press},
	author = {Perrin, W and Würsig, B. and Thewissen, JGM},
	year = {2009}
}

@article{mariani_analysis_2016,
	title = {Analysis of the natural markings of {Risso}’s dolphins ({Grampus} griseus) in the central {Mediterranean} {Sea}},
	volume = {97},
	issn = {0022-2372},
	url = {https://doi.org/10.1093/jmammal/gyw109},
	doi = {10.1093/jmammal/gyw109},
	abstract = {Risso’s dolphins are known for the persistency of their natural markings, possibly due to the loss of pigment during the healing process of skin wounds. Nonetheless, the actual longevity and reliability of each mark type has never been assessed. In this paper, we used photographs to investigate the etiology of skin marks in the species, analyze their distribution and temporal variability, and discuss implications for photo identification. Nineteen mark types were described on the dorsal fin of Risso’s dolphin, including 2 new to the literature: the snake-like mark and the protruding fat. Longevity of skin marks ranged from 6 years for the protruding fat to several decades for scrapes and dots. Persistent and reliable marks were notch, tooth-rake, and thick single and parallel scrapes. Mark change was sufficiently low that all mark types could be used for photo identification, provided that backlit or underexposed images were discarded as photographs taken under suboptimal light conditions proved to be unreliable. Finally, mark distribution and variability were unequal between age classes; juveniles were less marked and showed a higher rate of mark change than older individuals so that, even if they possessed enough notches to be classified as reliably marked, they could be confidently matched over a time interval of up to 3 years.},
	number = {6},
	urldate = {2021-01-08},
	journal = {Journal of Mammalogy},
	author = {Mariani, Monica and Miragliuolo, Angelo and Mussi, Barbara and Russo, Giovanni F. and Ardizzone, Giandomenico and Pace, Daniela S.},
	month = dec,
	year = {2016},
	pages = {1512--1524},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/DB8ZZ7TK/Mariani et al. - 2016 - Analysis of the natural markings of Risso’s dolphi.pdf:application/pdf;Snapshot:/Users/b3020111/Zotero/storage/6XHTNIJJ/2628015.html:text/html}
}

@article{rosso_colour_2008,
	title = {Colour patterns and pigmentation variability on striped dolphin {Stenella} coeruleoalba in north-western {Mediterranean} {Sea}},
	volume = {88},
	copyright = {Copyright © Marine Biological Association of the United Kingdom 2008},
	issn = {00253154},
	url = {https://search.proquest.com/docview/224126621/abstract/D21C0EDF3CA54108PQ/1},
	doi = {http://dx.doi.org/10.1017/S0025315408001641},
	abstract = {Studies on differences in external morphology and pigmentation patterns were historically carried out using stranded individuals or opportunistic sightings; few studies have involved sampling systematically free-ranging individuals. In order to investigate and describe main pigmentation characteristics, outlining 'typical' regional pigmentations, this work analysed systematic photographic information taken on free-ranging striped dolphins, Stenella coeruleoalba. Photographs of dolphins in the Ligurian Sea were collected between May 2004 and December 2006. All individuals were described by the presence/absence of pigmentation variables and by differences in colour shades. The frequency of all the pigmentation variables analysed is stable in the population (10 'gene' variables, 19 'allele' variables), and remains similar between each different group of dolphins. But population presents widespread pigmentation variability between specimens, allowing identification even at single individual level. Cluster analysis also found that the majority of the pigmentations derive from two main colour patterns, called 'mat' and 'pale' patterns (fmat = 0.68; fpale = 0.12). The Bray-Curtis index showed a high variability of the intra-group pigmentation distance between groups. This resulted in a positive correlation between group size and 'intra-group' pigmentation distance: the distance increases rapidly up to a group size of 40 individuals. According to the results obtained, the striped dolphins seem to be concentrated in small groups in which there is a large phenotypic similarity among individuals. These small units could be associated between them to form temporary large groups observed only in pelagic waters. [PUBLICATION ABSTRACT]},
	language = {English},
	number = {6},
	urldate = {2021-01-08},
	journal = {Marine Biological Association of the United Kingdom. Journal of the Marine Biological Association of the United Kingdom},
	author = {Rosso, Massimiliano and Moulins, Aurélie and Würtz, Maurizio},
	month = sep,
	year = {2008},
	note = {Num Pages: 9
Place: Cambridge, United Kingdom
Publisher: Cambridge University Press},
	keywords = {Biology},
	pages = {1211--1219},
	file = {Full Text PDF:/Users/b3020111/Zotero/storage/8WUYPLVZ/Rosso et al. - 2008 - Colour patterns and pigmentation variability on st.pdf:application/pdf}
}

@article{hammond_individual_1990,
	title = {Individual recognition of cetaceans: use of photo-identification and other techniques to estimate population parameters: incorporating the proceedings of the symposium and workshop on individual recognition and the estimation of cetacean population parameters},
	volume = {12},
	url = {http://www.vliz.be/imisdocs/publications/253951.pdf},
	journal = {International Whaling Commission},
	author = {Hammond, Philip S. and Mizroch, Sally A. and Donovan, Greg P.},
	year = {1990},
	file = {Full Text:/Users/b3020111/Zotero/storage/S9XJLGL8/Hammond et al. - 1990 - Individual recognition of cetaceans use of photo-.pdf:application/pdf}
}
