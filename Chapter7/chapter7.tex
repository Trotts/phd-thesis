\chapter{Evaluating Automatic Most Likely Catalogue Matching}\label{ch:SNNEvaluation}

In this chapter, the automated most likely catalogue matching system developed in this thesis is evaluated. Beginning by examining the effects of NDD AU SMRU dataset variation on SNN top-$N$ performance, the generalisability of the approach is then examined through the use of a second photo-id catalogue. The task of most likely catalogue matching is then framed as a standard image classification task, comparing against the approach taken throughout this thesis. Finally, the developed system is then compared against an existing photo-id aid found in the literature. 

\section{Effect of AU SMRU Data on Model Performance}\label{ch:SNNEvaluation,sec:EffectOfAUSMRU}

As outlined in Section \ref{ch:postProcessing,sec:NDD_AU_SMRU}, additional photo-id data for 23 individuals were provided by the Universities of Aberdeen and St Andrews upon completion of fieldwork in the Coquet to St. Mary's Marine Conservation Zone. Whilst this data was primarily utilised to better understand the home range of resident cetaceans, it was also used to provide additional training data for automatic most likely catalogue matching, creating the NDD AU SMRU dataset and providing a larger number of class examples for initial SNN feasibility studies. 

Work undertaken throughout Chapter \ref{ch:ID} confirmed the ability of SNNs to perform most likely catalogue matching on the combined NDD AU SMRU dataset. To understand the effect of additional training data on model performance, two models were generated. First, a model was trained only on the data collected during the fieldwork season in Northumberland, UK, contained within the Segmented NDD20 dataset as discussed in Section \ref{ch:postProcessing,sec:postProcessingNDD20}. 

As some classes within the Segmented NDD20 dataset are too small for semi-hard triplet mining to occur without extremely heavy data augmentation, only 17 classes in Segmented NDD20 were utilised here (hereafter denoted as Segmented NDD20 17). This is in contrast to the 24 classes utilised when training using the full NDD AU SMRU dataset, as discussed in Section \ref{ch:ID,sec:ModelSelection}. Next a second model was trained using the NDD AU SMRU dataset, limited to just the classes present in Segmented NDD20 17 (hereafter denoted as NDD AU SMRU 17). If both models perform equally well on the test set, it can be deduced that the inclusion of the additional data during training has not aided model generalisability. 

\begin{table}[h]
	\centering
	\begin{tabular}{cccccc}
		\hline
		\textbf{Dataset} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\Backbone\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data Augmentation\\Strategy\end{tabular}}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} \\ \cline{4-6} 
		&                                                                                    &                                                                                                & \textbf{Top-1}    & \textbf{Top-5}    & \textbf{Top-10}   \\ \hline
		Segmented NDD20 17                          & VarvaraNet                                                                         & None                                                                                           & 42.26         & 60.44         & 78.13         \\
		NDD AU SMRU 17                              & VarvaraNet                                                                       & None                                                                                           & 42.75         & 69.78        & 88.45         \\ \hline
	\end{tabular}
	\caption[Comparison between two SNNs trained for most likely catalogue matching, one using the Segmented NDD20 17 dataset, and the other using the NDD AU SMRU 17 dataset.]{Comparison between two SNNs trained for most likely catalogue matching, one using the Segmented NDD20 17 dataset, and the other using the NDD AU SMRU 17 dataset. Evaluation is performed using the NDD AU SMRU 17 test set.}
	\label{fig:effect-of-au-smru}
\end{table}

Table \ref{fig:effect-of-au-smru} shows the top-1, top-5, and top-10 accuracies for both models when evaluated on the NDD AU SMRU 17 test set. As can be seen, utilising the additional data as opposed to training solely on the Segmented NDD20 17 dataset provided a boost of 0.49\%, 9.34\%, and 10.30\% to top-1, top-5, and top-10 accuracies respectively. These results suggest that, whilst the additional data has not provided any great increase in the model's ability to distinguish the top-1 most likely individual, the extra training embeddings have allowed for more defined class clusters, greatly improving top-5 and top-10 performance. 

This supports the hypothesis that additional training examples improve most likely catalogue matching ability, even though the data was obtained from a secondary source. As such, this provides evidence to suggest that models trained on multiple photo-id catalogues may perform better than those trained on a single study, assuming there is some individual overlap.

However, performance of the model trained solely on the Segmented NDD20 17 dataset shows that even a model trained on the relatively small amount of data collected during the Northumberland fieldwork is able to greatly reduce the search space and can be utilised on data collected in a different spatio-temporal environment than the one on which it was trained, highlighting the robustness of the approach. 

\section{Effect of Background on Embedding Generation}\label{ch:SNNEvaluation,sec:EffectOfNoise}

Due to the free roaming nature of cetaceans, those in the NDD AU SMRU dataset were often photographed during only a single encounter leading to data with small intra-class but high inter-class background variation. This is similar to domains such as person re-identification, where images representing different classes often contain the same background information due to capture with a stationary camera. Recent work in this domain has shown that deep learning models may bias their similarity rankings based on background information \cite{tian_eliminating_2018}. 

To examine the effect that background removal has on downstream identification in the photo-id domain, an SNN was trained using the NDD AU SMRU dataset processed into bounding box class examples. All variables except the presence of background were kept consistent with those used when training the best performing masked SNN, a VarvaraNet without the use of any data augmentations (see Table \ref{fig:NDDAUSMRU-SNN-model-accuracies}). Data for this experiment was generated using the same Mask R-CNN detector as for previous experiments, modified to output bounding boxes rather than masks. Corresponding dorsal fin masks for each bounding boxed image were taken from the NDD AU SMRU dataset, with background masks generated by inverting the fin mask. 

The bounding boxed fin, fin mask, and background mask were then embedded into the model's latent space, and the Euclidean distances between them calculated. This analysis showed that embedding generation is likely to be influenced more by features in the background than the fin. For example, the Euclidean distance between the bounding box data in Figure \ref{fig:bboxvsmask} (Left) and its corresponding dorsal fin mask (Centre) is 0.36, compared to a distance of 0.30 between the bounding box and the background mask (Right) and a mean distance of 0.97 between the bounding box and generated class prototypes. 

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.5]{Chapter7/figs/embedding-check-images.jpg}
	\end{center}
	\caption[Example data used to examine the effect of retained background on most likely catalogue matching.]{Example data used to examine the effect of retained background on most likely catalogue matching. (Left) Bounding box detection containing both a dorsal fin and background. (Centre) Corresponding dorsal fin mask. (Right) Corresponding background mask.}
	\label{fig:bboxvsmask}
\end{figure}

This suggests the SNN is performing likely matching based on features found in the background rather than on the dorsal fins, reflected in the increased model performance as seen in Table \ref{fig:box-vs-best-masked} -- which shows the SNN trained using the bounding boxed data sees an increase of 22.94\% top-1, 15.58\% top-5, and 6.53\% top-10 accuracies when compared to the best performing SNN trained on masked data.

\begin{table}[!h]
	\centering
	\begin{tabular}{cccccc}
		\hline
		\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Backbone\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data Augmentation\\ Strategy\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data\\ Format\end{tabular}}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}}    \\ \cline{4-6} 
		&                                                                                                & & \textbf{Top-1}     & \textbf{Top-5}     & \textbf{Top-10}    \\ \hline
		VarvaraNet & None                                                                       & Bounding Boxed                                                                                           & 63.79          & 84.48          & 89.66          \\
		VarvaraNet & None                                                                       & Masked                                                                                           & 40.85          & 68.90          & 83.13          \\
		 \hline
	\end{tabular}
	\caption[Comparison of the Top-1, Top-5, and Top-10 accuracies between training an SNN on bounding boxed or masked photo-id data.]{Comparison of the Top-1, Top-5, and Top-10 accuracies between training an SNN on bounding boxed or masked photo-id data. Metrics for the model trained using the masked data are for the best performing SNN as found in Table \ref{fig:NDDAUSMRU-SNN-model-accuracies}.}
	\label{fig:box-vs-best-masked}
\end{table}

By removing all background, the masked SNN is prevented from utilising environmental conditions to aid matching. This finding raises important questions regarding the performance of photo-id aids which do not remove all background before performing matching. Section \ref{ch:Background,sec:conTech,sub:photoIDAides,subsub:Summary} provides a summary of currently available photo-id aids. Of these, four works (Bouma \textit{et al.} \cite{bouma_individual_2018}, Lee \textit{et al.} \cite{lee_backbone_2020}, finFindR \cite{thompson_finfindr_2022}, and DolFin \cite{maglietta_dolfin_2018}) perform dorsal fin detection and downstream individual identification, however only DolFin removes all background beforehand. If the photo-id catalogue utilised for evaluation of these systems has been collected over a small temporal scale, such that a temporally-robust train-test split cannot be achieved, then results obtained in this experiment suggest that performance may be artificially inflated by the retention of feature heavy background. 

\section{Examining Siamese Neural Network Catalogue Matching Generalisability }\label{ch:SNNEvaluation,sec:SDRP}

As outlined in Chapter \ref{ch:ID}, an automatic approach to most likely catalogue matching was developed through the use of SNNs. This approach, when tested using the NDD AU SMRU dataset developed in Section \ref{ch:postProcessing,sec:NDD_AU_SMRU}, yields high top-1, top-5, and top-10 accuracies. However, it is not yet clear if these results are to be expected regardless of the photo-id catalogue utilised, or if there is an underlying property inherent to the NDD AU SMRU dataset that makes it particularly susceptible to an SNN-based approach. In this chapter, automatic most likely matching is performed on a second, previously unseen, photo-id catalogue, allowing for an evaluation of the approach's generalisability.

\subsection{The SDRP Dataset}\label{ch:SNNEvaluation,sec:SDRP,sub:SDRPDataset}

To evaluate SNN generalisability, a subset of photo-id catalogue data was obtained from the Chicago Zoological Society's Sarasota Dolphin Research Program (SDRP)\nomenclature[z-SDRP]{SDRP}{Sarasota Dolphin Research Program}. This subset consisted of 250 images of 23 individual common bottlenose dolphins captured in the waters around Naples, FL, USA \cite{tyson_moore_final_2020}. Unlike the datasets collected from fieldwork in Northumberland, UK, the SDRP dataset was provided in a pre-processed form as the dataset had been previously utilised to compare photo-id methodologies \cite{tyson_moore_rise_2022}. Images were provided in a cropped format, removing a large amount of background noise and centring the dorsal fin, examples of which can be seen in Figure \ref{fig:sdrp-example}. 

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.3]{Chapter7/figs/SDRP_egs_tiled.png}
	\end{center}
	\caption{Example images from the SDRP dataset with filenames displayed.}
	\label{fig:sdrp-example}
\end{figure}

The SDRP data was provided pre-split, with 200 images (each of a unique individual) acting as the existing photo-id catalogue, with the remaining 50 serving as images captured during a given day's fieldwork. Each image in the encounter set contained a single individual, however some individuals were captured multiple times. As such, there was a 23 individual overlap between the catalogue and encounter sets. 

To generate a train-test split capable of training an SNN, the catalogue set was reduced down to contain only the 23 individuals contained within the encounter set. Once filtered, both sets of images were ran through the Mask R-CNN dorsal fin detector and post processed using morphological transformations, colour thresholding, and cropping -- outlined in Chapter \ref{ch:postProcessing}. No images in this dataset had been seen by the detector previously, either during training or evaluation. Once generated, a \texttt{noise} class was manually created which contained all erroneously detected mask components. However, as the detector failed to accurately detect examples of individual \texttt{19} this class was removed, resulting in a final dataset consisting of 23 classes (22 individuals plus \texttt{noise}). In cases where the detector had mistakenly detected the same fin twice, provided the two masks were not identical then both were kept -- analogous to offline data augmentation. An example of this can be seen in Figure \ref{fig:sdrp-double-mask-eg}.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.5]{Chapter7/figs/SDRP-double-mask-eg-indv-13.png}
	\end{center}
	\caption[Left: an image of individual \texttt{13} from the original SDRP catalogue set. Right: example masks generated for the Left image.]{Left: an image of individual \texttt{13} from the original SDRP catalogue set. Right: example masks generated for the Left image. Both masks are kept for use in training as they were deemed to be sufficiently different.}
	\label{fig:sdrp-double-mask-eg}
\end{figure}

After detection and processing the resultant SDRP dataset contained a total of 123 images, significantly smaller than the NDD AU SMRU dataset which was used to evaluate the SNN-based approach previously. Retaining the split provided by the SDRP, whereby the train set was generated from the catalogue and the test set from the encounter, leads to a 35-65 train-test split, an inversion of what would be expected when training machine learning models. The class distribution for the SDRP dataset can be seen in Figure \ref{fig:sdrp-dist}. As with the NDD AU SMRU dataset, the \texttt{noise} class is once again dominant. A colour threshold of 50\% was again utilised during post-processing with no correct detections erroneously discarded, suggesting this is an acceptable general value.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.38]{Chapter7/figs/SDRP-class-dist.png}
	\end{center}
	\caption{The class distribution for the SDRP dataset, split by set.}
	\label{fig:sdrp-dist}
\end{figure}

These properties lead to the SDRP dataset being extremely challenging for an SNN to train on. However it is also an accurate representation of what a real life photo-id catalogue dataset would look like in the initial stages of a survey, providing an excellent test of both the robustness and generalisability of the SNN-based approach to automatic most likely catalogue matching when only small amounts of training data are available. 

\subsection{Evaluation Using the SDRP Dataset}\label{ch:SNNEvaluation,sec:SDRP,sub:SNNEvalWithSDRP}

Due to the small amount of data, it was not possible to create a meaningfully large and diverse validation set for the SDRP dataset. This prevented hyperparameter optimisation, and as such the decision was made to utilise the optimal hyperparameters located for the best performing SNN on the NDD AU SMRU dataset, alongside the same backbone architectures and data augmentation strategies defined in Section \ref{ch:ID,sec:SNNDevelopment}.

The results of model training on the SDRP dataset can be seen in Table \ref{fig:SDRP-normal-split-model-comparison}, with each model evaluated using top-1, top-5, and top-10 accuracies.
Higher scores were achieved across the board on the SDRP dataset compared to the NDD AU SMRU dataset even without hyperparameter optimisation, however it is important to remember the smaller number of possible classes for the model to choose from which may inflate relative model performance.

\begin{table}[!h]
	\centering
	\begin{tabular}{ccccc}
		\hline
		\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Backbone\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data Augmentation\\ Strategy\end{tabular}}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}}    \\ \cline{3-5} 
		&                                                                                                & \textbf{Top-1}     & \textbf{Top-5}     & \textbf{Top-10}    \\ \hline
		EmbeddingNet                                                                       & None                                                                                           & 72.50          & 91.25          & 95.00          \\
		VarvaraNet                                                                         & None                                                                                           & 72.50          & 85.00          & 92.50          \\
		\textbf{EmbeddingNet}                                                              & \textbf{Colour Jitter}                                                                         & \textbf{81.25} & \textbf{95.00} & \textbf{97.50} \\
		VarvaraNet                                                                         & Colour Jitter                                                                                  & 70.00          & 91.25          & 96.25          \\
		EmbeddingNet                                                                       & Perspective Shift                                                                              & 62.50          & 92.50          & 97.50          \\
		VarvaraNet                                                                         & Perspective Shift                                                                              & 73.75          & 91.25          & 96.25          \\
		EmbeddingNet                                                                       & \begin{tabular}[t]{@{}c@{}}Perspective Shift \&\\ Colour Jitter\end{tabular}                    & 71.25          & 92.50          & 96.25          \\
		VarvaraNet                                                                         & \begin{tabular}[t]{@{}c@{}}Perspective Shift \&\\ Colour Jitter\end{tabular}                    & 63.75          & 88.75          & 97.50          \\ \hline
	\end{tabular}
	\caption[Results of SNN training for the task of most likely catalogue matching on the SDRP dataset.]{Results of SNN training for the task of most likely catalogue matching on the SDRP dataset. The best performing model is highlighted in bold.}
	\label{fig:SDRP-normal-split-model-comparison}
\end{table}

Unlike the NDD AU SMRU data where best results were achieved without any augmentation, here the results are more mixed. Whilst the best top-10 accuracy, 97.50\%, is obtained using Colour Jitter and Perspective Shift augmentations (both together and separately), the best top-5 and top-1 accuracies were obtained using Colour Jitter only. These findings suggest that data augmentation strategy may be catalogue dependent and have a large impact on final model performance, and as such a search of possible data augmentation strategies should be performed each time an SNN model is trained using a new or updated photo-id catalogue. 

Variation in backbone architecture had little effect on overall model performance. Interestingly models trained using a VarvaraNet backbone were more consistent, with a 10.00\% difference between the best and worst performing model compared to an 18.75\% difference between those trained using an EmbeddingNet backbone. Overall however, the best performing model was determined to be an SNN using an EmbeddingNet backbone architecture and Colour Jitter data augmentation, which achieved 81.25\% top-1, 95.00\% top-5, and 97.50\% top-10 accuracies. This is in contrast to the best performing NDD AU SMRU model, made up of a VarvaraNet backbone architecture without any data augmentation. Using the optimal NDD AU SMRU model setup achieved 72.50\% top-1, 85.00\% top-5, and 92.50\% top-10 accuracies on the SDRP dataset. The use of an EmbeddingNet backbone as optimal for this dataset suggests that a simpler model structure may be best when working with smaller catalogues.

\subsection{SDRP Uncatalogued Individual Thresholding}\label{ch:SNNEvaluation,sec:SDRP,sub:uncatalogued}

Evaluation of the best performing SDRP model's ability to flag uncatalogued individuals was undertaken. Unlike the model trained on the NDD AU SMRU dataset, a write-up of which is provided in Section \ref{ch:ID,sec:ModelSelection}, when trained on the SDRP dataset embedded images are placed closer together in the latent space. This means that the threshold values utilised for uncatalogued individual detection using prototype distance measurement and K-Nearest Neighbours (KNN) required modification to accurately output the necessary warnings. 

Previous experimentation determined it was possible to flag potentially uncatalogued individuals by measuring the distance between an embedded image and its closest class prototype. For the NDD AU SMRU dataset, a minimum distance of 4.0 was required before a warning was displayed. Analysis of the distances between SDRP image embeddings and class prototypes however determined that this value was too large to be used with this latent space, as warnings were displayed for only 25.37\% of uncatalogued individuals when used. Through empirical examination of the distances between each image and the class prototypes, in agreement with the related literature \cite{battle_siamese_2022}, a threshold of 0.15 was determined to be optimal for the SDRP dataset, being small enough to trigger a warning for uncatalogued individuals whilst still being large enough not to trigger in error.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Chapter7/figs/uncatalogued-individual-thresholding-updated.png}
	\end{center}
	\caption[Example uncatalogued individual thresholding for the SDRP dataset using an individual not present during training.]{Example uncatalogued individual thresholding for the SDRP dataset using an individual not present during training. Left: the input image seen by the model, taken from the NDD AU SMRU dataset.  Top Middle: The resultant Euclidean distances between the input image's embedding and the existing class prototypes, with the minimum distance threshold set to 4.0. No warning has been generated. Top Right: The resultant Euclidean distances between the input image's embedding and the existing class prototypes, with the minimum distance threshold set to 0.15. A warning has been generated. Bottom Middle: Uncertainty scores generated using K-Nearest Neighbours clustering, with $K = 10$. A warning has been generated using an uncertainty threshold of $>=30\%$. Bottom Right: Uncertainty scores generated using K-Nearest Neighbours clustering, with $K = 5$. A warning has been generated using an uncertainty threshold of $>=30\%$.}
	\label{fig:uncatalogued-individual-example-sdrp}
\end{figure}

Alongside prototype distance measurement, KNN can also be utilised for uncatalogued individual warning generation. Rather than measuring distances between embeddings as with prototype thresholding, here warnings are generated by calculating an uncertainty score based on the class labels of the $K$ nearest embeddings. When processing the NDD AU SMRU dataset it was determined that setting $K$ to 10 was sufficient, producing a warning when no single class made up 30\% or less of the nearest class labels. 

For the SDRP dataset, whilst an uncertainty of 30\% was found to be sufficient for generating a warning, setting $K$ to 10 was deemed too high. Utilising this value resulted in some catalogued individuals incorrectly producing a warning as a result of the latent space's more compact nature. Evaluation of each test image's neighbours in the latent space determined that setting $K$ to 5 was optimal. Using the updated values, warnings were displayed for 39.06\% of uncatalogued images, an increase of 13.69\% over utilising the NDD AU SMRU dataset thresholds. This reduction in warning generation efficacy when compared to results in Section \ref{ch:ID,sec:ModelSelection,subsec:UncataloguedIndividualThresholding} is likely due to the small number of images in the SDRP dataset, resulting in a more compressed latent space.

Example warning generation using both prototype distance measurement and KNN for an uncatalogued individual can be seen in Figure \ref{fig:uncatalogued-individual-example-sdrp}. Here, an example from the NDD AU SMRU dataset has been processed by the SNN trained on the SDRP dataset, and so warnings should be generated both by prototype distance measurement and KNN. As can be seen, when utilising a minimum prototype distance of 4.0 no warning is generated. Changing this value to 0.15 however ensures a warning is generated. When checking using KNN, warnings are produced both when $K$ is set to 10 and 5. Figure \ref{fig:catalogued-individual-example-sdrp} shows example warning generation for an individual present during SNN training on the SDRP dataset. No warning has been generated with either minimum prototype distances, as expected. When $K$ is set to 10 however, a warning is erroneously generated. This is prevented by setting $K$ to 5. 

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Chapter7/figs/catalogued-individual-thresholding-updated.png}
	\end{center}
	\caption[Example uncatalogued individual thresholding for the SDRP dataset using an individual present during training.]{Example uncatalogued individual thresholding for the SDRP dataset using an individual present during training. Left: the input image seen by the model, taken from the SDRP dataset. Top Middle: the resultant Euclidean distances between the input image's embedding and the existing class prototypes, with the minimum distance threshold set to 4.0. Top Right: the resultant Euclidean distances between the input image's embedding and the existing class prototypes, with the minimum distance threshold set to 0.15. No warning has been generated. Bottom Middle: uncertainty scores generated using K-Nearest Neighbours clustering, with $K = 10$. A warning has been generated using an uncertainty threshold of $>=30\%$. Bottom Right: uncertainty scores generated using K-Nearest Neighbours clustering, with $K = 5$. No warning has been generated using an uncertainty threshold of $>=30\%$.}
	\label{fig:catalogued-individual-example-sdrp}
\end{figure}

Based on the experimentation described previously, it can be seen that whilst uncatalogued individual detection can be performed on the SDRP catalogue using both prototype distance measurement and KNN as with the NDD AU SMRU catalogue, evidence suggests that threshold values for both techniques are catalogue dependent and require tuning before use. Whilst not tested here, it is thus likely that tuning is required if the model is retrained on an updated catalogue due to the change in data distribution. Further, whilst the KNN threshold values for flagging noise and uncatalogued individuals, $>=70\%$ and $<=30\%$  of an embedding's nearest neighbours respectively, were found to be sufficient for both catalogues examined in this work, there is no guarantee that this is the case for all catalogues globally. When adapting this work to a new catalogue, it is advised that consideration is given to all threshold values used by both prototype distance measurement and KNN for the task of uncatalogued individual detection. It may be the case that this tuning can be automated by utilising the properties of the latent space to determine sufficient thresholds based on the training data, an approach which should be explored in future.

\subsection{Evaluating the NDD AU SMRU SNN Using the SDRP Dataset}\label{ch:SNNEvaluation,sec:SDRP,sub:NDDuncatalogued}

As highlighted in Section \ref{ch:ID,sec:ModelSelection,subsec:UncataloguedIndividualThresholding}, the SNN created using the NDD AU SMRU dataset is able to generate warnings for 67.86\% of potentially previously uncatalogued individuals resident in the Northumberland survey area (see Section \ref{ch:datasetCreation,sec:NDD,sub:surveyArea}) through the use of prototype distance measurements and KNN. Using the SDRP dataset, experimentation was undertaken to examine if the Northumberland SNN was capable of generating warnings for the individuals and noise present in the SDRP dataset, without re-training or fine-tuning. 

Using the threshold values for the NDD AU SMRU-trained SNN (see Section \ref{ch:ID,sec:ModelSelection,subsec:UncataloguedIndividualThresholding}), warnings highlighting potentially previously uncatalogued individuals were generated for 79.25\% of images containing dorsal fins of individuals present in the SDRP dataset. Further, warnings highlighting potential noise were generated for 85.19\% of SDRP dataset images which were labelled as \texttt{noise}. These results highlight the strengths of utilising SNNs and their generated latent spaces to flag both potentially previously uncatalogued individuals and noise.

\subsection{Effect of Training Set Size on Model Backbone Selection}\label{ch:SNNEvaluation,sec:SDRP,sub:SDRPDataset,sub:reversedSDRP}

 As previously mentioned, the SDRP catalogue was provided pre-split which, once post-processed, produced a 35-65 train-test divide. As this training set is much smaller than what would normally be expected for developing a deep learning model, experimentation was undertaken to evaluate the hypothesis that a simpler model structure is best when low volumes of catalogue data are available during training. To this end, the dataset splits were reversed such that the test set was used for model training and the train set was used for evaluation. For consistency, the same model backbones and data augmentation strategies were utilised during training, as were the optimal NDD AU SMRU hyperparameters. 
 
The top-1, top-5, and top-10 accuracies for the trained models can be seen in Table \ref{fig:SDRP-reversed-split-model-comparison}. By reversing the train-test split, a drop in performance is observed for all bar one of the models trained using an EmbeddingNet backbone. For the single model where performance improves, increases of 11.92\% top-1 and 0.18\%  top-10 accuracies are observed, however a drop of 1.80\% top-5 accuracy is also seen. For those models trained using a VarvaraNet backbone, increases in performance are observed for all models, with only two reporting slight drops in top-5 accuracy. 

 \begin{table}[]
	\centering
	\begin{tabular}{ccccc}
		\hline
		\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Backbone\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data Augmentation\\ Strategy\end{tabular}}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}}      \\ \cline{3-5} 
		&                                                                                                & \textbf{Top-1}     & \textbf{Top-5}      & \textbf{Top-10}     \\ \hline
		EmbeddingNet                                                                       & None                                                                                           & 67.44          & 90.70           & 90.70           \\
		VarvaraNet                                                                         & None                                                                                           & 88.37          & 100.00          & 100.00          \\
		EmbeddingNet                                                                       & Colour Jitter                                                                                  & 69.77          & 88.37           & 90.70           \\
		VarvaraNet                                                                         & Colour Jitter                                                                                  & 72.09          & 88.37           & 97.67           \\
		EmbeddingNet                                                                       & Perspective Shift                                                                              & 74.42          & 90.70           & 97.68           \\
		VarvaraNet                                                                         & Perspective Shift                                                                              & 74.42          & 88.37           & 95.35           \\
		EmbeddingNet                                                                       & \begin{tabular}[t]{@{}c@{}}Perspective Shift \&\\ Colour Jitter\end{tabular}                    & 60.47          & 88.37           & 90.70           \\
		\textbf{VarvaraNet}                                                                & \textbf{\begin{tabular}[t]{@{}c@{}}Perspective Shift \&\\ Colour Jitter\end{tabular}}           & \textbf{90.70} & \textbf{100.00} & \textbf{100.00} \\ \hline
	\end{tabular}
	\caption[Results of SNN training for the task of most likely catalogue matching on the reversed SDRP dataset.]{Results of SNN training for the task of most likely catalogue matching on the reversed SDRP dataset. The best performing model is highlighted in bold.}
	\label{fig:SDRP-reversed-split-model-comparison}
\end{table}

In general, it can be observed that the increase in training data has led to an overall performance boost for models trained using a VarvaraNet backbone whilst those with an EmbeddingNet backbone observe a performance drop. As such, the best performing model for the reversed split SDRP dataset now makes use of a VarvaraNet backbone alongside both Colour Jitter and Perspective Shift data augmentation strategies. This supports the hypothesis that backbone architecture selection is influenced by the size of the initial training set, a finding backed up by other works in the area \cite{dutta_evaluation_2018, sug_effect_2010}. Further, it seems to be the case that more complex models are required to fully capture the fine-grained nature of larger photo-id catalogues. These findings also provide further evidence to support the idea that data augmentation strategies are catalogue dependent. 

It should be noted however that the best performing model achieves 100\% top-5 and top-10 accuracies, suggesting the possibility of model overfitting. Whilst the model may be capable of always providing a match within the first five results, this is likely a product of dataset size causing an inflation in model performance. It is expected that these accuracies would decrease when utilising more data, either through an increase in the number of classes or examples per class, as this would provide an increase in data variation.

\section{Considering Catalogue Matching as a Standard Classification Task}\label{ch:SNNEvaluation,sec:comparsion}

In order to better understand the performance of SNNs for the task of most likely catalogue matching, an evaluation was undertaken whereby the task was approached as a standard image classification problem. Using the NDD AU SMRU dataset, multiple backbone architectures (ResNet50, ResNet101 \cite{he_deep_2015}, VGG16 \cite{simonyan_very_2015}, EmbeddingNet, and VarvaraNet) were trained and evaluated against top-1, top-5, and top-10 accuracy metrics. This range was chosen to help identify whether model depth or setup has an effect on classification performance. 

As these models only require a single example image per training step, rather than a triplet of images as required when training SNNs with Triplet Ranking Loss, Cross Entropy Loss was instead utilised. This function compares the predicted class probabilities outputted by the model to those of the ground truths. A loss, $L$, is computed using Equation \ref{eq:crossEntropyLoss} (where $n$ is the number of classes, $t_{i}$ is the ground truth label for the $i^{th}$ class, and $p_{i}$ is the Softmax probability for the $i^{th}$ class) which in turn can be used to inform how the weights of the model should be changed. A model which performs perfectly would yield a Cross Entropy Loss of 0. 

\begin{equation}
	\label{eq:crossEntropyLoss}
	L = -\sum_{i=1}^{n} t_{i} log(p_{i})
\end{equation}

Bayesian hyperparameter optimisation during model training was performed using the Optuna framework \cite{akiba_optuna_2019}, as in Section \ref{ch:ID,sec:SNNDevelopment,sub:Optuna}. Due to the required change in loss function, the total number of hyperparameters required by the models was reduced. The search aimed to find an optimal \texttt{log uniform} learning rate between $1\times10^{-6}$ and $1\times10^{-3}$, a dropout \cite{srivastava_dropout:_2014} \texttt{uniform} probability between 0.1 and 0.7, a \texttt{log uniform} weight decay between $1\times10^{-6}$ and $1\times10^{-1}$, an \texttt{int} step size between 5 and 10, and a $\gamma$ \texttt{log uniform} value between $1\times10^{-3}$ and $1\times10^{-1}$. The learning rate optimiser was also tuned, allowing for a \texttt{categorical} choice between SGD or Adam \cite{kingma_adam:_2014}. When optimising ResNet50, ResNet101, and VGG16, hyperparameter optimisation included a \texttt{categorical} choice of starting from a pre-trained state (utilising ImageNet \cite{krizhevsky_learning_2009} weights) or not.

A total of 100 iterations were undertaken, with the optimal hyperparameters for each final model architecture decided by its best performing trial. The optimal hyperparameters for each model architecture can be seen in Appendix \ref{app:standard}. Due to memory constraints, the number of samples per class for each training batch was reduced from 9 for SNNs to 3 for ResNet50 and 2 for all other models.

The top-1, top-5, and top-10 accuracies for the best performing models can be seen in Table \ref{tab:optunaBestParamsStandard}. Compared to the best performing SNN, these model architectures all struggle to achieve high top-1 accuracy on the NDD AU SMRU dataset. For comparison, the best performing SNN trained on this dataset achieves 36.78\%, 54.47\%, and 55.28\% higher top-1, top-5, and top-10 accuracies respectively. The results provided suggest that shallower models generally perform best on this data, with the shallowest model, the EmbeddingNet, achieving the highest top-1 and top-5 accuracies, with its top-10 accuracy being only narrowly behind VarvaraNet model (the second shallowest architecture). The poor accuracy achieved here by the deeper models is hypothesised to be due to the extreme fine-grained nature and high intra-class variability of the dataset, as well as the relative lack of data to train such deep models causing overfitting. 

\begin{table}[]
	\centering
		\begin{tabular}{cccc}
			\hline
			\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Architecture\end{tabular}}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} \\ \cline{2-4} 
			& \textbf{Top-1}    & \textbf{Top-5}    & \textbf{Top-10}   \\ \hline
			ResNet50 \cite{he_deep_2015}                                                           & 3.46          & 11.79         & 27.03         \\
			ResNet101 \cite{he_deep_2015}                                                          & 4.07          & 14.43         & 27.85         \\
			VGG16 \cite{simonyan_very_2015}                                                        & 2.64          & 15.45         & 33.54  \\ 
			EmbeddingNet & 16.06 & 35.16 & 50.41 \\
			VarvaraNet \cite{vetrova_hidden_2018} & 6.10 & 29.67 & 58.54 \\\hline     
		\end{tabular}
	\caption[Top-$N$ accuracies of the best performing image classification models on the NDD AU SMRU dataset.]{Top-$N$ accuracies of the best performing image classification models on the NDD AU SMRU dataset.}
	\label{tab:optunaBestParamsStandard}
\end{table}

Examining the Bayesian optimised hyperparameter tuning runs suggests that starting from a pre-trained state is the most important indicator of model performance when utilising a ResNet architecture for this task. An example of this can be seen in Figure \ref{fig:resnet50baseline-hyperparam-importance-optuna} which shows the hyperparameter importances for the ResNet50 tuning run, including a pre-training importance of 0.81 -- for ResNet101 this value was 0.72. Whilst it is expected that this hyperparameter would have high importance due to the relatively small size of the NDD AU SMRU dataset and deepness of the architectures, the lack of importance for all other hyperparameters may suggest that this model setup is incapable of identifying meaningful features which allow the models to consistently differentiate between classes. 

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.4]{Chapter7/figs/resnet50baseline-hyperparam-importance-optuna-updated.png}
	\end{center}
	\caption{Importance scores for the hyperparameter tuning of the ResNet50 model.}
	\label{fig:resnet50baseline-hyperparam-importance-optuna}
\end{figure}

When training the VGG16 model however, the optimal model found did not require pre-training, with a hyperparameter importance of 0.02. Instead, weight decay was the most important hyperparameter with an importance of 0.39 as seen in Figure \ref{fig:vgg16baseline-hyperparam-importance-optuna}. This lack of pre-training importance may be due to the shallower depth of the architecture. However, as seen in Table \ref{tab:optunaBestParamsStandard}, the VGG model also did not achieve high accuracies on the NDD AU SMRU dataset. 

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.4]{Chapter7/figs/vgg16-baseline-hyperparam-importance-optuna-updated.png}
	\end{center}
	\caption{Importance scores for the hyperparameter tuning of the VGG16 model.}
	\label{fig:vgg16baseline-hyperparam-importance-optuna}
\end{figure}

These findings suggest that the use of triplets when training SNNs may allow for more meaningful features to be extracted, leading to more accurate most likely catalogue matching and top-1 accuracy. As such, this indicates that the use of an SNN to generate embeddings, plotting these into a latent space, and utilising clusters to provide most likely matches is a superior approach to training a standard image classifier, notwithstanding the difficulties which would arise if individuals not seen at train time were required to be classified.

\section{Comparison Against Related Work}\label{ch:SNNEvaluation,sec:FinFindRComparison}

Multiple photo-id aids have been proposed in recent years, each with varying degrees of autonomy as highlighted in Section \ref{ch:Background,sec:conTech,sub:photoIDAides}. According to a survey by Tyson Moore \textit{et al.} \cite{tyson_moore_rise_2022}, the most commonly utilised photo-id aid by cetacean researchers is finFindR \cite{thompson_finfindr_2022}. As such, an evaluation between finFindR and the system proposed in this thesis was undertaken. The latest full release of finFindR, version 0.1.10\footnote{\label{footnote:finFindR}finFindR version 0.1.10: \href{https://github.com/haimeh/finFindR/releases/tag/0.1.10}{github.com/haimeh/finFindR/releases/tag/0.1.10}}, was utilised during this evaluation. 

Like the methodology outlined in this thesis, finFindR detects dorsal fins in unedited field imagery before passing these to an identification model. However, unlike the Mask R-CNN used in this thesis which outputs masked detections like those seen in Figure \ref{fig:bboxvsmask} (Centre), dorsal fin detection in finFindR is performed using a ResNet \cite{he_deep_2015} model to produce bounding box outputs like those seen in Figure \ref{fig:bboxvsmask} (Left). Identification of individuals in both finFindR and this thesis is performed through embedding generation and comparison -- though whilst embeddings in this thesis are generated using all available identifying information, finFindR creates embeddings using the trailing edge of the dorsal fin only.

Evaluation was performed using the unedited images which make up the NDD AU SMRU dataset (outlined in Section \ref{ch:postProcessing,sec:NDD_AU_SMRU}). Whilst finFindR does not provide the ability to fine-tune its models using a user-provided photo-id catalogue, it does require both a query and reference set in order to perform matching. To facilitate a fair comparison, the unedited field images used to generate the query and reference sets for finFindR respectively were the same as those used to generate the test and train sets for SNN evaluation (outlined in Sections \ref{ch:ID,sec:SNNDevelopment} and \ref{ch:ID,sec:ModelSelection}). 

Images in the query and reference sets were first passed through finFindR's detection model to crop out any dorsal fins present. The default options for crop threshold (0.4) and crop type (Body \& Fin) were utilised. Each set's dorsal fins were then manually labelled with their individual ID number. As finFindR does not require the use of a \texttt{noise} class, any false-positive detections were removed. During this stage it was noted that finFindR failed to detect dorsal fins which were close to the camera, surrounded by large amounts of water splash, or where most of the dolphin's body had breached the waterline. In comparison, the Mask R-CNN model developed in this thesis was able to accurately detect fins in these conditions. Example images containing dorsal fins which finFindR failed to detect, alongside the crop produced by the Mask R-CNN, can be seen in Figure \ref{fig:finFindR-missed-fins-caught-by-mask-r-cnn}. As a result of this, and the lack of \texttt{noise} class, the reference set used to evaluate finFindR's identification ability is smaller than the test set used for SNN evaluation, with finFindR missing 19.03\% of dorsal fins present.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.8]{Chapter7/figs/finFindR-missed-fins-caught-by-mask-r-cnn.png}
	\end{center}
	\caption[Example dorsal fins which were not detected by finFindR, but were detected by the Mask R-CNN model developed in this thesis.]{Example dorsal fins which were not detected by finFindR, but were detected by the Mask R-CNN model developed in this thesis. Left: the original image provided to both finFindR and the Mask R-CNN. Right: the outputted dorsal fin from the Mask R-CNN after post-processing. A border has been added, and images resized, for clarity.}
	\label{fig:finFindR-missed-fins-caught-by-mask-r-cnn}
\end{figure}

Once the images in both the query and reference sets had been cropped, they were then passed through finFindR's identification model which aims to trace around the dorsal fin and generate an embedding of the trailing edge. Whilst this generally worked well, there were some instances where the model failed to produce an accurate trace. This often occurred when areas of swell were near the fin; examples of this can be seen in Figure \ref{fig:finFindR-bad-traces}. Though finFindR allows for users to edit the traces produced, this was not performed during this evaluation so as not to influence the program's output.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.5]{Chapter7/figs/ffr-bad-traces.png}
	\end{center}
	\caption[Examples of swell interfering with the finFindR tracing algorithm.]{Examples of swell interfering with the finFindR tracing algorithm. Traces are highlighted in red.}
	\label{fig:finFindR-bad-traces}
\end{figure} 

As the photo-id aid developed in this thesis makes use of a Mask R-CNN rather than a bounding box detector, the need to trace around the fin is not required, reducing the potential for error and interference from retained background noise during the matching process. Further, as each fin is segmented during the developed post-processing methodology, the chance that a crop contains multiple fins is reduced, increasing the likelihood that all fins in the original input image are either identified or flagged as potentially previously uncatalogued. As finFindR works with bounding boxes, there is a chance that multiple fins could be present in a crop, allowing for the possibility that the wrong fin if traced, or that multiple fins are traced as one. 

Once the dorsal fins present in both sets were traced and embedded, top-$N$ accuracy metrics for finFindR could be obtained. Here, the reference set acts as the catalogue which has been previously embedded, analogous to the SNN's training set, whilst the query set acts as the test set. The top-1, top-5, and top-10 accuracies obtained for both finFindR and the automated photo-id aid outlined in this thesis are shown in Table \ref{tab:ffr-vs-snn}.

\begin{table}[]
	\centering
	\begin{tabular}{cccc}
		\hline
		\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Photo-id Aid\end{tabular}}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} \\ \cline{2-4} 
		& \textbf{Top-1}    & \textbf{Top-5}    & \textbf{Top-10}   \\ \hline
		FinFindR \cite{thompson_finfindr_2022}  & 8.00          & 45.78         & 68.00         \\
		This Thesis & 40.85 & 68.90 & 83.13 \\\hline     
	\end{tabular}
	\caption[Top-$N$ accuracies obtained on the images which make up the NDD AU SMRU dataset for both finFindR and the automated photo-id aid outlined in this thesis.]{Top-$N$ accuracies obtained on the images which make up the NDD AU SMRU dataset for both finFindR and the automated photo-id aid outlined in this thesis (see Table \ref{fig:NDDAUSMRU-SNN-model-accuracies}).}
	\label{tab:ffr-vs-snn}
\end{table}

For all measured accuracies the photo-id system outlined in this thesis outperforms finFindR, reporting 32.85\%, 23.12\%, and 15.13\% higher top-1, top-5, and top-10 accuracies respectively. These results, alongside finFindR failing to detect 19.03\% of all dorsal fins in the images provided, highlight the strengths of the approach outlined in this thesis over the leading photo-id aid currently in use \cite{tyson_moore_rise_2022}. These improvements in re-identification likely arise from the ability of this thesis' developed photo-id aid to remove all background and individually segment fins into their own images, reducing the likelihood of embedding generation being adversely affected by noise. 

It should also be noted that finFindR currently does not have the ability to explicitly flag to users when a fin may be potentially previously uncatalogued, as in this thesis. Instead finFindR alerts uses implicitly stating that ``when matches are not found using finFindR (not present in the top 50 ranked images), researchers can either choose to manually search the entire catalog for a match or call the image a previously unseen individual. If researchers do the latter, and assuming photograph quality and fin distinctiveness are comparable to those of our test images, they can be \textasciitilde 97\% confident that the query image does not actually occur in the catalog and that the associated image is of a new individual'' \cite{thompson_finfindr_2022}. 

Whilst one advantage of finFindR is that it does not require non-technical users to train an identification model on their own catalogues, which is the case for the SNN component of the system outlined in this thesis, this may also lead to degraded performance where the user's catalogue is sufficiently different from that used to train finFindR. If the ability to re-train or fine-tune was provided, it may be the case that the top-$N$ accuracy obtained by finFindR on the given data would improve.

\section{Summary}\label{ch:SNNEvaluation,sec:Summary}

This chapter evaluates the approach to most likely catalogue matching developed in this thesis. To begin, the effect on model performance of additional photo-id data is examined. This work shows that while the additional data improves overall generalisability, a model capable of vastly reducing the search space can be obtained utilising only the data collected in a single fieldwork study, even when running inference on data from a different spatio-temporal environment.

Next, the effect of background retention on embedding generation is examined. By training a model using bounding box imagery rather than masks and examining the embeddings generated for various inputs, it can be seen that embeddings can be influenced more by feature heavy background surrounding a dorsal fin than the fin itself, at least when the model is trained on data collected over a small spatio-temporal scale. This emphasises the importance of detection masks over bounding boxes, further reinforcing the decision made in Section \ref{ch:cetDet,sec:deciding,sub:boundingBoxInvestigation}.

The generalisability of utilising SNNs for the task of automatic most likely catalogue matching is then examined through the use of a second smaller photo-id catalogue provided by the Chicago Zoological Society's Sarasota Dolphin Research Program. Training an SNN on this data yields a model which is highly accurate, achieving 81.25\% top-1, 95.00\% top-5, and 97.50\% top-10 accuracies. This provides evidence to suggest that SNNs are a viable approach to automatic most likely matching, regardless of catalogue size, species, or environment.

The task of most likely catalogue matching is then framed as a standard image classification problem in order to provide a baseline with which to compare the previously developed SNN approach. Experiments show that the model architectures trained are incapable of classifying images in the NDD AU SMRU dataset with high accuracy. This is hypothesised to be a result of the extreme fine-grained nature and high intra-class variability rendering the models incapable of identifying meaningful features for classification, which could be negated thanks to the use of triplets during training and clustering generated embeddings into a latent space.

Finally, a comparison between the system developed in this thesis and finFindR \cite{thompson_finfindr_2022}, the current most commonly used photo-id aid \cite{tyson_moore_rise_2022}, is presented. Evaluation of both systems using images which make up the NDD AU SMRU dataset shows that finFindR has a high likelihood of missing dorsal fins present in images under certain conditions and has a lower top-$N$ accuracy compared to the system developed in this thesis.

Experimentation throughout this chapter has shown that SNNs are an accurate and generalisable approach to the task of catalogue matching. When accompanied by a dorsal fin detector and post-processing methodology, a fully automatic photo-id aid can be created which is robust to changes in species of interest, geography, and time. A summary of the work undertaken throughout this thesis to develop an automatic photo-id aid is provided in Chapter \ref{ch:Conclusion}. Possible avenues for further work are also explored in detail.
